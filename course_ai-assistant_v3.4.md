# üéì Self-Paced Course: Building Your AI Director (v3.4)

**Project Theme:** The AI Director ‚Äî One-Man Marketing Agency  
**Based on:** `decodingai-magazine/second-brain-ai-assistant-course`  
**Instructor:** GitHub Copilot (Directed by You)  
**Platform:** GitHub Codespaces (Learning) & Google Colab (Training)  
**Last Updated:** January 2026

---

## üé¨ AI Director Concept

> **"One Brain (Director), Many Hands (Tools)"**  
> ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà Chatbot ‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∑‡∏≠ **Head of Marketing** ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏¥‡∏ï‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

### What is AI Director?

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    üß† AI DIRECTOR ARCHITECTURE v3.4             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ                    ‚îÇ   AI DIRECTOR    ‚îÇ                         ‚îÇ
‚îÇ                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                         ‚îÇ
‚îÇ                    ‚îÇ  ‚îÇ  T5Gemma 2 ‚îÇ  ‚îÇ  ‚Üê Multimodal Brain     ‚îÇ
‚îÇ                    ‚îÇ  ‚îÇ  (4B-4B)   ‚îÇ  ‚îÇ    (Text + Image)       ‚îÇ
‚îÇ                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                         ‚îÇ
‚îÇ                    ‚îÇ        +         ‚îÇ                         ‚îÇ
‚îÇ                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                         ‚îÇ
‚îÇ                    ‚îÇ  ‚îÇ Function   ‚îÇ  ‚îÇ  ‚Üê Tool Orchestrator    ‚îÇ
‚îÇ                    ‚îÇ  ‚îÇ   Gemma    ‚îÇ  ‚îÇ    (270M, On-device)    ‚îÇ
‚îÇ                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                         ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îÇ                             ‚îÇ                                    ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ                   ‚îÇ               ‚îÇ
‚îÇ         ‚ñº                   ‚ñº                   ‚ñº               ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ   ‚îÇ STRATEGY  ‚îÇ      ‚îÇ  CONTENT  ‚îÇ      ‚îÇPRODUCTION ‚îÇ          ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§          ‚îÇ
‚îÇ   ‚îÇ‚Ä¢ Campaign ‚îÇ      ‚îÇ‚Ä¢ Script   ‚îÇ      ‚îÇ‚Ä¢ Image Gen‚îÇ          ‚îÇ
‚îÇ   ‚îÇ‚Ä¢ Audience ‚îÇ      ‚îÇ‚Ä¢ Prompts  ‚îÇ      ‚îÇ‚Ä¢ Voice Gen‚îÇ          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ‚Ä¢ Video    ‚îÇ          ‚îÇ
‚îÇ                                          ‚îÇ  Compose  ‚îÇ          ‚îÇ
‚îÇ                                          ‚îÇ‚Ä¢ Smart Cut‚îÇ          ‚îÇ
‚îÇ                                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê      ‚îÇ
‚îÇ   AI Director: ‡∏£‡∏±‡∏ö‡πÇ‡∏à‡∏ó‡∏¢‡πå ‚Üí ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô ‚Üí ‡∏™‡∏£‡πâ‡∏≤‡∏á/‡∏ï‡∏±‡∏î‡∏ï‡πà‡∏≠ ‚Üí ‡∏™‡πà‡∏á‡∏°‡∏≠‡∏ö          ‚îÇ
‚îÇ   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê      ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üí∞ Zero Cost Strategy Edition

> **"‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏°‡πâ‡πÅ‡∏ï‡πà‡∏ö‡∏≤‡∏ó‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"** - ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏ö‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡∏π‡∏Å‡∏ö‡∏±‡∏ï‡∏£‡πÄ‡∏Ñ‡∏£‡∏î‡∏¥‡∏ï

---

## üÜï What's New in v3.4

| ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á | ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î |
|---------|-----------|
| üìö **Comprehensive Documentation** | ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å component |
| üîó **Official Links** | Links ‡πÑ‡∏õ‡∏¢‡∏±‡∏á official documentation |
| üìñ **Tutorials & Examples** | ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á notebooks ‡πÅ‡∏•‡∏∞ tutorials |
| üáπüá≠ **Thai Language Guide** | Best practices ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ |
| üéØ **Quick Reference** | ‡∏™‡∏£‡∏∏‡∏õ API, parameters, best practices |

### v3.4 vs v3.3 Comparison

| Feature | v3.3 | v3.4 |
|---------|------|------|
| Smart Cut | ‚úÖ | ‚úÖ |
| Documentation | Basic | **Comprehensive** |
| Official Links | ‚ùå | ‚úÖ **NEW** |
| Example Notebooks | ‚ùå | ‚úÖ **NEW** |
| Thai Language Guide | ‚ùå | ‚úÖ **NEW** |
| Troubleshooting | ‚ùå | ‚úÖ **NEW** |

---

## ‚úÖ Zero Cost Stack (Updated v3.4)

| ‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö | ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ (‡∏ü‡∏£‡∏µ 100%) | ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏ |
|-----------|---------------------|---------|
| **Compute** | GitHub Codespaces (2-core) | 120 ‡∏ä‡∏°./‡πÄ‡∏î‡∏∑‡∏≠‡∏ô |
| **Database** | MongoDB Atlas (M0) | Singapore, 512MB |
| **Vector DB** | ChromaDB (Local) | ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô Codespace |
| **GPU (Train)** | Google Colab (Free T4) | Save ‡∏•‡∏á Drive |
| **LLM API** | Gemini 1.5 Flash | Google AI Studio |
| **Thinker Model** | T5Gemma 2 (1B-1B) | HF Inference API |
| **Executor Model** | FunctionGemma (270M) | Run locally |
| **Optimization** | Meta Ax | Bayesian hyperparameter tuning |
| **Image Gen** | HF Inference API | SDXL/Flux |
| **Voice Gen** | Edge-TTS | Microsoft Edge voices |
| **Video Compose** | MoviePy | Python library |
| **Smart Cut** | FFmpeg + Whisper | ‡∏ï‡∏±‡∏î‡∏ï‡πà‡∏≠‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ |
| **Transcription** | Whisper (tiny/base) | ‡∏£‡∏±‡∏ô CPU ‡πÑ‡∏î‡πâ |

---

## üìÖ Course Syllabus

### üó∫Ô∏è Learning Path

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 AI DIRECTOR LEARNING PATH v3.4                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ   FOUNDATION                                                     ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ   ‚îÇModule 1 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇModule 2 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇModule 3 ‚îÇ                    ‚îÇ
‚îÇ   ‚îÇDual-Arch‚îÇ    ‚îÇ  ETL    ‚îÇ    ‚îÇDataset+ ‚îÇ                    ‚îÇ
‚îÇ   ‚îÇ Design  ‚îÇ    ‚îÇ         ‚îÇ    ‚îÇPrompts  ‚îÇ                    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                                       ‚îÇ                         ‚îÇ
‚îÇ   CORE                                ‚ñº                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ   ‚îÇModule 4 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇMod. 4.5 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇModule 5 ‚îÇ                    ‚îÇ
‚îÇ   ‚îÇFine-tune‚îÇ    ‚îÇ Meta Ax ‚îÇ    ‚îÇ  RAG    ‚îÇ                    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                                       ‚îÇ                         ‚îÇ
‚îÇ   PRODUCTION                          ‚ñº                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ   ‚îÇModule 6 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇMod. 6.5 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇModule 7 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇModule 8 ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ Tools   ‚îÇ    ‚îÇSmart Cut‚îÇ    ‚îÇ  Agent  ‚îÇ    ‚îÇ Deploy  ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ Setup   ‚îÇ    ‚îÇ         ‚îÇ    ‚îÇ         ‚îÇ    ‚îÇ         ‚îÇ    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                       ‚îÇ                         ‚îÇ
‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ                              ‚îÇ  FINAL PROJECT  ‚îÇ                ‚îÇ
‚îÇ                              ‚îÇ  Full Pipeline  ‚îÇ                ‚îÇ
‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

# üìö COMPREHENSIVE DOCUMENTATION & REFERENCES

> **v3.4 ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å component ‡∏û‡∏£‡πâ‡∏≠‡∏° links ‡πÅ‡∏•‡∏∞ tutorials**

---

## üß† Models & AI Documentation

### T5Gemma 2 (Thinker Model)

**Overview:**
T5Gemma 2 ‡πÄ‡∏õ‡πá‡∏ô encoder-decoder model ‡∏à‡∏≤‡∏Å Google ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö multimodal (text + image) ‡πÅ‡∏•‡∏∞ long context (128K tokens) ‡πÉ‡∏ô 140+ ‡∏†‡∏≤‡∏©‡∏≤

**Available Sizes:**

| Model | Total Params | VRAM | Model ID |
|-------|--------------|------|----------|
| 270M-270M | ~370M | ~1GB | `google/t5gemma-2-270m-270m` |
| 1B-1B | ~1.7B | ~4GB | `google/t5gemma-2-1b-1b` |
| 4B-4B | ~7B | ~16GB | `google/t5gemma-2-4b-4b` |

**Key Features:**
- Multimodal: ‡∏£‡∏±‡∏ö text + image input
- Long Context: 128K token window
- Multilingual: 140+ languages ‡∏£‡∏ß‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
- Tied embeddings: ‡∏•‡∏î parameters
- Merged attention: ‡∏£‡∏ß‡∏° self + cross attention

**Official Documentation:**

| Resource | Link |
|----------|------|
| üìÑ Model Card (270M) | https://huggingface.co/google/t5gemma-2-270m-270m |
| üìÑ Model Card (1B) | https://huggingface.co/google/t5gemma-2-1b-1b |
| üìÑ Model Card (4B) | https://huggingface.co/google/t5gemma-2-4b-4b |
| üìñ HF Transformers Doc | https://huggingface.co/docs/transformers/model_doc/t5gemma2 |
| üì∞ Google Blog | https://blog.google/technology/developers/t5gemma-2/ |
| üì¶ HF Collection | https://huggingface.co/collections/google/t5gemma-2 |
| üìù ArXiv Paper | Search "T5Gemma 2: Seeing, Reading, and Understanding Longer" |

**Quick Start Code:**

```python
from transformers import AutoProcessor, AutoModelForSeq2SeqLM
import requests
from PIL import Image

# Load model and processor
processor = AutoProcessor.from_pretrained("google/t5gemma-2-1b-1b")
model = AutoModelForSeq2SeqLM.from_pretrained("google/t5gemma-2-1b-1b")

# Text-only example
text = "Translate to Thai: Hello, how are you?"
inputs = processor(text=text, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=50)
print(processor.decode(outputs[0]))

# Multimodal example (text + image)
url = "https://example.com/image.jpg"
image = Image.open(requests.get(url, stream=True).raw)
prompt = "<start_of_image> Describe this image in detail"
inputs = processor(text=prompt, images=image, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)
print(processor.decode(outputs[0]))
```

---

### FunctionGemma (Executor Model)

**Overview:**
FunctionGemma ‡πÄ‡∏õ‡πá‡∏ô 270M model ‡∏à‡∏≤‡∏Å Google ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö function/tool calling ‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏ö‡∏ô edge devices

**Key Features:**
- **Tiny but Powerful:** 270M params, ‡∏£‡∏±‡∏ô CPU ‡πÑ‡∏î‡πâ
- **Tool Calling Expert:** Natural language ‚Üí JSON
- **Fine-tune Friendly:** 58% ‚Üí 85% accuracy after fine-tuning
- **On-device Ready:** 550MB RAM

**Official Documentation:**

| Resource | Link |
|----------|------|
| üìÑ Model Card | https://huggingface.co/google/functiongemma-270m-it |
| üìÑ Unsloth GGUF | https://huggingface.co/unsloth/functiongemma-270m-it-GGUF |
| üì∞ Google Blog | https://blog.google/technology/developers/functiongemma/ |
| üìñ Unsloth Guide | https://docs.unsloth.ai/models/functiongemma |
| üéÆ Ollama | https://ollama.com/library/functiongemma |
| üìñ LM Studio Guide | https://lmstudio.ai/blog/functiongemma-unsloth |

**Fine-Tuning Notebooks (Unsloth):**

| Notebook | Description |
|----------|-------------|
| [Reason Before Tool Calling](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/FunctionGemma_(270M)-Reasoning.ipynb) | Fine-tune to "think" before calling |
| [Mobile Actions](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/FunctionGemma_(270M)-MobileActions.ipynb) | Android system actions |
| [Multi-Turn Tool Calling](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/FunctionGemma_(270M)-MultiTurn.ipynb) | Chain tool calls |
| [LM Studio Export](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/FunctionGemma_(270M)-LMStudio.ipynb) | Fine-tune + export to GGUF |

**Tool Definition Format:**

```python
def get_weather(city: str) -> str:
    """
    Get the current weather for a city.
    
    Args:
        city: The name of the city
        
    Returns:
        A string describing the weather
    """
    return json.dumps({'city': city, 'temperature': 22, 'unit': 'celsius'})

# Apply chat template
tokenizer.apply_chat_template(
    [{"role": "user", "content": "What's the weather in Bangkok?"}],
    tools=[get_weather],
    add_generation_prompt=True,
    tokenize=False,
)
```

**Output Format:**

```
<start_function_call>call:get_weather{city:<escape>Bangkok<escape>}<end_function_call>
```

---

### Gemini API (Dataset Generation)

**Overview:**
Gemini API ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö generate training datasets ‡πÅ‡∏•‡∏∞ fallback inference

**Free Tier Limits (Dec 2025):**

| Model | RPM | TPM | RPD |
|-------|-----|-----|-----|
| Gemini 2.5 Pro | 2 | 250K | 50 |
| Gemini 2.5 Flash | 10 | 250K | 500 |
| Gemini 2.5 Flash-Lite | 15 | 250K | 1,000 |

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† API Homepage | https://ai.google.dev/gemini-api/docs |
| üöÄ Quickstart | https://ai.google.dev/gemini-api/docs/quickstart |
| üìã Models List | https://ai.google.dev/gemini-api/docs/models |
| üí∞ Pricing | https://ai.google.dev/pricing |
| üîë Get API Key | https://aistudio.google.com/apikey |

**Quick Start Code:**

```python
from google import genai

# Initialize client (API key from environment)
client = genai.Client()

# Simple generation
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Explain AI Director concept in Thai"
)
print(response.text)

# With system instruction
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Generate a marketing brief for coffee shop",
    config={
        "system_instruction": "You are a Thai marketing expert.",
        "temperature": 0.7,
        "max_output_tokens": 1000
    }
)
```

---

### Meta Ax (Hyperparameter Optimization)

**Overview:**
Ax ‡πÄ‡∏õ‡πá‡∏ô open-source Bayesian optimization platform ‡∏à‡∏≤‡∏Å Meta ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hyperparameter tuning

**Key Features:**
- **Bayesian Optimization:** ‡πÉ‡∏ä‡πâ Gaussian Process
- **Multi-objective:** Optimize ‡∏´‡∏•‡∏≤‡∏¢ metrics ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
- **Built on BoTorch:** PyTorch-based
- **Production Ready:** ‡πÉ‡∏ä‡πâ‡∏ó‡∏µ‡πà Meta scale

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† Ax Website | https://ax.dev/ |
| üì¶ GitHub | https://github.com/facebook/Ax |
| üìñ Tutorials | https://ax.dev/tutorials/ |
| üìÑ BoTorch | https://botorch.org/docs/introduction/ |
| üì∞ Meta Blog | https://engineering.fb.com/2025/11/18/open-source/efficient-optimization-ax-open-platform-adaptive-experimentation/ |

**Installation:**

```bash
pip install ax-platform botorch
```

**Quick Start Code:**

```python
from ax import Client, RangeParameterConfig

# Initialize client
client = Client()

# Define search space
client.configure_experiment(
    parameters=[
        RangeParameterConfig(name="learning_rate", bounds=(1e-5, 1e-3), parameter_type="float"),
        RangeParameterConfig(name="batch_size", bounds=(4, 32), parameter_type="int"),
        RangeParameterConfig(name="lora_r", bounds=(4, 32), parameter_type="int"),
        RangeParameterConfig(name="epochs", bounds=(1, 5), parameter_type="int"),
    ],
)

# Define objective (minimize loss)
client.configure_optimization(objective="-1 * eval_loss")

# Run optimization loop
for _ in range(20):
    for trial_index, parameters in client.get_next_trials(max_trials=1).items():
        # Train model with these parameters
        eval_loss = train_model(
            learning_rate=parameters["learning_rate"],
            batch_size=parameters["batch_size"],
            lora_r=parameters["lora_r"],
            epochs=parameters["epochs"]
        )
        
        # Report result
        client.complete_trial(
            trial_index=trial_index,
            raw_data={"eval_loss": eval_loss}
        )

# Get best parameters
best_params = client.get_best_parameterization()
print(f"Best parameters: {best_params}")
```

**Multi-Objective Example:**

```python
from ax.service.ax_client import AxClient
from ax.service.utils.instantiation import ObjectiveProperties

ax_client = AxClient()
ax_client.create_experiment(
    parameters=[
        {"name": "learning_rate", "type": "range", "bounds": [1e-5, 1e-3]},
        {"name": "lora_r", "type": "range", "bounds": [4, 32]},
    ],
    objectives={
        "eval_loss": ObjectiveProperties(minimize=True),
        "inference_speed": ObjectiveProperties(minimize=False),
        "memory_usage": ObjectiveProperties(minimize=True),
    },
)
```

---

### LoRA/PEFT (Efficient Fine-Tuning)

**Overview:**
LoRA (Low-Rank Adaptation) ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ parameter-efficient fine-tuning ‡∏ó‡∏µ‡πà train ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ small adapter weights

**Key Benefits:**
- **Memory Efficient:** ‡∏•‡∏î trainable params ~90%
- **Fast Training:** ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ full fine-tuning ‡∏°‡∏≤‡∏Å
- **Small Checkpoints:** Adapter weights ~10-50MB
- **No Inference Latency:** Merge weights ‡πÑ‡∏î‡πâ

**Official Documentation:**

| Resource | Link |
|----------|------|
| üì¶ PEFT GitHub | https://github.com/huggingface/peft |
| üìñ PEFT Docs | https://huggingface.co/docs/peft |
| üìñ LoRA Guide | https://huggingface.co/docs/peft/main/en/conceptual_guides/lora |
| üìñ Transformers PEFT | https://huggingface.co/docs/transformers/en/peft |
| üì∞ PEFT Blog | https://huggingface.co/blog/peft |
| üìñ Smol Course | https://huggingface.co/learn/smol-course/en/unit1/3a |

**Key Parameters:**

| Parameter | Description | Recommended |
|-----------|-------------|-------------|
| `r` | Rank of update matrices | 8-16 |
| `lora_alpha` | Scaling factor | 16-32 |
| `target_modules` | Which layers to adapt | `q_proj, k_proj, v_proj, o_proj` |
| `lora_dropout` | Dropout for LoRA layers | 0.05-0.1 |
| `bias` | Whether to train bias | `none` |

**Quick Start Code:**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType

# Load base model
model = AutoModelForCausalLM.from_pretrained("google/gemma-2-2b")
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-2b")

# Configure LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
)

# Apply LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Output: trainable params: 4,194,304 || all params: 2,506,172,416 || trainable%: 0.1673

# Train with HuggingFace Trainer
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./lora_model",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=2e-4,
    fp16=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)
trainer.train()

# Save adapter only (~50MB)
model.save_pretrained("./lora_adapter")

# Load and merge for inference
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained("google/gemma-2-2b")
model = PeftModel.from_pretrained(base_model, "./lora_adapter")
model = model.merge_and_unload()  # Merge weights
```

**QLoRA (4-bit) for Low VRAM:**

```python
from transformers import BitsAndBytesConfig
import torch

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

# Load quantized model
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2-2b",
    quantization_config=bnb_config,
    device_map="auto",
)

# Apply LoRA on quantized model
model = get_peft_model(model, lora_config)
```

---

## üóÑÔ∏è Data & Storage Documentation

### MongoDB Atlas (Free Tier)

**Overview:**
MongoDB Atlas M0 ‡πÉ‡∏´‡πâ cloud database ‡∏ü‡∏£‡∏µ 512MB ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö brand data, campaigns, transcripts

**Free Tier Specs:**
- Storage: 512MB
- Connections: 100 max
- RAM: Shared
- vCPU: Shared
- Regions: AWS, GCP, Azure

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† MongoDB Atlas | https://www.mongodb.com/atlas |
| üìñ Getting Started | https://www.mongodb.com/docs/atlas/getting-started/ |
| üìñ Deploy Free Cluster | https://www.mongodb.com/docs/atlas/tutorial/deploy-free-tier-cluster/ |
| üìñ M0 Limits | https://www.mongodb.com/docs/atlas/reference/free-shared-limitations/ |
| üìñ PyMongo Docs | https://pymongo.readthedocs.io/ |

**Setup Steps:**

1. ‡∏™‡∏£‡πâ‡∏≤‡∏á account ‡∏ó‡∏µ‡πà https://www.mongodb.com/cloud/atlas
2. Create Free Cluster (M0)
3. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Region: Singapore (ap-southeast-1)
4. Whitelist IP: 0.0.0.0/0 (for development)
5. Create Database User
6. Get Connection String

**Quick Start Code:**

```python
from pymongo import MongoClient
from datetime import datetime

# Connect to MongoDB Atlas
MONGO_URI = "mongodb+srv://user:password@cluster.mongodb.net/"
client = MongoClient(MONGO_URI)
db = client["ai_director"]

# Collections
brands = db["brands"]
campaigns = db["campaigns"]
transcripts = db["transcripts"]

# Insert brand data
brand_data = {
    "name": "CoffeeLab",
    "tone": "friendly, premium, modern",
    "colors": ["#8B4513", "#F5F5DC", "#2C1810"],
    "target_audience": "young professionals 25-35",
    "created_at": datetime.now()
}
brands.insert_one(brand_data)

# Query
brand = brands.find_one({"name": "CoffeeLab"})
print(brand)
```

**Schema Design for AI Director:**

```python
# Brand Schema
{
    "_id": ObjectId,
    "name": str,
    "description": str,
    "tone": str,
    "colors": [str],
    "fonts": [str],
    "target_audience": str,
    "brand_values": [str],
    "created_at": datetime,
    "updated_at": datetime
}

# Campaign Schema
{
    "_id": ObjectId,
    "brand_id": ObjectId,
    "name": str,
    "brief": str,
    "generated_prompts": [str],
    "assets": [{
        "type": "image|video|audio",
        "url": str,
        "metadata": dict
    }],
    "status": "draft|active|completed",
    "created_at": datetime
}

# Transcript Schema
{
    "_id": ObjectId,
    "video_id": str,
    "filename": str,
    "duration": float,
    "language": str,
    "segments": [{
        "start": float,
        "end": float,
        "text": str,
        "words": [{
            "word": str,
            "start": float,
            "end": float,
            "confidence": float
        }]
    }],
    "silence_regions": [{
        "start": float,
        "end": float,
        "duration": float
    }],
    "created_at": datetime
}
```

---

### ChromaDB (Vector Database)

**Overview:**
ChromaDB ‡πÄ‡∏õ‡πá‡∏ô open-source vector database ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAG ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡πá‡∏ö embeddings ‡∏Ç‡∏≠‡∏á brand knowledge ‡πÅ‡∏•‡∏∞ prompt templates

**Key Features:**
- In-memory ‡∏´‡∏£‡∏∑‡∏≠ persistent storage
- Built-in embedding functions
- Metadata filtering
- Python-native

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† ChromaDB | https://www.trychroma.com/ |
| üì¶ GitHub | https://github.com/chroma-core/chroma |
| üìñ Documentation | https://docs.trychroma.com/ |
| üìñ LangChain Integration | https://python.langchain.com/docs/integrations/vectorstores/chroma |
| üìñ Real Python Tutorial | https://realpython.com/chromadb-vector-database/ |

**Installation:**

```bash
pip install chromadb
```

**Quick Start Code:**

```python
import chromadb
from chromadb.utils import embedding_functions

# Create persistent client
client = chromadb.PersistentClient(path="./chroma_db")

# Use sentence-transformers for embeddings
embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# Create collection
collection = client.get_or_create_collection(
    name="brand_knowledge",
    embedding_function=embedding_fn,
    metadata={"hnsw:space": "cosine"}
)

# Add documents
collection.add(
    documents=[
        "CoffeeLab ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå‡∏Å‡∏≤‡πÅ‡∏ü‡∏û‡∏£‡∏µ‡πÄ‡∏°‡∏µ‡∏¢‡∏° ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ô‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà",
        "‡πÇ‡∏ó‡∏ô‡∏™‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏Ñ‡∏∑‡∏≠‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏•‡πÄ‡∏Ç‡πâ‡∏° ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏µ‡∏° ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô",
        "Target ‡∏Ñ‡∏∑‡∏≠ young professionals ‡∏≠‡∏≤‡∏¢‡∏∏ 25-35 ‡∏õ‡∏µ",
    ],
    metadatas=[
        {"category": "brand", "type": "description"},
        {"category": "brand", "type": "colors"},
        {"category": "brand", "type": "audience"},
    ],
    ids=["doc1", "doc2", "doc3"]
)

# Query
results = collection.query(
    query_texts=["‡πÉ‡∏Ñ‡∏£‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á CoffeeLab"],
    n_results=2,
    where={"category": "brand"}
)
print(results)
```

**RAG Pattern:**

```python
class BrandKnowledgeRAG:
    def __init__(self, collection):
        self.collection = collection
    
    def get_context(self, query: str, n_results: int = 3) -> str:
        results = self.collection.query(
            query_texts=[query],
            n_results=n_results
        )
        
        context = "\n".join(results["documents"][0])
        return context
    
    def generate_with_context(self, query: str, llm) -> str:
        context = self.get_context(query)
        
        prompt = f"""Use the following context to answer the question.

Context:
{context}

Question: {query}

Answer:"""
        
        return llm.generate(prompt)
```

---

### Hugging Face Datasets

**Overview:**
‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prepare ‡πÅ‡∏•‡∏∞ format training datasets

**Official Documentation:**

| Resource | Link |
|----------|------|
| üì¶ GitHub | https://github.com/huggingface/datasets |
| üìñ Documentation | https://huggingface.co/docs/datasets |
| üìñ Loading Datasets | https://huggingface.co/docs/datasets/loading |
| üìñ Process Data | https://huggingface.co/docs/datasets/process |

**Quick Start Code:**

```python
from datasets import Dataset, DatasetDict

# Create dataset from dict
train_data = {
    "instruction": [
        "‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏Å‡∏≤‡πÅ‡∏ü",
        "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå target audience",
    ],
    "input": [
        "‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå: CoffeeLab, ‡∏™‡πÑ‡∏ï‡∏•‡πå: minimal",
        "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤: Cold Brew Premium",
    ],
    "output": [
        "A minimalist coffee cup on marble surface...",
        "‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡∏Ñ‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏® ‡∏≠‡∏≤‡∏¢‡∏∏ 25-35...",
    ],
}

dataset = Dataset.from_dict(train_data)

# Split dataset
dataset = dataset.train_test_split(test_size=0.1)

# Save to disk
dataset.save_to_disk("./ai_director_dataset")

# Load from disk
loaded_dataset = DatasetDict.load_from_disk("./ai_director_dataset")
```

---

## üé® Production Tools Documentation

### Image Generation (HF Inference API)

**Overview:**
‡πÉ‡∏ä‡πâ Hugging Face Inference API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö generate images ‡∏î‡πâ‡∏ß‡∏¢ SDXL/Flux

**Official Documentation:**

| Resource | Link |
|----------|------|
| üìñ Inference API | https://huggingface.co/docs/api-inference |
| üìñ SDXL Model | https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0 |
| üìñ Flux Models | https://huggingface.co/black-forest-labs |

**Quick Start Code:**

```python
import requests
import io
from PIL import Image

HF_TOKEN = "your_token_here"
API_URL = "https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-xl-base-1.0"

def generate_image(prompt: str, negative_prompt: str = "") -> Image.Image:
    headers = {"Authorization": f"Bearer {HF_TOKEN}"}
    
    payload = {
        "inputs": prompt,
        "parameters": {
            "negative_prompt": negative_prompt,
            "num_inference_steps": 30,
            "guidance_scale": 7.5,
        }
    }
    
    response = requests.post(API_URL, headers=headers, json=payload)
    
    if response.status_code == 200:
        image = Image.open(io.BytesIO(response.content))
        return image
    else:
        raise Exception(f"Error: {response.status_code}")

# Usage
image = generate_image(
    prompt="A premium coffee cup on marble surface, soft morning light, minimal style, 4K",
    negative_prompt="blurry, low quality, watermark"
)
image.save("coffee_ad.png")
```

---

### Edge-TTS (Voice Generation)

**Overview:**
Edge-TTS ‡πÉ‡∏ä‡πâ Microsoft Edge's TTS service ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏û‡∏≤‡∏Å‡∏¢‡πå ‡∏ü‡∏£‡∏µ 100%

**Key Features:**
- Neural voices ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á
- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤ ‡∏£‡∏ß‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
- ‡∏õ‡∏£‡∏±‡∏ö rate, pitch, volume ‡πÑ‡∏î‡πâ
- ‡∏™‡∏£‡πâ‡∏≤‡∏á subtitles (.srt/.vtt) ‡πÑ‡∏î‡πâ

**Official Documentation:**

| Resource | Link |
|----------|------|
| üì¶ PyPI | https://pypi.org/project/edge-tts/ |
| üì¶ GitHub | https://github.com/rany2/edge-tts |

**Thai Voices:**

| Voice | Gender | Style |
|-------|--------|-------|
| `th-TH-PremwadeeNeural` | Female | Friendly, Positive |
| `th-TH-NiwatNeural` | Male | Friendly, Positive |

**Installation:**

```bash
pip install edge-tts
```

**Quick Start Code:**

```python
import edge_tts
import asyncio

async def generate_voice(text: str, voice: str, output_file: str):
    """Generate voice from text using Edge TTS"""
    communicate = edge_tts.Communicate(
        text=text,
        voice=voice,
        rate="+0%",      # -50% to +100%
        volume="+0%",    # -50% to +100%
        pitch="+0Hz"     # -50Hz to +50Hz
    )
    
    await communicate.save(output_file)
    print(f"Saved to {output_file}")

# Usage
text = "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡πÅ‡∏ü Cold Brew ‡πÉ‡∏´‡∏°‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å CoffeeLab"
asyncio.run(generate_voice(
    text=text,
    voice="th-TH-NiwatNeural",
    output_file="voiceover.mp3"
))

# Generate with subtitles
async def generate_with_subtitles(text: str, voice: str, audio_file: str, subtitle_file: str):
    communicate = edge_tts.Communicate(text, voice)
    
    submaker = edge_tts.SubMaker()
    
    with open(audio_file, "wb") as audio:
        async for chunk in communicate.stream():
            if chunk["type"] == "audio":
                audio.write(chunk["data"])
            elif chunk["type"] == "WordBoundary":
                submaker.feed(chunk)
    
    with open(subtitle_file, "w", encoding="utf-8") as srt:
        srt.write(submaker.generate_srt())

asyncio.run(generate_with_subtitles(
    text=text,
    voice="th-TH-NiwatNeural",
    audio_file="voiceover.mp3",
    subtitle_file="voiceover.srt"
))
```

**List Available Voices:**

```bash
edge-tts --list-voices | grep th-TH
```

---

### MoviePy (Video Composition)

**Overview:**
MoviePy ‡πÄ‡∏õ‡πá‡∏ô Python library ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö video editing: cuts, concatenations, text overlays, compositing

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† Documentation | https://zulko.github.io/moviepy/ |
| üì¶ GitHub | https://github.com/Zulko/moviepy |
| üì¶ PyPI | https://pypi.org/project/moviepy/ |
| üìñ User Guide | https://zulko.github.io/moviepy/user_guide/index.html |

**Installation:**

```bash
pip install moviepy

# Also need FFmpeg (usually auto-installed)
# Ubuntu: sudo apt-get install ffmpeg
# Mac: brew install ffmpeg
```

**Quick Start Code:**

```python
from moviepy import VideoFileClip, AudioFileClip, TextClip, CompositeVideoClip, concatenate_videoclips

# Load video
clip = VideoFileClip("raw_video.mp4")

# Basic operations
clip = clip.subclipped(10, 30)  # Cut from 10s to 30s
clip = clip.with_volume_scaled(0.8)  # Reduce volume to 80%
clip = clip.resized(width=1080)  # Resize

# Add text overlay
txt_clip = TextClip(
    text="CoffeeLab - Premium Coffee",
    font="Arial.ttf",
    font_size=70,
    color='white',
    bg_color='black',
    size=(1080, None)
)
txt_clip = txt_clip.with_position('center').with_duration(5)

# Composite
final = CompositeVideoClip([clip, txt_clip])

# Add audio
audio = AudioFileClip("voiceover.mp3")
final = final.with_audio(audio)

# Export
final.write_videofile(
    "output.mp4",
    fps=30,
    codec="libx264",
    audio_codec="aac"
)

# Concatenate multiple clips
clips = [
    VideoFileClip("intro.mp4"),
    VideoFileClip("content.mp4"),
    VideoFileClip("outro.mp4"),
]
final = concatenate_videoclips(clips, method="compose")
final.write_videofile("full_video.mp4")
```

**Common Operations:**

```python
# Fade in/out
clip = clip.with_effects([vfx.FadeIn(1), vfx.FadeOut(1)])

# Speed up/slow down
clip = clip.with_speed_scaled(1.5)  # 1.5x speed

# Crop
clip = clip.cropped(x1=100, y1=100, x2=800, y2=600)

# Rotate
clip = clip.rotated(90)

# Mirror
clip = clip.with_effects([vfx.MirrorX()])
```

---

### Pillow (Image Processing)

**Overview:**
Pillow ‡πÄ‡∏õ‡πá‡∏ô Python library ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö image processing

**Official Documentation:**

| Resource | Link |
|----------|------|
| üìñ Documentation | https://pillow.readthedocs.io/ |
| üì¶ GitHub | https://github.com/python-pillow/Pillow |
| üì¶ PyPI | https://pypi.org/project/pillow/ |

**Quick Start Code:**

```python
from PIL import Image, ImageDraw, ImageFont, ImageFilter

# Open image
img = Image.open("input.jpg")

# Resize
img = img.resize((1080, 1080))

# Add text
draw = ImageDraw.Draw(img)
font = ImageFont.truetype("Arial.ttf", 60)
draw.text((100, 100), "CoffeeLab", fill="white", font=font)

# Apply filter
img = img.filter(ImageFilter.GaussianBlur(radius=2))

# Save
img.save("output.jpg", quality=95)
```

---

## ‚òÅÔ∏è Infrastructure Documentation

### GitHub Codespaces

**Overview:**
GitHub Codespaces ‡πÉ‡∏´‡πâ cloud development environment ‡∏ü‡∏£‡∏µ 120 ‡∏ä‡∏°./‡πÄ‡∏î‡∏∑‡∏≠‡∏ô

**Free Tier Limits:**
- 120 core-hours/month
- 15GB storage/month
- 2-core, 8GB RAM default

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† Codespaces | https://github.com/features/codespaces |
| üìñ Quickstart | https://docs.github.com/en/codespaces/getting-started/quickstart |
| üìñ Billing | https://docs.github.com/en/billing/managing-billing-for-github-codespaces |

**Tips:**
1. ‡πÉ‡∏ä‡πâ 2-core machine (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î hours)
2. Stop codespace ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ
3. Set auto-stop timeout (30 mins)
4. ‡πÉ‡∏ä‡πâ `.devcontainer.json` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö setup

**devcontainer.json Example:**

```json
{
  "name": "AI Director",
  "image": "mcr.microsoft.com/devcontainers/python:3.11",
  "postCreateCommand": "pip install -r requirements.txt",
  "customizations": {
    "vscode": {
      "extensions": [
        "ms-python.python",
        "GitHub.copilot"
      ]
    }
  }
}
```

---

### Google Colab

**Overview:**
Google Colab ‡πÉ‡∏´‡πâ free GPU (T4) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö training

**Free Tier:**
- GPU: Tesla T4 (15GB VRAM)
- Runtime: ~12 hours max
- Storage: Save to Google Drive

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† Colab | https://colab.research.google.com/ |
| üìñ FAQ | https://research.google.com/colaboratory/faq.html |

**Tips:**

```python
# Check GPU
!nvidia-smi

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Save model to Drive
model.save_pretrained('/content/drive/MyDrive/models/ai_director')

# Install packages
!pip install transformers peft accelerate bitsandbytes

# Keep session alive (run in separate cell)
import time
while True:
    time.sleep(60)
```

---

### Hugging Face Spaces (Deployment)

**Overview:**
HF Spaces ‡πÉ‡∏´‡πâ free hosting ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Gradio apps

**Free Tier:**
- CPU: 2 vCPU
- RAM: 16GB
- Storage: Ephemeral
- Auto-sleep: After inactivity

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† Spaces | https://huggingface.co/spaces |
| üìñ Gradio Spaces | https://huggingface.co/docs/hub/en/spaces-sdks-gradio |
| üìñ Using HF Integrations | https://www.gradio.app/guides/using-hugging-face-integrations |

**Quick Deploy:**

1. Create new Space at https://huggingface.co/new-space
2. Select Gradio SDK
3. Create `app.py` and `requirements.txt`
4. Push files

**app.py Example:**

```python
import gradio as gr
from transformers import pipeline

# Load model
pipe = pipeline("text-generation", model="your-model")

def generate(prompt):
    result = pipe(prompt, max_length=200)
    return result[0]["generated_text"]

# Create interface
demo = gr.Interface(
    fn=generate,
    inputs=gr.Textbox(label="Brief", lines=5),
    outputs=gr.Textbox(label="Output", lines=10),
    title="AI Director",
    description="Generate marketing content"
)

if __name__ == "__main__":
    demo.launch()
```

**requirements.txt:**

```
gradio
transformers
torch
```

---

## üìñ Tutorials & Example Notebooks

### End-to-End RAG Examples

| Notebook | Description | Link |
|----------|-------------|------|
| RAG with ChromaDB | Basic RAG implementation | [DataCamp Tutorial](https://www.datacamp.com/tutorial/chromadb-tutorial-step-by-step-guide) |
| RAG with Ollama | Local LLM RAG | [Medium Article](https://medium.com/@arunpatidar26/rag-chromadb-ollama-python-guide-for-beginners-30857499d0a0) |
| LlamaIndex + ChromaDB | Production RAG | [Dev.to Tutorial](https://dev.to/sophyia/how-to-build-a-rag-solution-with-llama-index-chromadb-and-ollama-20lb) |
| LangChain + Chroma | LangChain integration | [LangChain Docs](https://docs.langchain.com/oss/python/integrations/vectorstores/chroma) |

### LoRA Fine-Tuning Notebooks

| Notebook | Description | Link |
|----------|-------------|------|
| Fine-tune LLM with PEFT | Comprehensive guide | [HF Blog](https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face) |
| QLoRA on T4 | Free Colab training | [Phil Schmid](https://www.philschmid.de/fine-tune-llms-in-2025) |
| FunctionGemma Fine-tune | Tool calling | [Unsloth Notebooks](https://docs.unsloth.ai/models/functiongemma) |
| Gemma 3 Fine-tune | Vision + Text | [Unsloth Guide](https://docs.unsloth.ai/models/gemma-3-how-to-run-and-fine-tune) |
| DreamBooth LoRA | Stable Diffusion | [PEFT Guide](https://huggingface.co/docs/peft/main/en/task_guides/dreambooth_lora) |

### Multi-Modal Prompting Guides

| Resource | Description | Link |
|----------|-------------|------|
| T5Gemma 2 Usage | Image + Text prompting | [HF Model Card](https://huggingface.co/google/t5gemma-2-1b-1b) |
| PaliGemma Guide | Vision-Language | [Google AI](https://ai.google.dev/gemma/docs/paligemma) |
| Gemini Multimodal | API examples | [Google AI Docs](https://ai.google.dev/gemini-api/docs/vision) |

---

## üáπüá≠ Thai Language Best Practices

### Recommended Models for Thai

| Task | Model | Notes |
|------|-------|-------|
| **Text Generation** | Qwen 2.5-7B | Strong Thai support |
| **Embeddings** | paraphrase-multilingual-MiniLM-L12-v2 | Good for Thai RAG |
| **TTS** | th-TH-NiwatNeural / th-TH-PremwadeeNeural | Edge-TTS voices |
| **Transcription** | Whisper (base/small) | Decent Thai accuracy |
| **Translation** | Helsinki-NLP/opus-mt-th-en | Thai ‚Üî English |

### Thai Text Processing Tips

```python
# Thai word segmentation
from pythainlp.tokenize import word_tokenize

text = "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏î‡∏µ‡∏°‡∏≤‡∏Å"
tokens = word_tokenize(text, engine="newmm")
print(tokens)  # ['‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ', '‡∏Ñ‡∏£‡∏±‡∏ö', '‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ', '‡∏≠‡∏≤‡∏Å‡∏≤‡∏®', '‡∏î‡∏µ', '‡∏°‡∏≤‡∏Å']

# Thai sentence segmentation
from pythainlp.tokenize import sent_tokenize

text = "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡πÑ‡∏õ‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÄ‡∏ñ‡∏≠‡∏∞"
sentences = sent_tokenize(text)

# Thai spell check
from pythainlp.spell import spell

misspelled = "‡∏™‡∏ß‡∏±‡∏î‡∏î‡∏µ"
corrections = spell(misspelled)
```

### Prompt Engineering for Thai

```python
# Good: Be specific about language
prompt = """
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢
‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô copy ‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå‡∏Å‡∏≤‡πÅ‡∏ü CoffeeLab
‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡∏Ñ‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏® ‡∏≠‡∏≤‡∏¢‡∏∏ 25-35 ‡∏õ‡∏µ
‡πÇ‡∏ó‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á: ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÅ‡∏ï‡πà‡∏î‡∏π‡∏û‡∏£‡∏µ‡πÄ‡∏°‡∏µ‡∏¢‡∏°
‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß: 100-150 ‡∏Ñ‡∏≥
"""

# Include examples (few-shot)
prompt = """
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á copy ‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ:
- "‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡πÅ‡∏ü‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πà CoffeeLab Cold Brew"
- "‡∏ó‡∏∏‡∏Å‡∏´‡∏¢‡∏î‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏¥‡∏ñ‡∏µ‡∏û‡∏¥‡∏ñ‡∏±‡∏ô ‡∏à‡∏≤‡∏Å bean ‡∏ñ‡∏∂‡∏á cup"

‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô copy ‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô...
"""
```

---

## üîß Troubleshooting Guide

### Common Issues

#### 1. CUDA Out of Memory

```python
# Solution 1: Use 4-bit quantization
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(load_in_4bit=True)
model = AutoModel.from_pretrained("...", quantization_config=bnb_config)

# Solution 2: Reduce batch size
training_args = TrainingArguments(per_device_train_batch_size=2)

# Solution 3: Gradient checkpointing
model.gradient_checkpointing_enable()
```

#### 2. Colab Session Disconnect

```python
# Keep session alive
import time
from IPython.display import display, Javascript

def keep_alive():
    display(Javascript('function Click(){document.querySelector("colab-connect-button").click()}setInterval(Click, 60000)'))

keep_alive()
```

#### 3. MongoDB Connection Issues

```python
# Check connection
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure

try:
    client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
    client.admin.command('ping')
    print("Connected!")
except ConnectionFailure:
    print("Failed to connect. Check:")
    print("1. IP whitelist (0.0.0.0/0 for dev)")
    print("2. Username/password")
    print("3. Network connectivity")
```

#### 4. Edge-TTS Rate Limiting

```python
# Add delay between requests
import asyncio

async def generate_batch(texts, voice, output_dir):
    for i, text in enumerate(texts):
        await generate_voice(text, voice, f"{output_dir}/audio_{i}.mp3")
        await asyncio.sleep(1)  # 1 second delay
```

#### 5. FFmpeg Not Found

```bash
# Ubuntu/Codespaces
sudo apt-get update && sudo apt-get install -y ffmpeg

# Mac
brew install ffmpeg

# Check installation
ffmpeg -version
```

---

## üìä Quick Reference Cards

### API Keys Required

| Service | Get Key At | Env Variable |
|---------|------------|--------------|
| Hugging Face | https://huggingface.co/settings/tokens | `HF_TOKEN` |
| Google Gemini | https://aistudio.google.com/apikey | `GEMINI_API_KEY` |
| MongoDB Atlas | Atlas Dashboard | `MONGO_URI` |

### Model Loading Cheatsheet

```python
# T5Gemma 2
from transformers import AutoProcessor, AutoModelForSeq2SeqLM
processor = AutoProcessor.from_pretrained("google/t5gemma-2-1b-1b")
model = AutoModelForSeq2SeqLM.from_pretrained("google/t5gemma-2-1b-1b")

# FunctionGemma (with Unsloth)
from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained("unsloth/functiongemma-270m-it")

# With LoRA
from peft import PeftModel, LoraConfig, get_peft_model
lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "o_proj"])
model = get_peft_model(model, lora_config)

# 4-bit Quantization
from transformers import BitsAndBytesConfig
bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4")
model = AutoModel.from_pretrained("...", quantization_config=bnb_config)
```

### Common pip Installs

```bash
# Core
pip install transformers torch accelerate

# Fine-tuning
pip install peft bitsandbytes trl datasets

# Optimization
pip install ax-platform botorch

# Vector DB
pip install chromadb pymongo

# Production Tools
pip install edge-tts moviepy pillow

# Smart Cut
pip install openai-whisper ffmpeg-python pydub

# Deployment
pip install gradio
```

---

## üéØ Success Metrics

| Metric | Target |
|--------|--------|
| Strategy generation (T5Gemma 2) | < 30s |
| Tool calling (FunctionGemma) | < 1s per call |
| Tool calling accuracy | ‚â• 85% |
| Prompt quality | 1000+ chars |
| Total pipeline | < 5 min |
| Transcription speed | < 0.5x realtime |
| Auto trim | < 1 min per 10 min video |
| **Cost** | **$0.00** |

---

## üìö Additional Resources

### Official Documentation Hub

| Category | Links |
|----------|-------|
| **Models** | [T5Gemma 2](https://huggingface.co/collections/google/t5gemma-2) ‚Ä¢ [FunctionGemma](https://huggingface.co/google/functiongemma-270m-it) ‚Ä¢ [Gemini API](https://ai.google.dev/) |
| **Training** | [PEFT](https://huggingface.co/docs/peft) ‚Ä¢ [Unsloth](https://docs.unsloth.ai/) ‚Ä¢ [Meta Ax](https://ax.dev/) |
| **Data** | [MongoDB Atlas](https://www.mongodb.com/docs/atlas/) ‚Ä¢ [ChromaDB](https://docs.trychroma.com/) ‚Ä¢ [HF Datasets](https://huggingface.co/docs/datasets) |
| **Production** | [MoviePy](https://zulko.github.io/moviepy/) ‚Ä¢ [Edge-TTS](https://github.com/rany2/edge-tts) ‚Ä¢ [Whisper](https://github.com/openai/whisper) |
| **Deploy** | [HF Spaces](https://huggingface.co/docs/hub/spaces) ‚Ä¢ [Gradio](https://www.gradio.app/) |

### Community Resources

| Resource | Link |
|----------|------|
| Hugging Face Forum | https://discuss.huggingface.co/ |
| Unsloth Discord | https://discord.gg/unsloth |
| MoviePy Reddit | https://www.reddit.com/r/moviepy/ |

---

# üéì MODULE 1: DUAL-MODEL ARCHITECTURE DESIGN

> **"Understanding the Brain and Hands of AI Director"**

**Location:** GitHub Codespaces  
**Duration:** 3-4 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á  
**Difficulty:** ‚≠ê‚≠ê

---

## üìã Module Overview

### Learning Objectives

‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏ö Module 1 ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:
- ‚úÖ ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Dual-Model Architecture ‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
- ‚úÖ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á T5Gemma 2 (Thinker) ‡πÅ‡∏•‡∏∞ FunctionGemma (Executor)
- ‚úÖ ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ model ‡πÑ‡∏´‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö task ‡πÑ‡∏´‡∏ô
- ‚úÖ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á models
- ‚úÖ ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å Smart Cut feature ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£ integrate ‡∏Å‡∏±‡∏ö architecture

### Prerequisites

- GitHub account (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Codespaces)
- ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß LLM ‡πÅ‡∏•‡∏∞ AI models
- Python basics

---

## üß† Part 1: Why Dual-Model Architecture?

### The Problem with Single-Model Approach

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          ‚ùå SINGLE LARGE MODEL (Traditional Way)            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ   User: "‡∏™‡∏£‡πâ‡∏≤‡∏á video ‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏Å‡∏≤‡πÅ‡∏ü"                              ‚îÇ
‚îÇ     ‚îÇ                                                        ‚îÇ
‚îÇ     ‚ñº                                                        ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ   ‚îÇ  Large LLM (e.g., GPT-4, Claude)      ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  7B-70B+ parameters                    ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Slow inference                      ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Expensive API calls                 ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Needs prompt engineering            ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Not good at structured output       ‚îÇ                ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   Problems:                                                  ‚îÇ
‚îÇ   üí∏ Cost: $0.01-0.10 per request                           ‚îÇ
‚îÇ   üêå Speed: 5-30 seconds                                    ‚îÇ
‚îÇ   üé≤ Reliability: 60-70% correct tool calls                 ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Our Solution: Dual-Model Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         ‚úÖ DUAL-MODEL ARCHITECTURE (Our Way)                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ   User: "‡∏™‡∏£‡πâ‡∏≤‡∏á video ‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏Å‡∏≤‡πÅ‡∏ü"                              ‚îÇ
‚îÇ     ‚îÇ                                                        ‚îÇ
‚îÇ     ‚ñº                                                        ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ   ‚îÇ  üß† THINKER (T5Gemma 2 - 1B)          ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  Role: Strategy & Content Creation     ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Understand brief                    ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Generate creative prompts           ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Write marketing copy                ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Select video highlights             ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  Output: Natural language plan         ‚îÇ                ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                  ‚îÇ "Create premium coffee ad                ‚îÇ
‚îÇ                  ‚îÇ  with moody lighting..."                 ‚îÇ
‚îÇ                  ‚ñº                                           ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ   ‚îÇ  ‚ö° EXECUTOR (FunctionGemma - 270M)   ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  Role: Tool Orchestration              ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Parse instructions                  ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Call image_gen()                    ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Call voice_gen()                    ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Call video_compose()                ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  ‚Ä¢ Call smart_cut()                    ‚îÇ                ‚îÇ
‚îÇ   ‚îÇ  Output: Structured tool calls         ‚îÇ                ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                  ‚îÇ                                           ‚îÇ
‚îÇ                  ‚ñº                                           ‚îÇ
‚îÇ         [Generated Content]                                  ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   Benefits:                                                  ‚îÇ
‚îÇ   üí∞ Cost: $0.00 (free tier)                                ‚îÇ
‚îÇ   ‚ö° Speed: < 5 seconds total                               ‚îÇ
‚îÇ   üéØ Reliability: 85%+ tool calls                           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Principles

| Principle | Explanation |
|-----------|-------------|
| **Separation of Concerns** | Thinking (creative) ‚â† Doing (execution) |
| **Right Tool for Right Job** | Large model for creativity, tiny model for precision |
| **Cost Efficiency** | 1B + 270M < 7B model but better results |
| **Speed Optimization** | Parallel processing possible |
| **Reliability** | Fine-tuned specialist > General purpose |

---

## üéØ Part 2: T5Gemma 2 (The Thinker)

### What is T5Gemma 2?

T5Gemma 2 ‡∏Ñ‡∏∑‡∏≠ **encoder-decoder multimodal model** ‡∏à‡∏≤‡∏Å Google ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô generation ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô

### Key Characteristics

```python
T5Gemma 2 Profile:
‚îú‚îÄ‚îÄ Architecture: Encoder-Decoder (T5-style)
‚îú‚îÄ‚îÄ Parameters: 270M / 1B / 4B
‚îú‚îÄ‚îÄ Context Window: 128K tokens
‚îú‚îÄ‚îÄ Modality: Text + Image (multimodal)
‚îú‚îÄ‚îÄ Languages: 140+ (including Thai)
‚îú‚îÄ‚îÄ Strengths:
‚îÇ   ‚îú‚îÄ‚îÄ Creative content generation
‚îÇ   ‚îú‚îÄ‚îÄ Long-form text
‚îÇ   ‚îú‚îÄ‚îÄ Translation & summarization
‚îÇ   ‚îú‚îÄ‚îÄ Image understanding
‚îÇ   ‚îî‚îÄ‚îÄ Reasoning over documents
‚îî‚îÄ‚îÄ Use Cases in AI Director:
    ‚îú‚îÄ‚îÄ Understand marketing briefs
    ‚îú‚îÄ‚îÄ Generate image prompts
    ‚îú‚îÄ‚îÄ Write video scripts
    ‚îú‚îÄ‚îÄ Analyze video transcripts
    ‚îî‚îÄ‚îÄ Select highlight moments
```

### When to Use T5Gemma 2?

Use T5Gemma 2 for **THINKING** tasks:

| Task Type | Example | Why T5Gemma 2? |
|-----------|---------|----------------|
| **Strategy** | "‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏Ñ‡∏°‡πÄ‡∏õ‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Gen Z" | Needs understanding + creativity |
| **Content Creation** | ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô copy ‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤ | Long-form generation |
| **Prompt Engineering** | ‡∏™‡∏£‡πâ‡∏≤‡∏á SDXL prompt | Creative + technical knowledge |
| **Analysis** | ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå brand guidelines | Long context understanding |
| **Multimodal** | ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û | Image + text input |
| **Highlight Selection** | ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å best moments ‡∏à‡∏≤‡∏Å transcript | Reasoning over long text |

### T5Gemma 2 Example

```python
from transformers import AutoProcessor, AutoModelForSeq2SeqLM

# Load T5Gemma 2 (1B)
processor = AutoProcessor.from_pretrained("google/t5gemma-2-1b-1b")
model = AutoModelForSeq2SeqLM.from_pretrained("google/t5gemma-2-1b-1b")

# Example 1: Generate marketing prompt
brief = """
Brand: CoffeeLab
Product: Cold Brew Premium
Audience: ‡∏Ñ‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏® 25-35 ‡∏õ‡∏µ
Mood: Premium, modern, minimal
"""

prompt = f"""You are a creative director. Generate a detailed photography prompt for this product.

{brief}

Photography prompt:"""

inputs = processor(text=prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=200)
result = processor.decode(outputs[0], skip_special_tokens=True)

print(result)
# Output: "A sleek glass bottle of cold brew coffee on white marble surface, 
#          soft morning light from window, minimalist composition, 
#          professional product photography, shot on Canon EOS R5, f/2.8..."
```

### T5Gemma 2 Strengths

‚úÖ **Long Context:** 128K tokens = ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏¢‡∏≤‡∏ß‡πÜ ‡πÑ‡∏î‡πâ  
‚úÖ **Multimodal:** ‡∏£‡∏π‡∏õ + ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô  
‚úÖ **Multilingual:** ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏î‡∏µ‡∏°‡∏≤‡∏Å  
‚úÖ **Creative:** ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô copy, script ‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏µ‡∏¢‡∏ô  
‚úÖ **Reasoning:** ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÑ‡∏î‡πâ

### T5Gemma 2 Limitations

‚ùå **Not for Tool Calling:** ‡πÑ‡∏°‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç JSON/structured output  
‚ùå **Slower:** Encoder-decoder architecture ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ decoder-only  
‚ùå **Larger:** 1B-4B parameters ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ resources ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤  
‚ùå **Overkill for Simple Tasks:** ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÜ

---

## ‚ö° Part 3: FunctionGemma (The Executor)

### What is FunctionGemma?

FunctionGemma ‡∏Ñ‡∏∑‡∏≠ **specialized 270M model** ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö **function/tool calling**

### Key Characteristics

```python
FunctionGemma Profile:
‚îú‚îÄ‚îÄ Architecture: Decoder-only (Gemma-based)
‚îú‚îÄ‚îÄ Parameters: 270M (tiny!)
‚îú‚îÄ‚îÄ Context Window: 8K tokens
‚îú‚îÄ‚îÄ Modality: Text only
‚îú‚îÄ‚îÄ Specialty: Function/Tool calling
‚îú‚îÄ‚îÄ Strengths:
‚îÇ   ‚îú‚îÄ‚îÄ Parse natural language ‚Üí JSON
‚îÇ   ‚îú‚îÄ‚îÄ Extremely fast inference
‚îÇ   ‚îú‚îÄ‚îÄ Low resource requirements
‚îÇ   ‚îú‚îÄ‚îÄ High accuracy after fine-tuning
‚îÇ   ‚îî‚îÄ‚îÄ Runs on CPU/edge devices
‚îî‚îÄ‚îÄ Use Cases in AI Director:
    ‚îú‚îÄ‚îÄ Parse user instructions
    ‚îú‚îÄ‚îÄ Call image_gen(prompt, style)
    ‚îú‚îÄ‚îÄ Call voice_gen(text, voice)
    ‚îú‚îÄ‚îÄ Call video_compose(clips, audio)
    ‚îî‚îÄ‚îÄ Call smart_cut(video, mode)
```

### When to Use FunctionGemma?

Use FunctionGemma for **EXECUTION** tasks:

| Task Type | Example | Why FunctionGemma? |
|-----------|---------|-------------------|
| **Tool Calling** | `image_gen("coffee cup")` | Specialized for this |
| **Parsing** | Extract parameters from text | Fast + accurate |
| **Orchestration** | Call multiple tools in sequence | Low latency |
| **Structured Output** | Generate JSON/API calls | Trained specifically |
| **Multi-step** | Chain tool calls | Reliable execution |

### FunctionGemma Example

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load FunctionGemma
tokenizer = AutoTokenizer.from_pretrained("google/functiongemma-270m-it")
model = AutoModelForCausalLM.from_pretrained("google/functiongemma-270m-it")

# Define tools
def image_gen(prompt: str, style: str = "realistic") -> str:
    """Generate an image from text prompt"""
    return f"image_{style}.png"

def voice_gen(text: str, voice: str = "th-TH-NiwatNeural") -> str:
    """Generate voiceover from text"""
    return f"voice.mp3"

# User instruction
instruction = "‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ‡∏Å‡∏≤‡πÅ‡∏ü‡πÅ‡∏ö‡∏ö minimal ‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏û‡∏≤‡∏Å‡∏¢‡πå‡∏ß‡πà‡∏≤ 'Cold Brew Premium'"

# Apply chat template with tools
messages = [{"role": "user", "content": instruction}]
formatted = tokenizer.apply_chat_template(
    messages,
    tools=[image_gen, voice_gen],
    add_generation_prompt=True,
    tokenize=False
)

# Generate
inputs = tokenizer(formatted, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)
result = tokenizer.decode(outputs[0])

print(result)
# Output: <start_function_call>call:image_gen{prompt:<escape>‡∏Å‡∏≤‡πÅ‡∏ü‡πÅ‡∏ö‡∏ö minimal<escape>,style:<escape>minimal<escape>}<end_function_call>
#         <start_function_call>call:voice_gen{text:<escape>Cold Brew Premium<escape>,voice:<escape>th-TH-NiwatNeural<escape>}<end_function_call>
```

### FunctionGemma Strengths

‚úÖ **Tiny but Mighty:** 270M params ‡πÅ‡∏ï‡πà‡πÅ‡∏°‡πà‡∏ô‡∏°‡∏≤‡∏Å  
‚úÖ **Lightning Fast:** < 1 second per call  
‚úÖ **CPU-Friendly:** ‡∏£‡∏±‡∏ô‡∏ö‡∏ô Codespace ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢  
‚úÖ **Fine-tunable:** ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏á‡πà‡∏≤‡∏¢ accuracy ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏¢‡∏≠‡∏∞  
‚úÖ **Structured Output:** JSON perfect ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á

### FunctionGemma Limitations

‚ùå **Not Creative:** ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô content  
‚ùå **Text Only:** ‡πÑ‡∏°‡πà‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û  
‚ùå **Short Context:** 8K tokens only  
‚ùå **Needs Fine-tuning:** Out-of-box accuracy ~60%

---

## üîÑ Part 4: How They Work Together

### The Dual-Model Workflow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    COMPLETE WORKFLOW EXAMPLE                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ  INPUT: "‡∏™‡∏£‡πâ‡∏≤‡∏á video ad 15 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CoffeeLab Cold Brew"      ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  STEP 1: T5GEMMA 2 (THINKER) - Strategy & Planning                 ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ        ‚îÇ
‚îÇ  Input: User brief + Brand knowledge (from RAG)                     ‚îÇ
‚îÇ  Process:                                                            ‚îÇ
‚îÇ    ‚Ä¢ Analyze brief                                                   ‚îÇ
‚îÇ    ‚Ä¢ Recall brand guidelines                                         ‚îÇ
‚îÇ    ‚Ä¢ Generate creative strategy                                      ‚îÇ
‚îÇ  Output:                                                             ‚îÇ
‚îÇ    {                                                                 ‚îÇ
‚îÇ      "concept": "Premium morning ritual",                            ‚îÇ
‚îÇ      "image_prompt": "Sleek cold brew bottle on marble...",         ‚îÇ
‚îÇ      "voice_script": "‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏Å‡∏±‡∏ö CoffeeLab...",        ‚îÇ
‚îÇ      "duration": 15,                                                 ‚îÇ
‚îÇ      "music_mood": "calm, sophisticated"                             ‚îÇ
‚îÇ    }                                                                 ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  STEP 2: FUNCTIONGEMMA (EXECUTOR) - Tool Orchestration              ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÇ
‚îÇ  Input: T5Gemma 2's output (natural language)                       ‚îÇ
‚îÇ  Process:                                                            ‚îÇ
‚îÇ    ‚Ä¢ Parse the plan                                                  ‚îÇ
‚îÇ    ‚Ä¢ Map to tool calls                                               ‚îÇ
‚îÇ    ‚Ä¢ Execute in correct order                                        ‚îÇ
‚îÇ  Output:                                                             ‚îÇ
‚îÇ    [                                                                 ‚îÇ
‚îÇ      {                                                               ‚îÇ
‚îÇ        "tool": "image_gen",                                          ‚îÇ
‚îÇ        "params": {                                                   ‚îÇ
‚îÇ          "prompt": "Sleek cold brew bottle on marble...",           ‚îÇ
‚îÇ          "style": "minimal",                                         ‚îÇ
‚îÇ          "aspect_ratio": "9:16"                                      ‚îÇ
‚îÇ        }                                                             ‚îÇ
‚îÇ      },                                                              ‚îÇ
‚îÇ      {                                                               ‚îÇ
‚îÇ        "tool": "voice_gen",                                          ‚îÇ
‚îÇ        "params": {                                                   ‚îÇ
‚îÇ          "text": "‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏Å‡∏±‡∏ö CoffeeLab...",            ‚îÇ
‚îÇ          "voice": "th-TH-NiwatNeural",                               ‚îÇ
‚îÇ          "rate": "+0%"                                               ‚îÇ
‚îÇ        }                                                             ‚îÇ
‚îÇ      },                                                              ‚îÇ
‚îÇ      {                                                               ‚îÇ
‚îÇ        "tool": "video_compose",                                      ‚îÇ
‚îÇ        "params": {                                                   ‚îÇ
‚îÇ          "images": ["coffee_01.png"],                                ‚îÇ
‚îÇ          "audio": "voiceover.mp3",                                   ‚îÇ
‚îÇ          "duration": 15,                                             ‚îÇ
‚îÇ          "transitions": ["fade_in", "fade_out"]                      ‚îÇ
‚îÇ        }                                                             ‚îÇ
‚îÇ      }                                                               ‚îÇ
‚îÇ    ]                                                                 ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  STEP 3: TOOL EXECUTION - Actual Production                         ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                      ‚îÇ
‚îÇ  Each tool executes and returns results                             ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  OUTPUT: final_video.mp4 ‚úÖ                                         ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Communication Protocol

```python
class DualModelAgent:
    """AI Director with dual-model architecture"""
    
    def __init__(self):
        # Load models
        self.thinker = T5Gemma2Model()
        self.executor = FunctionGemmaModel()
        
        # Load tools
        self.tools = {
            "image_gen": ImageGenerator(),
            "voice_gen": VoiceGenerator(),
            "video_compose": VideoComposer(),
            "smart_cut": SmartCutTool()
        }
    
    def process_brief(self, brief: str) -> dict:
        """Main workflow"""
        
        # Step 1: THINKER generates strategy
        strategy = self.thinker.generate_strategy(brief)
        
        # Step 2: EXECUTOR parses and calls tools
        tool_calls = self.executor.parse_to_tools(strategy)
        
        # Step 3: Execute tools
        results = []
        for call in tool_calls:
            tool = self.tools[call["tool"]]
            result = tool.execute(**call["params"])
            results.append(result)
        
        return {
            "strategy": strategy,
            "tool_calls": tool_calls,
            "outputs": results
        }
```

### Decision Tree: Which Model to Use?

```
Start: New Task
    ‚îÇ
    ‚îú‚îÄ Is it creative/strategic? ‚îÄ‚îÄ‚îÄ‚îÄYES‚îÄ‚îÄ‚ñ∫ T5Gemma 2 (Thinker)
    ‚îÇ                                       ‚îú‚îÄ Generate strategy
    ‚îÇ                                       ‚îú‚îÄ Write content
    ‚îÇ                                       ‚îú‚îÄ Analyze video/image
    ‚îÇ                                       ‚îî‚îÄ Select highlights
    ‚îÇ
    ‚îú‚îÄ Is it tool calling? ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄYES‚îÄ‚îÄ‚ñ∫ FunctionGemma (Executor)
    ‚îÇ                                       ‚îú‚îÄ Parse instructions
    ‚îÇ                                       ‚îú‚îÄ Call APIs/tools
    ‚îÇ                                       ‚îú‚îÄ Generate JSON
    ‚îÇ                                       ‚îî‚îÄ Orchestrate workflow
    ‚îÇ
    ‚îî‚îÄ Is it both? ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄYES‚îÄ‚îÄ‚ñ∫ Use BOTH in sequence
                                            1. T5Gemma 2 plans
                                            2. FunctionGemma executes
```

---

## üé¨ Part 5: Smart Cut Integration

### New in v3.4: Video Editing Workflow

Smart Cut feature ‡πÄ‡∏û‡∏¥‡πà‡∏° use case ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö dual-model architecture:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              SMART CUT WORKFLOW (NEW USE CASE)                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ  INPUT: Raw video footage + "‡∏ï‡∏±‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 2 ‡∏ô‡∏≤‡∏ó‡∏µ ‡πÄ‡∏≠‡∏≤‡πÅ‡∏ï‡πà‡πÑ‡∏Æ‡πÑ‡∏•‡∏ó‡πå"      ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  STEP 1: ANALYSIS (Tools)                                           ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                          ‚îÇ
‚îÇ  ‚Ä¢ Whisper transcribes ‚Üí Full transcript with timestamps            ‚îÇ
‚îÇ  ‚Ä¢ FFmpeg detects ‚Üí Silence regions                                 ‚îÇ
‚îÇ  ‚Ä¢ Frame analysis ‚Üí Visual content                                  ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  STEP 2: T5GEMMA 2 (THINKER) - Editing Decisions                   ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ            ‚îÇ
‚îÇ  Input: Transcript + Silence data + User requirements               ‚îÇ
‚îÇ  Process:                                                            ‚îÇ
‚îÇ    ‚Ä¢ Read entire transcript                                          ‚îÇ
‚îÇ    ‚Ä¢ Identify key moments                                            ‚îÇ
‚îÇ    ‚Ä¢ Decide what to keep/remove                                      ‚îÇ
‚îÇ    ‚Ä¢ Plan narrative flow                                             ‚îÇ
‚îÇ  Output:                                                             ‚îÇ
‚îÇ    {                                                                 ‚îÇ
‚îÇ      "keep_segments": [                                              ‚îÇ
‚îÇ        {"start": 15.0, "end": 45.0, "reason": "strong intro"},     ‚îÇ
‚îÇ        {"start": 120.5, "end": 155.0, "reason": "key demo"},       ‚îÇ
‚îÇ        {"start": 480.0, "end": 510.0, "reason": "CTA"}             ‚îÇ
‚îÇ      ],                                                              ‚îÇ
‚îÇ      "remove_reasons": {                                             ‚îÇ
‚îÇ        "silence": 45.2,                                              ‚îÇ
‚îÇ        "tangent": 150.0,                                             ‚îÇ
‚îÇ        "repetition": 80.0                                            ‚îÇ
‚îÇ      },                                                              ‚îÇ
‚îÇ      "suggested_order": "chronological",                             ‚îÇ
‚îÇ      "target_duration": 120                                          ‚îÇ
‚îÇ    }                                                                 ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  STEP 3: FUNCTIONGEMMA (EXECUTOR) - Video Operations                ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ             ‚îÇ
‚îÇ  Input: T5Gemma 2's edit decision                                   ‚îÇ
‚îÇ  Output: FFmpeg commands                                             ‚îÇ
‚îÇ    [                                                                 ‚îÇ
‚îÇ      {"tool": "cut_segment", "params": {"start": 15, "end": 45}},  ‚îÇ
‚îÇ      {"tool": "cut_segment", "params": {"start": 120.5, "end": 155}},‚îÇ
‚îÇ      {"tool": "concatenate", "params": {"segments": [...]}},        ‚îÇ
‚îÇ      {"tool": "add_transitions", "params": {"type": "fade"}}        ‚îÇ
‚îÇ    ]                                                                 ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  OUTPUT: Edited video (2 minutes) ‚úÖ                                ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Why Dual-Model for Video Editing?

| Aspect | Why Not Single Model? | How Dual-Model Helps |
|--------|----------------------|---------------------|
| **Understanding Context** | Need to read long transcripts | T5Gemma 2: 128K tokens |
| **Creative Judgment** | What's a "highlight"? Subjective | T5Gemma 2: Good at reasoning |
| **Precise Execution** | Must cut at exact timestamps | FunctionGemma: Structured output |
| **Speed** | Large model = slow | FunctionGemma: < 1s for parsing |

---

## üéì Part 6: Hands-On with Copilot

### Exercise 1: Understanding Model Differences

**Copilot Prompt:**

```
I'm learning about dual-model architecture for AI Director v3.4.
Help me understand:

1. Compare T5Gemma 2 and FunctionGemma in terms of:
   - Architecture (encoder-decoder vs decoder-only)
   - Parameter count
   - Strengths and weaknesses
   - Typical use cases

2. Show me a Python code example demonstrating:
   - Loading T5Gemma 2 for creative content generation
   - Loading FunctionGemma for tool calling
   - How they complement each other

3. For these tasks, which model should I use?
   - Writing a marketing copy
   - Calling image_gen() API
   - Analyzing a video transcript
   - Parsing JSON from natural language
```

### Exercise 2: Workflow Design

**Copilot Prompt:**

```
Design a complete workflow for AI Director v3.4 that:

1. Takes input: "‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤ Instagram Reel ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡πÅ‡∏ü"

2. Shows step-by-step:
   - What T5Gemma 2 does (strategy, prompts, script)
   - What FunctionGemma does (parse to tool calls)
   - What tools get called (image_gen, voice_gen, video_compose)

3. Include error handling:
   - What if T5Gemma 2 output is unclear?
   - What if tool call fails?

4. Show the complete Python class structure
```

### Exercise 3: Smart Cut Integration

**Copilot Prompt:**

```
Explain how dual-model architecture handles video editing with Smart Cut:

1. Input: Raw interview footage (30 mins) + "‡∏ï‡∏±‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 3 ‡∏ô‡∏≤‡∏ó‡∏µ"

2. Show the workflow:
   - Analysis phase (Whisper, FFmpeg)
   - T5Gemma 2's role in selecting highlights
   - FunctionGemma's role in executing cuts

3. Why is this better than using a single large model?

4. Provide example code for the highlight selection logic
```

---

## ‚úÖ Completion Criteria

Check ‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠‡∏Å‡πà‡∏≠‡∏ô‡πÑ‡∏õ Module 2:

### Understanding (‡∏ó‡∏§‡∏©‡∏é‡∏µ)
- [ ] ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡πÉ‡∏ä‡πâ dual-model ‡πÅ‡∏ó‡∏ô single large model
- [ ] ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á T5Gemma 2 ‡πÅ‡∏•‡∏∞ FunctionGemma ‡πÑ‡∏î‡πâ
- [ ] ‡∏ö‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤ task ‡πÑ‡∏´‡∏ô‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ model ‡πÑ‡∏´‡∏ô
- [ ] ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à communication protocol ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 2 models

### Practical (‡∏•‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏≥)
- [ ] ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á GitHub Codespaces ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
- [ ] Load T5Gemma 2 ‡πÅ‡∏•‡∏∞ generate text ‡πÑ‡∏î‡πâ
- [ ] Load FunctionGemma ‡πÅ‡∏•‡∏∞ parse tool calls ‡πÑ‡∏î‡πâ
- [ ] ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö workflow ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö use case ‡∏´‡∏ô‡∏∂‡πà‡∏á

### Smart Cut (‡πÉ‡∏´‡∏°‡πà)
- [ ] ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤ Smart Cut ‡πÉ‡∏ä‡πâ dual-model architecture ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
- [ ] ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏° T5Gemma 2 ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å highlights
- [ ] ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏Ç‡∏≠‡∏á FunctionGemma ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° FFmpeg

---

## üìù Knowledge Check

### Quiz Questions

1. **‡∏ó‡∏≥‡πÑ‡∏° AI Director ‡πÉ‡∏ä‡πâ 2 models ‡πÅ‡∏ó‡∏ô 1 large model?**
   - [ ] A. ‡∏ñ‡∏π‡∏Å‡∏Å‡∏ß‡πà‡∏≤
   - [ ] B. ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤
   - [ ] C. ‡πÅ‡∏°‡πà‡∏ô‡∏Å‡∏ß‡πà‡∏≤
   - [ ] D. ‡∏ñ‡∏π‡∏Å‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠ ‚úÖ

2. **T5Gemma 2 ‡πÉ‡∏ä‡πâ architecture ‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô?**
   - [ ] A. Encoder-only
   - [ ] B. Decoder-only
   - [ ] C. Encoder-Decoder ‚úÖ
   - [ ] D. Mixture of Experts

3. **FunctionGemma ‡∏°‡∏µ parameters ‡∏Å‡∏µ‡πà‡∏ï‡∏±‡∏ß?**
   - [ ] A. 1B
   - [ ] B. 270M ‚úÖ
   - [ ] C. 7B
   - [ ] D. 70B

4. **Task ‡πÑ‡∏´‡∏ô‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ T5Gemma 2?**
   - [ ] A. Parse JSON
   - [ ] B. Call API
   - [ ] C. ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô marketing copy ‚úÖ
   - [ ] D. ‡∏ñ‡∏π‡∏Å‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠

5. **Smart Cut ‡πÉ‡∏ä‡πâ T5Gemma 2 ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£?**
   - [ ] A. Transcribe audio
   - [ ] B. Detect silence
   - [ ] C. Select highlights ‚úÖ
   - [ ] D. Cut video

### Answer Key
1. D, 2. C, 3. B, 4. C, 5. C

---

## üíª COMPLETE IMPLEMENTATION CODE

> **‡∏û‡∏£‡πâ‡∏≠‡∏° copy-paste ‡πÅ‡∏•‡∏∞ run ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢!**

---

## üöÄ AUTOMATED SETUP (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÄ‡∏≠‡∏á!)

> **‡πÉ‡∏ä‡πâ Dev Container ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Codespace ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥**

---

## üìã ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÄ‡∏û‡∏µ‡∏¢‡∏á 4 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß 10 ‡∏ô‡∏≤‡∏ó‡∏µ)

### ‚ö†Ô∏è ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ú‡∏° (GitHub Copilot) ‡∏ó‡∏≥‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ:
- ‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á GitHub repository ‡πÉ‡∏´‡πâ
- ‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô GitHub ‡πÉ‡∏´‡πâ
- ‚ùå ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° "Create Codespace" ‡πÉ‡∏´‡πâ
- ‚ùå Run ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÉ‡∏ô GitHub ‡∏´‡∏£‡∏∑‡∏≠ Codespace ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì

### ‚úÖ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÄ‡∏≠‡∏á (‡πÅ‡∏ï‡πà‡∏á‡πà‡∏≤‡∏¢‡∏°‡∏≤‡∏Å):

---

### üéØ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á GitHub Repository (2 ‡∏ô‡∏≤‡∏ó‡∏µ)

1. **‡πÄ‡∏õ‡∏¥‡∏î browser** ‡πÑ‡∏õ‡∏ó‡∏µ‡πà https://github.com
2. **‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö** ‡∏î‡πâ‡∏ß‡∏¢ GitHub account
3. **‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏°‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß** "New" ‡∏´‡∏£‡∏∑‡∏≠ "New repository" (‡∏°‡∏∏‡∏°‡∏Ç‡∏ß‡∏≤‡∏ö‡∏ô)
4. **‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**
   - Repository name: `ai-director-course`
   - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Public
   - ‚úÖ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å "Add a README file"
5. **‡∏Ñ‡∏•‡∏¥‡∏Å** "Create repository" (‡∏õ‡∏∏‡πà‡∏°‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á)

‚úÖ **‡πÄ‡∏™‡∏£‡πá‡∏à‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1**

---

### üéØ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà 1 - devcontainer.json (3 ‡∏ô‡∏≤‡∏ó‡∏µ)

1. **‡πÉ‡∏ô repository ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á** ‡∏Ñ‡∏•‡∏¥‡∏Å "Add file" ‚Üí "Create new file"
2. **‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå:** `.devcontainer/devcontainer.json` 
   - ‚ö†Ô∏è **‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ `/` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á folder** `.devcontainer`
3. **Copy code ‡∏ô‡∏µ‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î** ‡πÑ‡∏õ‡∏ß‡∏≤‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå:

```json
{
  "name": "AI Director Course",
  "image": "mcr.microsoft.com/devcontainers/python:3.11",
  
  "features": {
    "ghcr.io/devcontainers/features/python:1": {
      "version": "3.11"
    }
  },
  
  "postCreateCommand": "bash .devcontainer/setup.sh",
  
  "customizations": {
    "vscode": {
      "extensions": [
        "ms-python.python",
        "ms-python.vscode-pylance",
        "GitHub.copilot",
        "GitHub.copilot-chat"
      ],
      "settings": {
        "python.defaultInterpreterPath": "/usr/local/bin/python",
        "python.linting.enabled": true,
        "python.linting.pylintEnabled": true,
        "python.formatting.provider": "black"
      }
    }
  },
  
  "forwardPorts": [8000, 8501],
  
  "remoteUser": "vscode"
}
```

4. **Scroll ‡∏•‡∏á‡∏•‡πà‡∏≤‡∏á** ‚Üí ‡∏Ñ‡∏•‡∏¥‡∏Å "Commit changes" (‡∏õ‡∏∏‡πà‡∏°‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß)
5. **‡∏Ñ‡∏•‡∏¥‡∏Å** "Commit changes" ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏õ‡πä‡∏≠‡∏õ‡∏≠‡∏±‡∏û

‚úÖ **‡πÄ‡∏™‡∏£‡πá‡∏à‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2** - ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå `.devcontainer/devcontainer.json` ‡πÉ‡∏ô repository

---

### üéØ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà 2 - setup.sh (3 ‡∏ô‡∏≤‡∏ó‡∏µ)

1. **‡∏Ñ‡∏•‡∏¥‡∏Å** "Add file" ‚Üí "Create new file" ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
2. **‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå:** `.devcontainer/setup.sh`
3. **Copy script ‡∏ô‡∏µ‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î** ‡πÑ‡∏õ‡∏ß‡∏≤‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå:

```bash
#!/bin/bash
set -e

echo "üöÄ Setting up AI Director Course environment..."

# Upgrade pip
pip install --upgrade pip

# Install core dependencies
echo "üì¶ Installing dependencies..."
pip install transformers==4.45.0
pip install torch==2.5.0
pip install accelerate==0.34.0
pip install pillow==10.4.0
pip install requests

# Install additional tools
pip install ipython jupyter

# Create project structure
echo "üìÅ Creating project structure..."
mkdir -p module1
mkdir -p module2
mkdir -p module3
mkdir -p module4
mkdir -p module5
mkdir -p module6
mkdir -p module7
mkdir -p module8

# Create .gitignore
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/
*.egg-info/

# Models
*.bin
*.safetensors
models/
.cache/

# OS
.DS_Store
Thumbs.db

# IDE
.vscode/
.idea/

# Jupyter
.ipynb_checkpoints/
EOF

echo "‚úÖ Setup complete! You can start coding now."
echo ""
echo "üìç Current directory: $(pwd)"
echo "üìç Python version: $(python --version)"
echo "üìç Pip packages installed:"
pip list | grep -E "(transformers|torch|accelerate)"

echo ""
echo "üéâ Ready to start Module 1!"
```

4. **Scroll ‡∏•‡∏á‡∏•‡πà‡∏≤‡∏á** ‚Üí ‡∏Ñ‡∏•‡∏¥‡∏Å "Commit changes"
5. **‡∏Ñ‡∏•‡∏¥‡∏Å** "Commit changes" ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á

‚úÖ **‡πÄ‡∏™‡∏£‡πá‡∏à‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3** - ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå `.devcontainer/setup.sh` ‡πÉ‡∏ô repository

---

### üéØ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡πÄ‡∏õ‡∏¥‡∏î Codespace (2 ‡∏ô‡∏≤‡∏ó‡∏µ)

1. **‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å repository** (‡∏Ñ‡∏•‡∏¥‡∏Å "ai-director-course" ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô)
2. **‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏°‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß** `<> Code`
3. **‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ó‡πá‡∏ö** "Codespaces"
4. **‡∏Ñ‡∏•‡∏¥‡∏Å** "Create codespace on main" (‡∏õ‡∏∏‡πà‡∏°‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß)
5. **‡∏£‡∏≠ 3-5 ‡∏ô‡∏≤‡∏ó‡∏µ** ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà Codespace ‡∏Å‡∏≥‡∏•‡∏±‡∏á:
   - ‡∏™‡∏£‡πâ‡∏≤‡∏á container
   - ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Python
   - ‡∏£‡∏±‡∏ô setup.sh (‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)
   - ‡∏™‡∏£‡πâ‡∏≤‡∏á folders module1-8
   - ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á VS Code extensions

**‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á Terminal ‡πÅ‡∏™‡∏î‡∏á:**
```
üöÄ Setting up AI Director Course environment...
üì¶ Installing dependencies...
üìÅ Creating project structure...
‚úÖ Setup complete! You can start coding now.

üìç Current directory: /workspaces/ai-director-course
üìç Python version: Python 3.11.x
üìç Pip packages installed:
transformers    4.45.0
torch           2.5.0
accelerate      0.34.0

üéâ Ready to start Module 1!
```

‚úÖ **‡πÄ‡∏™‡∏£‡πá‡∏à‡∏ó‡∏±‡πâ‡∏á 4 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô!** VS Code ‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ô browser ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

---

## üéä ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß!

### ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°:

‡πÄ‡∏õ‡∏¥‡∏î Terminal ‡πÉ‡∏ô VS Code (‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß) ‡πÅ‡∏•‡∏∞‡∏•‡∏≠‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå:

```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ packages ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß
python -c "import transformers; import torch; print('‚úÖ ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°!')"
```

‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô: `‚úÖ ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°!`

```bash
# ‡∏î‡∏π folders ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß
ls
```

‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô: `module1  module2  module3  module4  module5  module6  module7  module8`

---

## üìä ‡∏™‡∏£‡∏∏‡∏õ: ‡πÉ‡∏Ñ‡∏£‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£?

| ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô | ‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏≥ | ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ |
|---------|-------|-----------------|
| ‡∏™‡∏£‡πâ‡∏≤‡∏á repository | ‚úÖ ‡∏Ñ‡∏•‡∏¥‡∏Å 5 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á | - |
| ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå config 2 ‡πÑ‡∏ü‡∏•‡πå | ‚úÖ Copy-paste 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á | - |
| ‡πÄ‡∏õ‡∏¥‡∏î Codespace | ‚úÖ ‡∏Ñ‡∏•‡∏¥‡∏Å 1 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á | - |
| ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Python | - | ‚úÖ ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ |
| ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î | - | ‚úÖ ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ |
| ‡∏™‡∏£‡πâ‡∏≤‡∏á folders | - | ‚úÖ ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ |
| ‡∏™‡∏£‡πâ‡∏≤‡∏á .gitignore | - | ‚úÖ ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ |
| ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á VS Code extensions | - | ‚úÖ ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ |
| **‡∏£‡∏ß‡∏°** | **‡∏Ñ‡∏•‡∏¥‡∏Å 6 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á + Copy-paste 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á** | **‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏≠‡∏á** |

---

## üîÑ ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ (‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏°‡∏≤‡∏Å!)

‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì‡∏õ‡∏¥‡∏î Codespace ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏´‡∏°‡πà:

1. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà https://github.com/codespaces
2. ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ó‡∏µ‡πà Codespace `ai-director-course`
3. **‡πÄ‡∏™‡∏£‡πá‡∏à!** ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á setup ‡πÉ‡∏´‡∏°‡πà)

---

## üí° FAQ

### Q: ‡∏ú‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏´‡∏°?
**A:** ‡πÑ‡∏°‡πà‡πÄ‡∏•‡∏¢! ‡πÅ‡∏Ñ‡πà copy-paste code 2 ‡πÑ‡∏ü‡∏•‡πå ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏•‡∏¥‡∏Å "Create Codespace"

### Q: ‡∏ñ‡πâ‡∏≤ setup ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏•‡πà‡∏∞?
**A:** ‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏ó‡∏µ‡πà Terminal ‡∏à‡∏∞‡∏°‡∏µ error message ‡∏ö‡∏≠‡∏Å ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞:
- setup.sh ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå execute ‚Üí ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏£ Codespace ‡∏à‡∏∞‡πÅ‡∏Å‡πâ‡πÉ‡∏´‡πâ
- Network ‡∏ä‡πâ‡∏≤ ‚Üí ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏à‡∏ö ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏Å‡∏ß‡πà‡∏≤ 5 ‡∏ô‡∏≤‡∏ó‡∏µ

### Q: ‡∏ú‡∏°‡∏à‡∏∞‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°‡∏ß‡πà‡∏≤ setup ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß?
**A:** ‡πÄ‡∏´‡πá‡∏ô `üéâ Ready to start Module 1!` ‡πÉ‡∏ô Terminal

### Q: ‡∏ú‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç setup.sh ‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°?
**A:** ‡πÑ‡∏î‡πâ! ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß rebuild container:
1. ‡∏Å‡∏î `F1` 
2. ‡∏û‡∏¥‡∏°‡∏û‡πå "Rebuild Container"
3. Enter

---

## üéØ Next: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î Module 1

‡πÄ‡∏°‡∏∑‡πà‡∏≠ setup ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏ß‡∏ô **"Part 1: T5Gemma 2 Implementation"** ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î!

---

#### Step 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á GitHub Repository

1. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà https://github.com
2. ‡∏Ñ‡∏•‡∏¥‡∏Å **New repository** (‡∏õ‡∏∏‡πà‡∏°‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß)
3. ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠: `ai-director-course`
4. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å **Public** (‡∏´‡∏£‡∏∑‡∏≠ Private ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
5. ‚úÖ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å "Add a README file"
6. ‡∏Ñ‡∏•‡∏¥‡∏Å **Create repository**

#### Step 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Dev Container Config

**‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç!** ‡∏ó‡∏≥‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏õ‡∏¥‡∏î Codespace

1. ‡πÉ‡∏ô repository ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á ‡∏Ñ‡∏•‡∏¥‡∏Å **Add file** ‚Üí **Create new file**
2. ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå: `.devcontainer/devcontainer.json`
3. ‡∏ß‡∏≤‡∏á code ‡∏ô‡∏µ‡πâ:

```json
{
  "name": "AI Director Course",
  "image": "mcr.microsoft.com/devcontainers/python:3.11",
  
  "features": {
    "ghcr.io/devcontainers/features/python:1": {
      "version": "3.11"
    }
  },
  
  "postCreateCommand": "bash .devcontainer/setup.sh",
  
  "customizations": {
    "vscode": {
      "extensions": [
        "ms-python.python",
        "ms-python.vscode-pylance",
        "GitHub.copilot",
        "GitHub.copilot-chat"
      ],
      "settings": {
        "python.defaultInterpreterPath": "/usr/local/bin/python",
        "python.linting.enabled": true,
        "python.linting.pylintEnabled": true,
        "python.formatting.provider": "black"
      }
    }
  },
  
  "forwardPorts": [8000, 8501],
  
  "remoteUser": "vscode"
}
```

4. ‡∏Ñ‡∏•‡∏¥‡∏Å **Commit changes**

#### Step 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Setup Script

1. ‡∏Ñ‡∏•‡∏¥‡∏Å **Add file** ‚Üí **Create new file** ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
2. ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå: `.devcontainer/setup.sh`
3. ‡∏ß‡∏≤‡∏á script ‡∏ô‡∏µ‡πâ:

```bash
#!/bin/bash
set -e

echo "üöÄ Setting up AI Director Course environment..."

# Upgrade pip
pip install --upgrade pip

# Install core dependencies
echo "üì¶ Installing dependencies..."
pip install transformers==4.45.0
pip install torch==2.5.0
pip install accelerate==0.34.0
pip install pillow==10.4.0
pip install requests

# Install additional tools
pip install ipython jupyter

# Create project structure
echo "üìÅ Creating project structure..."
mkdir -p module1
mkdir -p module2
mkdir -p module3
mkdir -p module4
mkdir -p module5
mkdir -p module6
mkdir -p module7
mkdir -p module8

# Create .gitignore
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/
*.egg-info/

# Models
*.bin
*.safetensors
models/
.cache/

# OS
.DS_Store
Thumbs.db

# IDE
.vscode/
.idea/

# Jupyter
.ipynb_checkpoints/
EOF

echo "‚úÖ Setup complete! You can start coding now."
echo ""
echo "üìç Current directory: $(pwd)"
echo "üìç Python version: $(python --version)"
echo "üìç Pip packages installed:"
pip list | grep -E "(transformers|torch|accelerate)"

echo ""
echo "üéâ Ready to start Module 1!"
```

4. ‡∏Ñ‡∏•‡∏¥‡∏Å **Commit changes**

#### Step 4: ‡πÄ‡∏õ‡∏¥‡∏î Codespace (‡∏à‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)

1. ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á repository
2. ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏°‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß **<> Code**
3. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ó‡πá‡∏ö **Codespaces**
4. ‡∏Ñ‡∏•‡∏¥‡∏Å **Create codespace on main**
5. ‡∏£‡∏≠ 3-5 ‡∏ô‡∏≤‡∏ó‡∏µ (‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)

‚úÖ **‡πÄ‡∏°‡∏∑‡πà‡∏≠ Codespace ‡πÄ‡∏õ‡∏¥‡∏î‡πÅ‡∏•‡πâ‡∏ß ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ!**

---

## üéØ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Setup ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à

‡πÄ‡∏õ‡∏¥‡∏î Terminal ‡πÅ‡∏•‡∏∞‡∏•‡∏≠‡∏á:

```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà directory ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
pwd
# Output: /workspaces/ai-director-course

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Python
python --version
# Output: Python 3.11.x

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö packages
pip list | grep transformers
# Output: transformers    4.45.0

# ‡∏î‡∏π‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á folders
ls -la
# Output: module1/ module2/ ... module8/

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö import
python -c "import transformers; import torch; print('‚úÖ All good!')"
# Output: ‚úÖ All good!
```

‡∏ñ‡πâ‡∏≤‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡πà‡∏≤‡∏ô ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡πÅ‡∏•‡πâ‡∏ß! üéâ

---

## üÜö ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö 2 ‡∏ß‡∏¥‡∏ò‡∏µ

| | Manual Setup | Dev Container (Automated) |
|---|---|---|
| **‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies** | ‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÄ‡∏≠‡∏á | ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ 100% |
| **‡∏™‡∏£‡πâ‡∏≤‡∏á folders** | ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏á | ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ |
| **Install extensions** | ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏≠‡∏á | ‡∏°‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß |
| **‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ** | 5-10 ‡∏ô‡∏≤‡∏ó‡∏µ | 3-5 ‡∏ô‡∏≤‡∏ó‡∏µ (‡∏£‡∏≠ setup) |
| **‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ** | ‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á | ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ |

**‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:** ‡πÉ‡∏ä‡πâ Dev Container ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î Codespace ‡πÉ‡∏´‡∏°‡πà ‡∏à‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÄ‡∏™‡∏°‡∏≠!

---

## üí° Tips ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç dependencies

‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏ü‡∏•‡πå `.devcontainer/setup.sh` ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° packages:

```bash
# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡πÉ‡∏ô setup.sh
pip install package_name
```

‡πÅ‡∏•‡πâ‡∏ß rebuild container:
1. ‡∏Å‡∏î `F1` ‡∏´‡∏£‡∏∑‡∏≠ `Ctrl+Shift+P`
2. ‡∏û‡∏¥‡∏°‡∏û‡πå: "Codespaces: Rebuild Container"
3. Enter

### ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏° VS Code Extensions

‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏ü‡∏•‡πå `.devcontainer/devcontainer.json`:

```json
"extensions": [
  "ms-python.python",
  "GitHub.copilot",
  "extension-id-here"  // ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
]
```

---

## üöÄ GITHUB CODESPACES SETUP (‡πÅ‡∏ö‡∏ö Manual - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ Dev Container)

> **‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:** ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ Dev Container ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô ‡∏Ç‡πâ‡∏≤‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢

### Step 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á GitHub Repository (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥)

1. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà https://github.com
2. ‡∏Ñ‡∏•‡∏¥‡∏Å **New repository** (‡∏õ‡∏∏‡πà‡∏°‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß)
3. ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠: `ai-director-course`
4. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å **Public** (‡∏´‡∏£‡∏∑‡∏≠ Private ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
5. ‚úÖ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å "Add a README file"
6. ‡∏Ñ‡∏•‡∏¥‡∏Å **Create repository**

### Step 2: ‡πÄ‡∏õ‡∏¥‡∏î Codespaces

1. ‡πÉ‡∏ô repository ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏°‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß **<> Code**
2. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ó‡πá‡∏ö **Codespaces**
3. ‡∏Ñ‡∏•‡∏¥‡∏Å **Create codespace on main**
4. ‡∏£‡∏≠ 1-2 ‡∏ô‡∏≤‡∏ó‡∏µ (Codespaces ‡∏Å‡∏≥‡∏•‡∏±‡∏á setup)

‚úÖ **‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î VS Code ‡πÉ‡∏ô browser ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥**

### Step 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Codespace ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

‡πÄ‡∏õ‡∏¥‡∏î Terminal ‡πÉ‡∏ô VS Code:
- ‡∏Ñ‡∏•‡∏¥‡∏Å **Terminal** ‚Üí **New Terminal** (‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô)
- ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏î `` Ctrl + ` `` (backtick)

‡∏•‡∏≠‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå:
```bash
python --version
# ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô: Python 3.x.x

pwd
# ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô: /workspaces/ai-director-course
```

### Step 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ

```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á folders ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Module 1
mkdir -p module1
cd module1

# ‡∏™‡∏£‡πâ‡∏≤‡∏á virtual environment (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
python -m venv venv
source venv/bin/activate  # ‡πÉ‡∏ô Codespaces ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ô‡∏µ‡πâ

# ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ PowerShell (‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Codespaces)
# venv\Scripts\activate
```

### Step 5: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies

```bash
# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
pip install --upgrade pip

# Core dependencies
pip install transformers==4.45.0
pip install torch==2.5.0
pip install accelerate==0.34.0
pip install pillow==10.4.0
pip install requests

# Save dependencies
pip freeze > requirements.txt
```

‚è≥ **‡∏£‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 2-3 ‡∏ô‡∏≤‡∏ó‡∏µ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á**

### Step 6: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à

```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö packages
pip list | grep transformers
pip list | grep torch

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö import
python -c "import transformers; print(transformers.__version__)"
python -c "import torch; print(torch.__version__)"
```

‡∏ñ‡πâ‡∏≤‡πÄ‡∏´‡πá‡∏ô version numbers ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß! ‚úÖ

---

## üìÅ ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á

```
ai-director-course/
‚îú‚îÄ‚îÄ README.md                    (‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
‚îú‚îÄ‚îÄ module1/                     (‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà)
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt         (‡∏à‡∏≤‡∏Å pip freeze)
‚îÇ   ‚îú‚îÄ‚îÄ t5gemma_thinker.py      (‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ)
‚îÇ   ‚îú‚îÄ‚îÄ functiongemma_executor.py
‚îÇ   ‚îú‚îÄ‚îÄ ai_director_agent.py
‚îÇ   ‚îî‚îÄ‚îÄ test_module1.py
‚îî‚îÄ‚îÄ .gitignore                   (‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)
```

### ‡∏™‡∏£‡πâ‡∏≤‡∏á .gitignore (‡πÑ‡∏°‡πà commit files ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)

```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå .gitignore ‡∏ó‡∏µ‡πà root
cd /workspaces/ai-director-course
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/
*.egg-info/

# Models (‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà)
*.bin
*.safetensors
models/

# OS
.DS_Store
Thumbs.db

# IDE
.vscode/
.idea/
EOF
```

---

## üìù ‡∏ß‡∏¥‡∏ò‡∏µ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô Codespaces

### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡πÉ‡∏ä‡πâ VS Code UI (‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)

1. ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏Ç‡∏ß‡∏≤‡∏ó‡∏µ‡πà folder `module1` ‡πÉ‡∏ô Explorer (‡∏ã‡πâ‡∏≤‡∏¢‡∏°‡∏∑‡∏≠)
2. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å **New File**
3. ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠: `t5gemma_thinker.py`
4. Copy code ‡∏à‡∏≤‡∏Å‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÑ‡∏õ‡∏ß‡∏≤‡∏á
5. ‡∏Å‡∏î `Ctrl + S` ‡πÄ‡∏û‡∏∑‡πà‡∏≠ save

### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡πÉ‡∏ä‡πâ Command Line

```bash
cd /workspaces/ai-director-course/module1

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡πÜ
touch t5gemma_thinker.py
touch functiongemma_executor.py
touch ai_director_agent.py
touch test_module1.py

# ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏õ‡∏¥‡∏î‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏ô VS Code
# ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô Explorer ‡πÅ‡∏•‡πâ‡∏ß copy code ‡πÑ‡∏õ‡∏ß‡∏≤‡∏á
```

---

## ‚ö° Quick Start ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏µ‡∏ö

```bash
# 1. ‡πÄ‡∏õ‡∏¥‡∏î Terminal ‡πÉ‡∏ô Codespaces
# 2. Copy-paste ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏•‡∏¢:

cd /workspaces/ai-director-course
mkdir -p module1
cd module1

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies
pip install transformers torch accelerate pillow requests

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
touch t5gemma_thinker.py
touch functiongemma_executor.py  
touch ai_director_agent.py
touch test_module1.py

echo "‚úÖ Setup complete! ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ copy code ‡πÑ‡∏õ‡∏ß‡∏≤‡∏á‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢"
```

**‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô:** ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Explorer (‡∏ã‡πâ‡∏≤‡∏¢‡∏°‡∏∑‡∏≠) ‚Üí ‡πÄ‡∏õ‡∏¥‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå ‚Üí Copy code ‡∏à‡∏≤‡∏Å‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÑ‡∏õ‡∏ß‡∏≤‡∏á ‚Üí Save

---

## üéì ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡πÅ‡∏•‡πâ‡∏ß!

‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ code ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå üëá

### Part 1: T5Gemma 2 Implementation

```python
# t5gemma_thinker.py
"""
T5Gemma 2 - The Thinker
‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö: Strategy, Creative Content, Analysis
"""

from transformers import AutoProcessor, AutoModelForSeq2SeqLM
import torch
from typing import Dict, List, Optional
from PIL import Image
import requests
from io import BytesIO

class T5GemmaThinker:
    """T5Gemma 2 model wrapper ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI Director"""
    
    def __init__(
        self, 
        model_size: str = "1b-1b",  # "270m-270m", "1b-1b", "4b-4b"
        device: str = "auto"
    ):
        """
        Initialize T5Gemma 2 model
        
        Args:
            model_size: ‡∏Ç‡∏ô‡∏≤‡∏î model (1b-1b ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Codespaces)
            device: "auto", "cpu", "cuda"
        """
        self.model_name = f"google/t5gemma-2-{model_size}"
        print(f"Loading {self.model_name}...")
        
        # Load processor ‡πÅ‡∏•‡∏∞ model
        self.processor = AutoProcessor.from_pretrained(self.model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map=device
        )
        
        print(f"‚úÖ Loaded on {self.model.device}")
    
    def generate_strategy(
        self, 
        brief: str,
        brand_context: Optional[str] = None,
        max_length: int = 500,
        temperature: float = 0.7
    ) -> str:
        """
        Generate marketing strategy ‡∏à‡∏≤‡∏Å brief
        
        Args:
            brief: Marketing brief
            brand_context: Brand guidelines (‡∏à‡∏≤‡∏Å RAG)
            max_length: Maximum output tokens
            temperature: Creativity level (0.0-1.0)
            
        Returns:
            Generated strategy as text
        """
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt
        prompt = f"""You are a creative director for a marketing agency.
Generate a detailed content strategy based on this brief.

BRAND CONTEXT:
{brand_context if brand_context else "No specific brand guidelines."}

BRIEF:
{brief}

Generate:
1. Creative Concept
2. Image Description (for SDXL)
3. Voice Script (Thai)
4. Technical Specs

STRATEGY:"""

        # Generate
        inputs = self.processor(text=prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=temperature,
                do_sample=True,
                top_p=0.9
            )
        
        result = self.processor.decode(outputs[0], skip_special_tokens=True)
        return result
    
    def generate_image_prompt(
        self,
        brief: str,
        style: str = "realistic",
        aspect_ratio: str = "1:1"
    ) -> str:
        """
        Generate detailed SDXL prompt
        
        Args:
            brief: Product/concept description
            style: "realistic", "minimal", "artistic", "cinematic"
            aspect_ratio: "1:1", "16:9", "9:16"
            
        Returns:
            Detailed prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SDXL
        """
        prompt = f"""You are an expert photography director.
Generate a detailed prompt for Stable Diffusion XL based on this brief.

BRIEF: {brief}
STYLE: {style}
ASPECT RATIO: {aspect_ratio}

Generate a prompt that includes:
- Subject details
- Lighting setup
- Camera settings
- Mood and atmosphere
- Technical quality descriptors

SDXL PROMPT:"""

        inputs = self.processor(text=prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.8,
                do_sample=True
            )
        
        result = self.processor.decode(outputs[0], skip_special_tokens=True)
        return result.strip()
    
    def analyze_transcript(
        self,
        transcript: str,
        target_duration: int = 120,
        style: str = "highlight"
    ) -> Dict:
        """
        Analyze video transcript ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å highlights
        ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Smart Cut feature
        
        Args:
            transcript: Full transcript with timestamps
            target_duration: Target video length (seconds)
            style: "highlight", "summary", "tutorial"
            
        Returns:
            Dict with selected segments and reasoning
        """
        prompt = f"""You are a professional video editor.
Analyze this transcript and select the best moments for a {target_duration}-second {style} video.

TRANSCRIPT:
{transcript}

TARGET DURATION: {target_duration} seconds
STYLE: {style}

Identify:
1. Key moments (with timestamps)
2. What to remove (silence, tangents, repetition)
3. Suggested narrative order
4. Reasoning for each decision

Return as structured text with sections: KEEP, REMOVE, ORDER, REASONING

ANALYSIS:"""

        inputs = self.processor(text=prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=400,
                temperature=0.5,  # Lower for more consistent decisions
                do_sample=True
            )
        
        result = self.processor.decode(outputs[0], skip_special_tokens=True)
        
        # Parse result to structured format
        analysis = self._parse_analysis(result)
        return analysis
    
    def analyze_image(
        self,
        image_url: str,
        question: str = "Describe this image in detail"
    ) -> str:
        """
        Multimodal: Analyze image + text
        
        Args:
            image_url: URL or path to image
            question: What to analyze
            
        Returns:
            Image analysis text
        """
        # Load image
        if image_url.startswith("http"):
            response = requests.get(image_url)
            image = Image.open(BytesIO(response.content))
        else:
            image = Image.open(image_url)
        
        # Create prompt with image
        prompt = f"<start_of_image> {question}"
        
        # Process with image
        inputs = self.processor(
            text=prompt,
            images=image,
            return_tensors="pt"
        ).to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=200
            )
        
        result = self.processor.decode(outputs[0], skip_special_tokens=True)
        return result
    
    def _parse_analysis(self, text: str) -> Dict:
        """Parse analysis text to structured format"""
        # Simple parsing - ‡∏à‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏ô Module ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        return {
            "raw_analysis": text,
            "keep_segments": [],  # ‡∏à‡∏∞ implement ‡πÉ‡∏ô Module 6.5
            "remove_segments": [],
            "reasoning": text
        }
    
    def __repr__(self):
        return f"T5GemmaThinker(model={self.model_name}, device={self.model.device})"


# ===== USAGE EXAMPLES =====

if __name__ == "__main__":
    # Initialize model
    thinker = T5GemmaThinker(model_size="1b-1b")
    
    # Example 1: Generate strategy
    print("\n=== Example 1: Generate Strategy ===")
    brief = """
    Brand: CoffeeLab
    Product: Cold Brew Premium
    Target: ‡∏Ñ‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏® 25-35 ‡∏õ‡∏µ
    Goal: Instagram Reel 15 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
    Mood: Premium, modern, minimal
    """
    
    strategy = thinker.generate_strategy(brief)
    print(strategy)
    
    # Example 2: Generate image prompt
    print("\n=== Example 2: Generate Image Prompt ===")
    image_prompt = thinker.generate_image_prompt(
        brief="Cold brew coffee in minimal setting",
        style="minimal",
        aspect_ratio="9:16"
    )
    print(image_prompt)
    
    # Example 3: Analyze transcript (for Smart Cut)
    print("\n=== Example 3: Analyze Transcript ===")
    sample_transcript = """
    [0:00-0:15] ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡πÅ‡∏ü Cold Brew
    [0:15-0:45] Cold Brew ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡πÅ‡∏ü‡∏ó‡∏µ‡πà‡∏ä‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡πâ‡∏≥‡πÄ‡∏¢‡πá‡∏ô ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô 12-24 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
    [0:45-1:00] ...‡πÄ‡∏≠‡πà‡∏≠... ‡∏≠‡∏∑‡∏°... ‡∏•‡∏∑‡∏°‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡∏û‡∏π‡∏î‡∏≠‡∏∞‡πÑ‡∏£
    [1:00-1:30] ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏ó‡∏µ‡πà Cold Brew ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥‡∏à‡∏∞‡∏ô‡∏∏‡πà‡∏°‡∏ô‡∏ß‡∏• ‡πÑ‡∏°‡πà‡∏Ç‡∏°
    [1:30-2:00] CoffeeLab ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏•‡πá‡∏î‡∏Ñ‡∏±‡∏î‡∏û‡∏¥‡πÄ‡∏®‡∏©
    """
    
    analysis = thinker.analyze_transcript(
        transcript=sample_transcript,
        target_duration=60,
        style="highlight"
    )
    print(analysis)
```

### Part 2: FunctionGemma Implementation

```python
# functiongemma_executor.py
"""
FunctionGemma - The Executor
‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö: Tool Calling, Orchestration, JSON Generation
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import json
import re
from typing import Dict, List, Callable, Any, Optional

class FunctionGemmaExecutor:
    """FunctionGemma model wrapper ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI Director"""
    
    def __init__(
        self,
        model_name: str = "google/functiongemma-270m-it",
        device: str = "auto"
    ):
        """
        Initialize FunctionGemma model
        
        Args:
            model_name: HuggingFace model ID
            device: "auto", "cpu", "cuda"
        """
        print(f"Loading {model_name}...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map=device
        )
        
        # Tool registry
        self.tools = {}
        
        print(f"‚úÖ Loaded on {self.model.device}")
    
    def register_tool(
        self,
        func: Callable,
        name: Optional[str] = None,
        description: Optional[str] = None
    ):
        """
        Register a tool/function for calling
        
        Args:
            func: Python function
            name: Tool name (default: function name)
            description: Tool description (default: from docstring)
        """
        tool_name = name or func.__name__
        tool_desc = description or func.__doc__ or "No description"
        
        self.tools[tool_name] = {
            "function": func,
            "description": tool_desc,
            "name": tool_name
        }
        
        print(f"‚úÖ Registered tool: {tool_name}")
    
    def parse_to_tools(
        self,
        instruction: str,
        available_tools: Optional[List[Callable]] = None,
        max_new_tokens: int = 200
    ) -> List[Dict]:
        """
        Parse natural language instruction ‡πÄ‡∏õ‡πá‡∏ô tool calls
        
        Args:
            instruction: Natural language command
            available_tools: List of functions (default: all registered)
            max_new_tokens: Max tokens for generation
            
        Returns:
            List of tool calls with parameters
        """
        # Get tools to use
        if available_tools is None:
            tools_list = list(self.tools.values())
        else:
            tools_list = [
                {"function": f, "name": f.__name__, "description": f.__doc__}
                for f in available_tools
            ]
        
        # Prepare messages
        messages = [{"role": "user", "content": instruction}]
        
        # Apply chat template with tools
        formatted_prompt = self.tokenizer.apply_chat_template(
            messages,
            tools=[t["function"] for t in tools_list],
            add_generation_prompt=True,
            tokenize=False
        )
        
        # Generate
        inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False  # Deterministic for tool calling
            )
        
        result = self.tokenizer.decode(outputs[0], skip_special_tokens=False)
        
        # Parse tool calls from output
        tool_calls = self._parse_function_calls(result)
        
        return tool_calls
    
    def execute_tools(
        self,
        tool_calls: List[Dict],
        error_handling: str = "continue"  # "continue", "stop", "skip"
    ) -> List[Dict]:
        """
        Execute parsed tool calls
        
        Args:
            tool_calls: List of tool calls from parse_to_tools()
            error_handling: How to handle errors
            
        Returns:
            List of results
        """
        results = []
        
        for call in tool_calls:
            tool_name = call.get("tool")
            params = call.get("params", {})
            
            try:
                # Get function
                if tool_name not in self.tools:
                    raise ValueError(f"Tool '{tool_name}' not registered")
                
                func = self.tools[tool_name]["function"]
                
                # Execute
                print(f"üîß Executing: {tool_name}({params})")
                result = func(**params)
                
                results.append({
                    "tool": tool_name,
                    "params": params,
                    "result": result,
                    "status": "success"
                })
                
            except Exception as e:
                error_msg = str(e)
                print(f"‚ùå Error in {tool_name}: {error_msg}")
                
                results.append({
                    "tool": tool_name,
                    "params": params,
                    "error": error_msg,
                    "status": "error"
                })
                
                if error_handling == "stop":
                    break
                elif error_handling == "skip":
                    continue
        
        return results
    
    def _parse_function_calls(self, text: str) -> List[Dict]:
        """
        Parse FunctionGemma output format to structured calls
        
        Format: <start_function_call>call:tool_name{param1:<escape>value<escape>}<end_function_call>
        """
        tool_calls = []
        
        # Pattern for function calls
        pattern = r'<start_function_call>call:(\w+)\{([^}]+)\}<end_function_call>'
        matches = re.findall(pattern, text)
        
        for tool_name, params_str in matches:
            # Parse parameters
            params = self._parse_params(params_str)
            
            tool_calls.append({
                "tool": tool_name,
                "params": params
            })
        
        return tool_calls
    
    def _parse_params(self, params_str: str) -> Dict:
        """Parse parameter string to dict"""
        params = {}
        
        # Simple parsing: key:<escape>value<escape>
        param_pattern = r'(\w+):<escape>([^<]+)<escape>'
        matches = re.findall(param_pattern, params_str)
        
        for key, value in matches:
            # Try to convert types
            if value.isdigit():
                params[key] = int(value)
            elif value.replace('.', '').isdigit():
                params[key] = float(value)
            elif value.lower() in ['true', 'false']:
                params[key] = value.lower() == 'true'
            else:
                params[key] = value
        
        return params
    
    def __repr__(self):
        return f"FunctionGemmaExecutor(tools={len(self.tools)}, device={self.model.device})"


# ===== EXAMPLE TOOLS =====

def image_gen(prompt: str, style: str = "realistic", size: str = "1024x1024") -> str:
    """
    Generate an image from text prompt
    
    Args:
        prompt: Image description
        style: Art style (realistic, minimal, artistic)
        size: Image size (1024x1024, 512x512)
        
    Returns:
        Path to generated image
    """
    print(f"üé® Generating image: {prompt[:50]}...")
    # Placeholder - ‡∏à‡∏∞ implement ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô Module 6
    return f"generated_image_{style}.png"

def voice_gen(text: str, voice: str = "th-TH-NiwatNeural", rate: str = "+0%") -> str:
    """
    Generate voiceover from text using Edge-TTS
    
    Args:
        text: Script text
        voice: Voice name
        rate: Speaking rate
        
    Returns:
        Path to audio file
    """
    print(f"üéôÔ∏è Generating voice: {text[:30]}...")
    # Placeholder - ‡∏à‡∏∞ implement ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô Module 6
    return "voiceover.mp3"

def video_compose(
    images: List[str],
    audio: str,
    duration: int = 15,
    transitions: List[str] = None
) -> str:
    """
    Compose video from images and audio
    
    Args:
        images: List of image paths
        audio: Audio file path
        duration: Video duration in seconds
        transitions: Transition effects
        
    Returns:
        Path to composed video
    """
    print(f"üé¨ Composing video: {len(images)} images, {duration}s")
    # Placeholder - ‡∏à‡∏∞ implement ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô Module 6
    return "final_video.mp4"

def smart_cut(
    video_path: str,
    mode: str = "trim_silence",
    target_duration: Optional[int] = None
) -> str:
    """
    Smart video editing with AI
    
    Args:
        video_path: Input video path
        mode: Edit mode (trim_silence, highlights, jump_cut)
        target_duration: Target video length
        
    Returns:
        Path to edited video
    """
    print(f"‚úÇÔ∏è Smart cutting: {video_path} ({mode})")
    # Placeholder - ‡∏à‡∏∞ implement ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô Module 6.5
    return f"{video_path}_edited.mp4"


# ===== USAGE EXAMPLES =====

if __name__ == "__main__":
    # Initialize executor
    executor = FunctionGemmaExecutor()
    
    # Register tools
    executor.register_tool(image_gen)
    executor.register_tool(voice_gen)
    executor.register_tool(video_compose)
    executor.register_tool(smart_cut)
    
    # Example 1: Simple tool calling
    print("\n=== Example 1: Parse Instruction ===")
    instruction = "‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ‡∏Å‡∏≤‡πÅ‡∏ü‡πÅ‡∏ö‡∏ö minimal ‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏û‡∏≤‡∏Å‡∏¢‡πå‡∏ß‡πà‡∏≤ 'Cold Brew Premium ‡∏à‡∏≤‡∏Å CoffeeLab'"
    
    tool_calls = executor.parse_to_tools(instruction)
    print(f"Parsed {len(tool_calls)} tool calls:")
    for call in tool_calls:
        print(f"  - {call['tool']}({call['params']})")
    
    # Example 2: Execute tools
    print("\n=== Example 2: Execute Tools ===")
    results = executor.execute_tools(tool_calls)
    for result in results:
        print(f"‚úÖ {result['tool']}: {result.get('result', result.get('error'))}")
    
    # Example 3: Complex workflow
    print("\n=== Example 3: Complete Workflow ===")
    complex_instruction = """
    Create a 15-second Instagram Reel for CoffeeLab:
    1. Generate a minimal style image of cold brew coffee
    2. Generate Thai voiceover saying '‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏Å‡∏±‡∏ö CoffeeLab Cold Brew'
    3. Compose video with fade transitions
    """
    
    tool_calls = executor.parse_to_tools(complex_instruction)
    results = executor.execute_tools(tool_calls, error_handling="continue")
    
    print(f"\nüìä Summary: {len(results)} tools executed")
    success_count = sum(1 for r in results if r['status'] == 'success')
    print(f"‚úÖ Success: {success_count}/{len(results)}")
```

### Part 3: Dual-Model Agent Implementation

```python
# ai_director_agent.py
"""
AI Director Agent - Complete Dual-Model System
‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ô T5Gemma 2 ‡πÅ‡∏•‡∏∞ FunctionGemma ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
"""

from t5gemma_thinker import T5GemmaThinker
from functiongemma_executor import FunctionGemmaExecutor
from typing import Dict, List, Optional
import json

class AIDirectorAgent:
    """
    AI Director Agent with Dual-Model Architecture
    
    Workflow:
    1. User Brief ‚Üí T5Gemma 2 (Strategy)
    2. Strategy ‚Üí FunctionGemma (Tool Calls)
    3. Tool Calls ‚Üí Execute ‚Üí Results
    """
    
    def __init__(
        self,
        thinker_size: str = "1b-1b",
        verbose: bool = True
    ):
        """
        Initialize AI Director
        
        Args:
            thinker_size: T5Gemma 2 size (270m-270m, 1b-1b, 4b-4b)
            verbose: Print detailed logs
        """
        self.verbose = verbose
        
        # Initialize models
        if self.verbose:
            print("üß† Initializing AI Director...")
        
        self.thinker = T5GemmaThinker(model_size=thinker_size)
        self.executor = FunctionGemmaExecutor()
        
        # Register default tools
        self._register_default_tools()
        
        if self.verbose:
            print("‚úÖ AI Director ready!")
    
    def _register_default_tools(self):
        """Register production tools"""
        from functiongemma_executor import (
            image_gen, voice_gen, video_compose, smart_cut
        )
        
        self.executor.register_tool(image_gen)
        self.executor.register_tool(voice_gen)
        self.executor.register_tool(video_compose)
        self.executor.register_tool(smart_cut)
    
    def process_brief(
        self,
        brief: str,
        brand_context: Optional[str] = None,
        mode: str = "auto"  # "create", "edit", "auto"
    ) -> Dict:
        """
        Complete workflow: Brief ‚Üí Strategy ‚Üí Execution ‚Üí Results
        
        Args:
            brief: Marketing brief or instruction
            brand_context: Brand guidelines from RAG
            mode: Workflow mode
            
        Returns:
            Dict with strategy, tool_calls, and results
        """
        if self.verbose:
            print(f"\n{'='*60}")
            print(f"üé¨ AI Director Processing Brief")
            print(f"{'='*60}")
            print(f"Brief: {brief[:100]}...")
        
        # Detect mode if auto
        if mode == "auto":
            mode = self._detect_mode(brief)
            if self.verbose:
                print(f"üìã Detected mode: {mode}")
        
        # STEP 1: THINKER - Generate Strategy
        if self.verbose:
            print(f"\nüß† Step 1: T5Gemma 2 (Strategy Generation)")
        
        strategy = self.thinker.generate_strategy(
            brief=brief,
            brand_context=brand_context
        )
        
        if self.verbose:
            print(f"Strategy: {strategy[:200]}...")
        
        # STEP 2: EXECUTOR - Parse to Tool Calls
        if self.verbose:
            print(f"\n‚ö° Step 2: FunctionGemma (Tool Parsing)")
        
        tool_calls = self.executor.parse_to_tools(strategy)
        
        if self.verbose:
            print(f"Parsed {len(tool_calls)} tool calls")
            for i, call in enumerate(tool_calls, 1):
                print(f"  {i}. {call['tool']}()")
        
        # STEP 3: Execute Tools
        if self.verbose:
            print(f"\nüîß Step 3: Tool Execution")
        
        results = self.executor.execute_tools(
            tool_calls,
            error_handling="continue"
        )
        
        # Summary
        if self.verbose:
            success = sum(1 for r in results if r['status'] == 'success')
            print(f"\n{'='*60}")
            print(f"‚úÖ Workflow Complete: {success}/{len(results)} successful")
            print(f"{'='*60}")
        
        return {
            "brief": brief,
            "mode": mode,
            "strategy": strategy,
            "tool_calls": tool_calls,
            "results": results,
            "summary": {
                "total_tools": len(tool_calls),
                "successful": sum(1 for r in results if r['status'] == 'success'),
                "failed": sum(1 for r in results if r['status'] == 'error')
            }
        }
    
    def edit_video(
        self,
        video_path: str,
        instruction: str,
        target_duration: Optional[int] = None
    ) -> Dict:
        """
        Smart Cut workflow ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏î‡∏ï‡πà‡∏≠ video
        
        Args:
            video_path: Input video file
            instruction: Edit instruction
            target_duration: Target length in seconds
            
        Returns:
            Dict with analysis and edited video
        """
        if self.verbose:
            print(f"\nüé¨ Smart Cut Workflow")
            print(f"Video: {video_path}")
            print(f"Instruction: {instruction}")
        
        # Step 1: Analyze (‡∏à‡∏∞ implement ‡πÉ‡∏ô Module 6.5)
        # - Transcribe with Whisper
        # - Detect silence with FFmpeg
        # - Extract frames
        
        # Step 2: T5Gemma 2 decides what to keep
        analysis_prompt = f"""
        Video: {video_path}
        Instruction: {instruction}
        Target Duration: {target_duration}s
        
        Decide which segments to keep and remove.
        """
        
        edit_decision = self.thinker.analyze_transcript(
            transcript=analysis_prompt,  # ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô real transcript ‡πÉ‡∏ô Module 6.5
            target_duration=target_duration or 120
        )
        
        # Step 3: FunctionGemma executes cuts
        tool_calls = self.executor.parse_to_tools(
            f"Edit video {video_path} according to: {edit_decision['raw_analysis']}"
        )
        
        results = self.executor.execute_tools(tool_calls)
        
        return {
            "video": video_path,
            "instruction": instruction,
            "analysis": edit_decision,
            "tool_calls": tool_calls,
            "results": results
        }
    
    def _detect_mode(self, brief: str) -> str:
        """Detect if task is create or edit"""
        edit_keywords = ["‡∏ï‡∏±‡∏î", "edit", "trim", "highlight", "cut"]
        create_keywords = ["‡∏™‡∏£‡πâ‡∏≤‡∏á", "create", "generate", "‡∏ó‡∏≥"]
        
        brief_lower = brief.lower()
        
        if any(kw in brief_lower for kw in edit_keywords):
            return "edit"
        elif any(kw in brief_lower for kw in create_keywords):
            return "create"
        else:
            return "create"  # default
    
    def get_stats(self) -> Dict:
        """Get system statistics"""
        return {
            "thinker": {
                "model": self.thinker.model_name,
                "device": str(self.thinker.model.device)
            },
            "executor": {
                "model": "google/functiongemma-270m-it",
                "device": str(self.executor.model.device),
                "registered_tools": len(self.executor.tools)
            }
        }
    
    def __repr__(self):
        return f"AIDirectorAgent(thinker={self.thinker.model_name}, tools={len(self.executor.tools)})"


# ===== USAGE EXAMPLES =====

if __name__ == "__main__":
    # Initialize AI Director
    agent = AIDirectorAgent(thinker_size="1b-1b", verbose=True)
    
    # Example 1: Create content from brief
    print("\n" + "="*70)
    print("EXAMPLE 1: CREATE CONTENT")
    print("="*70)
    
    brief_create = """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Instagram Reel 15 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CoffeeLab Cold Brew
    
    Target: ‡∏Ñ‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏® 25-35 ‡∏õ‡∏µ
    Mood: Premium, modern, minimal
    Message: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡πÅ‡∏ü‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
    """
    
    result = agent.process_brief(
        brief=brief_create,
        brand_context="CoffeeLab ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå‡∏Å‡∏≤‡πÅ‡∏ü‡∏û‡∏£‡∏µ‡πÄ‡∏°‡∏µ‡∏¢‡∏° ‡πÇ‡∏ó‡∏ô‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏•-‡∏Ñ‡∏£‡∏µ‡∏°"
    )
    
    # Print results
    print("\nüìä RESULTS:")
    print(json.dumps(result['summary'], indent=2))
    
    # Example 2: Edit existing video
    print("\n" + "="*70)
    print("EXAMPLE 2: EDIT VIDEO")
    print("="*70)
    
    edit_result = agent.edit_video(
        video_path="interview_raw.mp4",
        instruction="‡∏ï‡∏±‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 2 ‡∏ô‡∏≤‡∏ó‡∏µ ‡πÄ‡∏≠‡∏≤‡πÅ‡∏ï‡πà‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå",
        target_duration=120
    )
    
    # Example 3: Get system stats
    print("\n" + "="*70)
    print("SYSTEM STATS")
    print("="*70)
    stats = agent.get_stats()
    print(json.dumps(stats, indent=2))
```

### Testing Script

```python
# test_module1.py
"""
Test script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Module 1
‡∏ó‡∏î‡∏™‡∏≠‡∏ö dual-model architecture ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
"""

import sys

def test_thinker():
    """Test T5Gemma 2"""
    print("\n" + "="*70)
    print("TEST 1: T5GEMMA 2 (THINKER)")
    print("="*70)
    
    try:
        from t5gemma_thinker import T5GemmaThinker
        
        thinker = T5GemmaThinker(model_size="1b-1b")
        
        # Test strategy generation
        strategy = thinker.generate_strategy(
            brief="‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏Å‡∏≤‡πÅ‡∏ü 15 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ",
            max_length=200
        )
        
        print("‚úÖ Strategy generation: PASS")
        print(f"Output length: {len(strategy)} chars")
        
        # Test image prompt
        prompt = thinker.generate_image_prompt(
            brief="Premium coffee cup",
            style="minimal"
        )
        
        print("‚úÖ Image prompt generation: PASS")
        print(f"Prompt: {prompt[:100]}...")
        
        return True
        
    except Exception as e:
        print(f"‚ùå FAILED: {e}")
        return False

def test_executor():
    """Test FunctionGemma"""
    print("\n" + "="*70)
    print("TEST 2: FUNCTIONGEMMA (EXECUTOR)")
    print("="*70)
    
    try:
        from functiongemma_executor import FunctionGemmaExecutor, image_gen
        
        executor = FunctionGemmaExecutor()
        executor.register_tool(image_gen)
        
        # Test tool parsing
        instruction = "Generate a minimal style image of coffee"
        tool_calls = executor.parse_to_tools(instruction)
        
        print(f"‚úÖ Tool parsing: PASS")
        print(f"Parsed {len(tool_calls)} tool calls")
        
        # Test execution
        results = executor.execute_tools(tool_calls)
        
        print(f"‚úÖ Tool execution: PASS")
        print(f"Results: {len(results)}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå FAILED: {e}")
        return False

def test_agent():
    """Test complete agent"""
    print("\n" + "="*70)
    print("TEST 3: AI DIRECTOR AGENT (COMPLETE)")
    print("="*70)
    
    try:
        from ai_director_agent import AIDirectorAgent
        
        agent = AIDirectorAgent(thinker_size="1b-1b", verbose=False)
        
        # Test workflow
        result = agent.process_brief(
            brief="‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ‡∏Å‡∏≤‡πÅ‡∏ü‡πÅ‡∏ö‡∏ö minimal"
        )
        
        print(f"‚úÖ Complete workflow: PASS")
        print(f"Summary: {result['summary']}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå FAILED: {e}")
        return False

def main():
    """Run all tests"""
    print("\n" + "="*70)
    print("MODULE 1 - DUAL-MODEL ARCHITECTURE TEST SUITE")
    print("="*70)
    
    results = {
        "Thinker": test_thinker(),
        "Executor": test_executor(),
        "Agent": test_agent()
    }
    
    # Summary
    print("\n" + "="*70)
    print("TEST SUMMARY")
    print("="*70)
    
    for name, passed in results.items():
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"{name:.<40} {status}")
    
    total = len(results)
    passed = sum(results.values())
    
    print(f"\nTotal: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nüéâ All tests passed! Module 1 complete!")
        return 0
    else:
        print("\n‚ö†Ô∏è Some tests failed. Please check errors above.")
        return 1

if __name__ == "__main__":
    sys.exit(main())
```

---

## üéØ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏±‡∏ô Code ‡πÉ‡∏ô Codespaces

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏£‡∏ö

```bash
# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß
cd /workspaces/ai-director-course/module1
ls -la

# ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô:
# t5gemma_thinker.py
# functiongemma_executor.py
# ai_director_agent.py
# test_module1.py
```

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏£‡∏±‡∏ô Test (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)

```bash
# ‡∏£‡∏±‡∏ô test suite ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
python test_module1.py
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô:**
```
======================================================================
MODULE 1 - DUAL-MODEL ARCHITECTURE TEST SUITE
======================================================================

======================================================================
TEST 1: T5GEMMA 2 (THINKER)
======================================================================
Loading google/t5gemma-2-1b-1b...
‚úÖ Loaded on cuda:0
‚úÖ Strategy generation: PASS
Output length: 245 chars
‚úÖ Image prompt generation: PASS
Prompt: A sleek glass bottle of cold brew coffee...

======================================================================
TEST 2: FUNCTIONGEMMA (EXECUTOR)
======================================================================
Loading google/functiongemma-270m-it...
‚úÖ Loaded on cuda:0
‚úÖ Registered tool: image_gen
‚úÖ Tool parsing: PASS
Parsed 1 tool calls
üîß Executing: image_gen({'prompt': '...', 'style': 'minimal'})
‚úÖ Tool execution: PASS
Results: 1

======================================================================
TEST 3: AI DIRECTOR AGENT (COMPLETE)
======================================================================
üß† Initializing AI Director...
‚úÖ AI Director ready!
‚úÖ Complete workflow: PASS
Summary: {'total_tools': 2, 'successful': 2, 'failed': 0}

======================================================================
TEST SUMMARY
======================================================================
Thinker...................................... ‚úÖ PASS
Executor..................................... ‚úÖ PASS
Agent........................................ ‚úÖ PASS

Total: 3/3 tests passed

üéâ All tests passed! Module 1 complete!
```

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Module ‡πÅ‡∏¢‡∏Å

```bash
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö T5Gemma 2 ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
python t5gemma_thinker.py

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö FunctionGemma ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
python functiongemma_executor.py

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Agent ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
python ai_director_agent.py
```

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö Interactive

‡πÄ‡∏õ‡∏¥‡∏î Python REPL:
```bash
python
```

‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:
```python
# Import modules
from ai_director_agent import AIDirectorAgent

# ‡∏™‡∏£‡πâ‡∏≤‡∏á agent
agent = AIDirectorAgent(thinker_size="1b-1b", verbose=True)

# ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
result = agent.process_brief(
    brief="‡∏™‡∏£‡πâ‡∏≤‡∏á Instagram post ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡πÅ‡∏ü"
)

# ‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
print(result['summary'])

# ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Python
exit()
```

---

## üêõ ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 1: ModuleNotFoundError

```bash
# Error: ModuleNotFoundError: No module named 'transformers'

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
pip install transformers torch accelerate pillow requests
```

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 2: CUDA/GPU ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°

```python
# Warning: CUDA not available, using CPU

# ‡∏ô‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏ô Codespaces (‡πÑ‡∏°‡πà‡∏°‡∏µ GPU ‡∏ü‡∏£‡∏µ)
# Code ‡∏à‡∏∞‡∏£‡∏±‡∏ô‡∏ö‡∏ô CPU ‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤
# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö training ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Google Colab (Module 4)
```

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 3: Out of Memory

```bash
# Error: OutOfMemoryError

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ model ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤
# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å "1b-1b" ‡πÄ‡∏õ‡πá‡∏ô "270m-270m"
```

```python
# ‡πÉ‡∏ô code
thinker = T5GemmaThinker(model_size="270m-270m")  # ‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤
```

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 4: Model Download ‡∏ä‡πâ‡∏≤

```bash
# Model ‡∏Å‡∏≥‡∏•‡∏±‡∏á download ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å (1-2 GB)
# ‡πÉ‡∏ô Codespaces ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 5-10 ‡∏ô‡∏≤‡∏ó‡∏µ

# Tips: ‡∏£‡∏≠‡πÉ‡∏´‡πâ download ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å
# ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏à‡∏∞‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô (cached ‡πÉ‡∏ô Codespace)
```

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 5: Import Error ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Files

```bash
# Error: ImportError: cannot import name 'T5GemmaThinker'

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤:
# 1. ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô folder ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (module1/)
# 2. ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (t5gemma_thinker.py)
# 3. ‡πÑ‡∏°‡πà‡∏°‡∏µ syntax error ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå

# ‡∏•‡∏≠‡∏á‡πÄ‡∏ä‡πá‡∏Ñ:
cd /workspaces/ai-director-course/module1
python -c "from t5gemma_thinker import T5GemmaThinker; print('OK')"
```

---

## üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏á‡∏≤‡∏ô‡∏•‡∏á GitHub

```bash
# 1. ‡πÄ‡∏ä‡πá‡∏Ñ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
cd /workspaces/ai-director-course
git status

# 2. Add files
git add module1/
git add .gitignore

# 3. Commit
git commit -m "Complete Module 1: Dual-Model Architecture"

# 4. Push to GitHub
git push
```

‚úÖ **‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ code ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏ö‡∏ô GitHub ‡πÅ‡∏•‡πâ‡∏ß!**

---

## üìä ‡πÄ‡∏ä‡πá‡∏Ñ Free Tier Usage

```bash
# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ Codespaces ‡πÑ‡∏õ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà‡πÅ‡∏•‡πâ‡∏ß
# ‡πÑ‡∏õ‡∏ó‡∏µ‡πà: https://github.com/settings/billing
# ‡∏î‡∏π‡∏ó‡∏µ‡πà: Codespaces usage

# Free tier: 120 core-hours/month
# 2-core machine: ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ 60 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á/‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
```

**Tips ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î:**
- ‚úÖ Stop Codespace ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ (‡∏õ‡∏∏‡πà‡∏° Stop ‡∏ó‡∏µ‡πà‡∏°‡∏∏‡∏°‡∏•‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≤‡∏¢)
- ‚úÖ Set auto-stop timeout: 30 ‡∏ô‡∏≤‡∏ó‡∏µ
- ‚úÖ ‡∏•‡∏ö Codespace ‡πÄ‡∏Å‡πà‡∏≤‡πÜ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß

---

## ‚úÖ Checklist: Module 1 Complete

‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡∏Ñ‡∏£‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á:

### Setup
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á GitHub repository
- [ ] ‡πÄ‡∏õ‡∏¥‡∏î Codespaces ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
- [ ] ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies (transformers, torch, etc.)
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á folder structure

### Code
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `t5gemma_thinker.py`
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `functiongemma_executor.py`
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `ai_director_agent.py`
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `test_module1.py`

### Testing
- [ ] ‡∏£‡∏±‡∏ô `python test_module1.py` ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
- [ ] ‡∏ó‡∏î‡∏™‡∏≠‡∏ö T5Gemma 2 ‡∏ú‡πà‡∏≤‡∏ô
- [ ] ‡∏ó‡∏î‡∏™‡∏≠‡∏ö FunctionGemma ‡∏ú‡πà‡∏≤‡∏ô
- [ ] ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Complete Agent ‡∏ú‡πà‡∏≤‡∏ô

### Git
- [ ] Commit code ‡∏•‡∏á git
- [ ] Push ‡∏Ç‡∏∂‡πâ‡∏ô GitHub

---

## üéì ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£?

‡∏ñ‡πâ‡∏≤‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÑ‡∏õ:

**Module 2: Data Engineering & Pipeline (ETL)**
- Setup MongoDB Atlas (‡∏ü‡∏£‡∏µ)
- Setup ChromaDB
- ‡∏™‡∏£‡πâ‡∏≤‡∏á data schemas
- Load sample data

**‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏° Module 2 ‡∏û‡∏£‡πâ‡∏≠‡∏° complete code ‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö?**

---

## üéØ Next Steps

‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Module 1 ‡πÅ‡∏•‡πâ‡∏ß ‡∏Ñ‡∏∏‡∏ì‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:

**Module 2: Data Engineering & Pipeline (ETL)**  
- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏•‡∏±‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI Director
- ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö MongoDB schemas
- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ChromaDB ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAG
- ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• transcript ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Smart Cut

---

## üìö Additional Resources for Module 1

### Architecture Comparisons
- [T5 vs GPT: Encoder-Decoder vs Decoder-Only](https://huggingface.co/blog/encoder-decoder)
- [Function Calling with Small Models](https://huggingface.co/blog/function-calling)

### Model Documentation
- [T5Gemma 2 Model Card](https://huggingface.co/google/t5gemma-2-1b-1b)
- [FunctionGemma Guide](https://huggingface.co/google/functiongemma-270m-it)

### Video Tutorials
- Search YouTube: "Multi-agent architecture AI"
- Search YouTube: "Function calling with small models"

---

**üé¨ Module 1 Complete! Ready for Module 2? üöÄ**

---

**üé¨ AI Director v3.4 - Complete Documentation Edition**

**New in v3.4:**
- üìö Comprehensive documentation for all components
- üîó Official links to documentation
- üìñ Tutorial notebooks and examples
- üáπüá≠ Thai language best practices
- üîß Troubleshooting guide
- üìä Quick reference cards

**Zero Cost Stack ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°: $0.00**

**Happy Directing! üöÄ**
