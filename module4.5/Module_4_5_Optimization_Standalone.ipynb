{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfaf AI Director: Meta Ax Hyperparameter Optimization (Standalone)\n",
    "\n",
    "**Module 4.5: Bayesian Optimization for LoRA Training**\n",
    "\n",
    "This notebook uses **Meta Ax** to find optimal hyperparameters.\n",
    "\n",
    "**Target**: Beat baseline loss of 0.6097  \n",
    "**Time**: ~2.5 hours on T4 GPU (20 trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\ufe0f\u20e3 Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\ufe0f\u20e3 Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q torch transformers peft datasets accelerate bitsandbytes ax-platform botorch gpytorch loguru pandas pyyaml scipy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\ufe0f\u20e3 Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DATASET_PATH = '/content/drive/MyDrive/ai_director_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\ufe0f\u20e3 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'search_space': {\n",
    "        'lora_rank': {'type': 'choice', 'values': [8, 16, 32]},  # \u0e25\u0e14: \u0e44\u0e21\u0e48\u0e21\u0e35 64\n",
    "        'lora_alpha': {'type': 'choice', 'values': [16, 32, 64]},  # \u0e25\u0e14: \u0e44\u0e21\u0e48\u0e21\u0e35 128\n",
    "        'learning_rate': {'type': 'range', 'bounds': [0.00001, 0.0005], 'log_scale': True},\n",
    "        'batch_size': {'type': 'choice', 'values': [1]}  # \u0e25\u0e07: \u0e40\u0e2b\u0e25\u0e37\u0e2d 1 \u0e40\u0e17\u0e48\u0e32\u0e19\u0e31\u0e49\u0e19\n",
    "    },\n",
    "    'baseline': {'loss': 0.6097},\n",
    "    'num_trials': 20,\n",
    "    'dataset_path': DATASET_PATH\n",
    "}\n",
    "print(f\"\u26a0\ufe0f LOW MEMORY MODE: batch_size=1 only, max_rank=32\")\n",
    "print(f\"Target: Beat {CONFIG['baseline']['loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\ufe0f\u20e3 Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from datasets import load_dataset\n",
    "\n",
    "@dataclass\n",
    "class FineTuningConfig:\n",
    "    base_model: str = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    max_seq_length: int = 1024\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: List[str] = field(default_factory=lambda: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"])\n",
    "    num_train_epochs: int = 1\n",
    "    per_device_train_batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    learning_rate: float = 2e-4\n",
    "    warmup_ratio: float = 0.1\n",
    "    logging_steps: int = 5\n",
    "    eval_steps: int = 50\n",
    "    save_steps: int = 50\n",
    "    dataset_path: str = \"/content/drive/MyDrive/ai_director_dataset\"\n",
    "    output_dir: str = \"/content/models/temp\"\n",
    "\n",
    "class AIDirectorFineTuner:\n",
    "    def __init__(self, config: FineTuningConfig):\n",
    "        self.config = config\n",
    "        self.output_dir = Path(config.output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.dataset = None\n",
    "    \n",
    "    def load_tokenizer(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.base_model, trust_remote_code=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def load_model(self):\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.base_model,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "    \n",
    "    def apply_lora(self):\n",
    "        lora_config = LoraConfig(\n",
    "            r=self.config.lora_r,\n",
    "            lora_alpha=self.config.lora_alpha,\n",
    "            target_modules=self.config.target_modules,\n",
    "            lora_dropout=self.config.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM\n",
    "        )\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        dataset_path = Path(self.config.dataset_path)\n",
    "        self.dataset = load_dataset(\"json\", data_files={\n",
    "            \"train\": str(dataset_path / \"train_v2.jsonl\"),\n",
    "            \"validation\": str(dataset_path / \"val_v2.jsonl\")\n",
    "        })\n",
    "        \n",
    "        def format_prompt(sample):\n",
    "            return f'''<|im_start|>system\\nYou are an AI Director.<|im_end|>\\n<|im_start|>user\\n{sample[\"instruction\"]}\\n\\n{sample[\"input\"]}<|im_end|>\\n<|im_start|>assistant\\n{sample[\"output\"]}<|im_end|>'''\n",
    "        \n",
    "        def tokenize(examples):\n",
    "            texts = [format_prompt(ex) for ex in examples]\n",
    "            tok = self.tokenizer(texts, truncation=True, max_length=self.config.max_seq_length, padding=\"max_length\")\n",
    "            tok[\"labels\"] = tok[\"input_ids\"].copy()\n",
    "            return tok\n",
    "        \n",
    "        self.dataset[\"train\"] = self.dataset[\"train\"].map(lambda x: tokenize([x]), batched=False)\n",
    "        self.dataset[\"validation\"] = self.dataset[\"validation\"].map(lambda x: tokenize([x]), batched=False)\n",
    "    \n",
    "    def train(self):\n",
    "        args = TrainingArguments(\n",
    "            output_dir=str(self.output_dir),\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            warmup_ratio=self.config.warmup_ratio,\n",
    "            logging_steps=self.config.logging_steps,\n",
    "            eval_steps=self.config.eval_steps,\n",
    "            save_steps=self.config.save_steps,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            load_best_model_at_end=True,\n",
    "            bf16=True,\n",
    "            report_to=[]\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=args,\n",
    "            train_dataset=self.dataset[\"train\"],\n",
    "            eval_dataset=self.dataset[\"validation\"],\n",
    "            data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        )\n",
    "        result = trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        return {'eval_loss': eval_result['eval_loss']}\n",
    "\n",
    "print(\"\u2705 Training function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\ufe0f\u20e3 Optimization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from ax.service.ax_client import AxClient\n",
    "from ax.service.utils.instantiation import ObjectiveProperties\n",
    "\n",
    "class HyperparameterOptimizer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.ax_client = None\n",
    "        self.trial_results = []\n",
    "        self.checkpoint_path = Path('/content/drive/MyDrive/ai_director_optimization_checkpoint.json')\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        \"\"\"Save checkpoint after each trial\"\"\"\n",
    "        checkpoint = {\n",
    "            'trial_results': self.trial_results,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'completed_trials': len(self.trial_results)\n",
    "        }\n",
    "        self.checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(self.checkpoint_path, 'w') as f:\n",
    "            json.dump(checkpoint, f, indent=2)\n",
    "        print(f\"\ud83d\udcbe Checkpoint saved: {len(self.trial_results)} trials completed\")\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load checkpoint if exists\"\"\"\n",
    "        if self.checkpoint_path.exists():\n",
    "            with open(self.checkpoint_path, 'r') as f:\n",
    "                checkpoint = json.load(f)\n",
    "            self.trial_results = checkpoint['trial_results']\n",
    "            print(f\"\u2705 Loaded checkpoint: {len(self.trial_results)} trials already completed\")\n",
    "            return len(self.trial_results)\n",
    "        return 0\n",
    "    \n",
    "    def setup_ax(self):\n",
    "        self.ax_client = AxClient()\n",
    "        parameters = []\n",
    "        for name, cfg in self.config['search_space'].items():\n",
    "            if cfg['type'] == 'choice':\n",
    "                parameters.append({\"name\": name, \"type\": \"choice\", \"values\": cfg['values'], \"value_type\": \"int\"})\n",
    "            elif cfg['type'] == 'range':\n",
    "                parameters.append({\"name\": name, \"type\": \"range\", \"bounds\": cfg['bounds'], \"value_type\": \"float\", \"log_scale\": cfg.get('log_scale', False)})\n",
    "        \n",
    "        self.ax_client.create_experiment(\n",
    "            name=\"ai_director_optimization\",\n",
    "            parameters=parameters,\n",
    "            objectives={\"eval_loss\": ObjectiveProperties(minimize=True)}\n",
    "        )\n",
    "        print(\"\u2705 Ax setup complete\")\n",
    "    \n",
    "    def evaluate(self, params):\n",
    "        print(f\"\\n\ud83d\udd2c Trial: {params}\")\n",
    "        try:\n",
    "            cfg = FineTuningConfig(\n",
    "                lora_r=params['lora_rank'],\n",
    "                lora_alpha=params['lora_alpha'],\n",
    "                learning_rate=params['learning_rate'],\n",
    "                per_device_train_batch_size=params['batch_size'],\n",
    "                output_dir=f\"/content/models/trial_{datetime.now():%H%M%S}\",\n",
    "                dataset_path=self.config['dataset_path']\n",
    "            )\n",
    "            trainer = AIDirectorFineTuner(cfg)\n",
    "            trainer.load_tokenizer()\n",
    "            trainer.load_model()\n",
    "            trainer.apply_lora()\n",
    "            trainer.load_dataset()\n",
    "            result = trainer.train()\n",
    "            loss = result['eval_loss']\n",
    "            print(f\"\u2705 Loss: {loss:.4f}\")\n",
    "            if Path(cfg.output_dir).exists():\n",
    "                shutil.rmtree(cfg.output_dir)\n",
    "            # Clear CUDA cache\n",
    "            import torch\n",
    "            torch.cuda.empty_cache()\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            return loss\n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Error: {e}\")\n",
    "            return 1.0\n",
    "    \n",
    "    def run(self, num_trials=None):\n",
    "        if num_trials is None:\n",
    "            num_trials = self.config['num_trials']\n",
    "        \n",
    "        # Try to load checkpoint\n",
    "        start_trial = self.load_checkpoint()\n",
    "        \n",
    "        if start_trial > 0:\n",
    "            print(f\"\\n\ud83d\udd04 RESUMING from trial {start_trial + 1}/{num_trials}\")\n",
    "            print(f\"Already completed: {start_trial} trials\\n\")\n",
    "        \n",
    "        self.setup_ax()\n",
    "        \n",
    "        # Replay completed trials to Ax\n",
    "        if start_trial > 0:\n",
    "            print(\"\ud83d\udd04 Replaying completed trials to Ax...\")\n",
    "            for result in self.trial_results:\n",
    "                params, trial_idx = self.ax_client.get_next_trial()\n",
    "                self.ax_client.complete_trial(trial_index=trial_idx, raw_data=result['loss'])\n",
    "            print(f\"\u2705 Replayed {start_trial} trials\\n\")\n",
    "        \n",
    "        print(f\"\ud83d\ude80 Starting optimization from trial {start_trial + 1}/{num_trials}\\n\")\n",
    "        \n",
    "        # Run remaining trials\n",
    "        for i in range(start_trial, num_trials):\n",
    "            print(f\"\\n{'#'*50}\\nTRIAL {i+1}/{num_trials}\\n{'#'*50}\")\n",
    "            params, trial_idx = self.ax_client.get_next_trial()\n",
    "            loss = self.evaluate(params)\n",
    "            self.ax_client.complete_trial(trial_index=trial_idx, raw_data=loss)\n",
    "            self.trial_results.append({'trial': i+1, 'params': params, 'loss': loss, 'timestamp': datetime.now().isoformat()})\n",
    "            \n",
    "            # Save checkpoint after each trial\n",
    "            self.save_checkpoint()\n",
    "        \n",
    "        best_params, values = self.ax_client.get_best_parameters()\n",
    "        best_loss = values[0]['eval_loss']\n",
    "        improvement = (self.config['baseline']['loss'] - best_loss) / self.config['baseline']['loss'] * 100\n",
    "        \n",
    "        print(f\"\\n{'='*50}\\n\ud83c\udfc6 RESULTS\\n{'='*50}\")\n",
    "        print(f\"Best: {best_params}\")\n",
    "        print(f\"Loss: {best_loss:.4f} (baseline: {self.config['baseline']['loss']:.4f})\")\n",
    "        print(f\"Improvement: {improvement:+.1f}%\")\n",
    "        \n",
    "        # Delete checkpoint after completion\n",
    "        if self.checkpoint_path.exists():\n",
    "            self.checkpoint_path.unlink()\n",
    "            print(\"\\n\ud83d\uddd1\ufe0f Checkpoint deleted (optimization complete)\")\n",
    "        \n",
    "        return {'best_params': best_params, 'best_loss': best_loss, 'improvement': improvement, 'trials': self.trial_results}\n",
    "\n",
    "print(\"\u2705 Optimizer ready (with checkpoint support)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\ufe0f\u20e3 Run Optimization \ud83d\ude80\n",
    "\n",
    "**\u26a0\ufe0f WARNING**: This will take ~2.5 hours on T4 GPU!\n",
    "\n",
    "**\u2705 Checkpoint Feature**: \n",
    "- Auto-saves after each trial to Google Drive\n",
    "- If Colab disconnects, just run this cell again\n",
    "- Will automatically resume from last completed trial\n",
    "- No need to start over!\n",
    "\n",
    "**Quick test**: Change to `num_trials=3` (~25 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if checkpoint exists\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "checkpoint_path = Path('/content/drive/MyDrive/ai_director_optimization_checkpoint.json')\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    with open(checkpoint_path, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "    print(f\"\ud83d\udcc1 Checkpoint found!\")\n",
    "    print(f\"Completed trials: {checkpoint['completed_trials']}/{CONFIG['num_trials']}\")\n",
    "    print(f\"Last updated: {checkpoint['timestamp']}\")\n",
    "    print(f\"\\n\u2705 Will resume from trial {checkpoint['completed_trials'] + 1}\")\n",
    "else:\n",
    "    print(\"\ud83d\udcc1 No checkpoint found - will start from trial 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = HyperparameterOptimizer(CONFIG)\n",
    "results = optimizer.run(num_trials=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\uddd1\ufe0f Delete Checkpoint (Optional)\n",
    "\n",
    "Run this if you want to **start over** from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u26a0\ufe0f WARNING: This will delete checkpoint - you'll start from trial 1\n",
    "checkpoint_path = Path('/content/drive/MyDrive/ai_director_optimization_checkpoint.json')\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    checkpoint_path.unlink()\n",
    "    print(\"\ud83d\uddd1\ufe0f Checkpoint deleted - will start from trial 1 next time\")\n",
    "else:\n",
    "    print(\"\ud83d\udcc1 No checkpoint found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8\ufe0f\u20e3 Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "output_dir = Path('/content/drive/MyDrive/ai_director_optimization_results')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# JSON\n",
    "with open(output_dir / 'results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# YAML\n",
    "with open(output_dir / 'best_config.yaml', 'w') as f:\n",
    "    yaml.dump(results['best_params'], f)\n",
    "\n",
    "print(f\"\u2705 Saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9\ufe0f\u20e3 Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trials = [r['trial'] for r in results['trials']]\n",
    "losses = [r['loss'] for r in results['trials']]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(trials, losses, 'bo-')\n",
    "plt.axhline(y=CONFIG['baseline']['loss'], color='r', linestyle='--', label='Baseline')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Optimization Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(output_dir / 'plot.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"\ud83c\udfaf SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best Loss: {results['best_loss']:.4f}\")\n",
    "print(f\"Improvement: {results['improvement']:+.1f}%\")\n",
    "print(f\"Best Config: {results['best_params']}\")\n",
    "print(f\"\\nResults saved to Google Drive: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}