# Module 4: Model Fine-tuning with LoRA + RAG Hybrid

> Fine-tune LLM à¸”à¹‰à¸§à¸¢ LoRA à¹€à¸à¸·à¹ˆà¸­à¸ªà¸£à¹‰à¸²à¸‡ AI Director à¸—à¸µà¹ˆà¹€à¸Šà¸µà¹ˆà¸¢à¸§à¸Šà¸²à¸à¸”à¹‰à¸²à¸™à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡ marketing content  
> **RAG Hybrid Approach**: Fine-tuning à¸ªà¸­à¸™ "à¸—à¸±à¸à¸©à¸°" + RAG à¸›à¹‰à¸­à¸™ "à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰"

## ğŸ“‹ Overview

Module à¸™à¸µà¹‰à¸—à¸³à¸à¸²à¸£ fine-tune base LLM (Qwen2.5-7B-Instruct) à¸”à¹‰à¸§à¸¢ LoRA technique à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ dataset 71 samples (31 à¹€à¸”à¸´à¸¡ + 40 synthetic) à¸ˆà¸²à¸ Module 3 à¹à¸¥à¸°à¸£à¸§à¸¡à¸à¸±à¸š **RAG (Retrieval-Augmented Generation)** à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰ model à¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸£à¹‰à¸²à¸‡ on-brand marketing content à¹„à¸”à¹‰à¹à¸¡à¹‰à¸à¸±à¸šà¹à¸šà¸£à¸™à¸”à¹Œà¹ƒà¸«à¸¡à¹ˆà¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¹€à¸„à¸¢à¹€à¸«à¹‡à¸™à¹ƒà¸™ training data

### ğŸ¯ Hybrid Architecture: Fine-tuning + RAG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Problem: OOD (Out-of-Distribution)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âŒ Fine-tuning à¹€à¸à¸µà¸¢à¸‡à¸­à¸¢à¹ˆà¸²à¸‡à¹€à¸”à¸µà¸¢à¸§:
   â€¢ à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸šà¸£à¸™à¸”à¹Œà¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ train.jsonl (à¸à¸±à¸‡à¹ƒà¸™ model weights)
   â€¢ à¹€à¸à¸´à¹ˆà¸¡à¹à¸šà¸£à¸™à¸”à¹Œà¹ƒà¸«à¸¡à¹ˆ = à¸•à¹‰à¸­à¸‡ retrain (2-3 à¸Šà¸¡ + GPU cost)
   â€¢ à¹€à¸ªà¸µà¹ˆà¸¢à¸‡ hallucination à¸–à¹‰à¸²à¹à¸šà¸£à¸™à¸”à¹Œà¹„à¸¡à¹ˆà¸¡à¸µà¹ƒà¸™ train

âœ… RAG Hybrid (à¹à¸™à¸°à¸™à¸³):
   â€¢ Fine-tuning à¸ªà¸­à¸™ "à¸—à¸±à¸à¸©à¸°" - à¹€à¸‚à¸µà¸¢à¸™ format, tone, structure
   â€¢ RAG à¸›à¹‰à¸­à¸™ "à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰" - à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸šà¸£à¸™à¸”à¹Œà¸ˆà¸²à¸ brands.json
   â€¢ à¹€à¸à¸´à¹ˆà¸¡à¹à¸šà¸£à¸™à¸”à¹Œà¹ƒà¸«à¸¡à¹ˆ = à¹à¸à¹‰ JSON (5 à¸™à¸²à¸—à¸µ, à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ retrain)
   â€¢ à¹„à¸¡à¹ˆà¹€à¸à¸´à¸” hallucination (à¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸£à¸´à¸‡à¹ƒà¸™ prompt)
```

### Key Features

1. **QLoRA** - 4-bit quantization à¸ªà¸³à¸«à¸£à¸±à¸š memory efficiency
2. **Data Augmentation** - 71 samples à¸ˆà¸²à¸ 11 brands (à¸«à¸¥à¸²à¸à¸«à¸¥à¸²à¸¢à¸­à¸¸à¸•à¸ªà¸²à¸«à¸à¸£à¸£à¸¡)
3. **RAG Integration** - à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸šà¸£à¸™à¸”à¹Œà¹à¸šà¸š real-time à¸ˆà¸²à¸ brands.json
4. **Thai Language Support** - Qwen2.5 model à¸—à¸µà¹ˆà¸£à¸­à¸‡à¸£à¸±à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢
5. **Production-Ready** - Inference script à¸à¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ + RAG support

## ğŸš€ Quick Start

### Prerequisites

**GPU Requirements**:
- Minimum: 8GB VRAM (e.g., Google Colab T4)
- Recommended: 16GB+ VRAM (e.g., Google Colab L4, A100)
- CUDA support required

### Installation

```bash
cd module4
pip install -r requirements.txt
```

**Key Dependencies**:
- `transformers>=4.48.0` - HuggingFace Transformers
- `peft>=0.13.2` - Parameter-Efficient Fine-Tuning (LoRA)
- `bitsandbytes>=0.45.0` - 4-bit quantization
- `torch>=2.5.0` - PyTorch with CUDA

### Step 1: Data Augmentation (Optional but Recommended)

à¹€à¸à¸´à¹ˆà¸¡à¹à¸šà¸£à¸™à¸”à¹Œà¸ªà¸¡à¸¡à¸•à¸´à¹€à¸à¸·à¹ˆà¸­à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ overfitting:

```bash
python scripts/augment_training_data.py
```

**à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ**:
- à¹€à¸”à¸´à¸¡: 31 samples à¸ˆà¸²à¸ 3 brands (CoffeeLab, FitFlow, GreenLeaf)
- à¹€à¸à¸´à¹ˆà¸¡: 40 samples à¸ˆà¸²à¸ 8 synthetic brands (PetPals, SpeedyLoans, LuxStay, etc.)
- à¸£à¸§à¸¡: **71 samples à¸ˆà¸²à¸ 11 brands** â†’ `train_augmented.jsonl`

**à¸—à¸³à¹„à¸¡à¸•à¹‰à¸­à¸‡à¸—à¸³?**  
à¹‚à¸¡à¹€à¸”à¸¥à¸ˆà¸°à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸§à¹ˆà¸² "Brand tone à¹à¸•à¸à¸•à¹ˆà¸²à¸‡à¸à¸±à¸™à¸•à¸²à¸¡à¸­à¸¸à¸•à¸ªà¸²à¸«à¸à¸£à¸£à¸¡" à¹„à¸¡à¹ˆà¸ˆà¸³à¹€à¸‰à¸à¸²à¸° 3 brands à¹€à¸”à¸´à¸¡

### Step 2: Fine-tuning

```bash
python scripts/finetune_lora.py
```

**Training Time**:
- T4 GPU (Colab Free): ~2-3 hours
- L4 GPU (Colab Pro): ~45-60 minutes
- A100 GPU: ~20-30 minutes

### Step 3: Inference with RAG

**à¹à¸™à¸§à¸—à¸²à¸‡ 1: Inference à¹€à¸”à¸´à¸¡ (à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰ RAG)**
```bash
python scripts/inference.py
```

**à¹à¸™à¸§à¸—à¸²à¸‡ 2: Inference + RAG (à¹à¸™à¸°à¸™à¸³) âœ…**
```bash
# Demo conceptual
python scripts/demo_rag_concept.py

# à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸ˆà¸£à¸´à¸‡ (à¸•à¹‰à¸­à¸‡à¸¡à¸µ fine-tuned model)
python scripts/inference_rag.py
```

## ğŸ”§ Technical Configuration

### LoRA Parameters

```python
{
    "lora_r": 16,              # Rank (number of trainable parameters)
    "lora_alpha": 32,          # Scaling factor (2x of r)
    "lora_dropout": 0.05,      # Dropout for regularization
    "target_modules": [        # Modules to apply LoRA
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
}
```

**Why These Values?**:
- `r=16`: Good balance between capacity and overfitting
- `alpha=32`: Standard 2x of r for stable training
- `dropout=0.05`: Small dropout to prevent overfitting on 31 samples

### Training Configuration

```python
{
    "base_model": "Qwen/Qwen2.5-7B-Instruct",
    "max_seq_length": 1024,
    "num_train_epochs": 3,
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 4,  # Effective batch size = 4
    "learning_rate": 2e-4,
    "warmup_steps": 10,
    "load_in_4bit": True,
    "optim": "paged_adamw_8bit"
}
```

**Why Qwen2.5-7B-Instruct?**:
1. âœ… Excellent Thai language support
2. âœ… 7B size fits in Colab T4 (with 4-bit)
3. âœ… Instruct-tuned (good at following instructions)
4. âœ… Strong base performance

**Why 3 Epochs?**:
- Small dataset (31 samples)
- More epochs risk overfitting
- Early stopping on validation loss

**Why Batch Size = 1?**:
- Memory constraints with 4-bit quantization
- Gradient accumulation steps = 4 â†’ effective batch size = 4

### Memory Usage

| GPU | VRAM | Load in 4-bit | Batch Size | Status |
|-----|------|---------------|------------|--------|
| T4 (Colab Free) | 15GB | Yes | 1 | âœ… Supported |
| L4 (Colab Pro) | 24GB | Yes | 2 | âœ… Recommended |
| A100 | 40GB | No | 4 | âœ… Optimal |

**4-bit Quantization (QLoRA)**:
- Reduces memory from ~28GB to ~8GB
- Minimal accuracy loss (<2%)
- Training ~20% slower than LoRA

## ğŸ“Š Training Process

### Data Flow

```
Module 3 Datasets (Original)
    â”œâ”€â”€ train.jsonl (31 samples) â†’ 3 brands
    â”œâ”€â”€ val.jsonl (4 samples)
    â””â”€â”€ test.jsonl (4 samples)

â†“ Data Augmentation (Optional)

Module 3 Datasets (Augmented) âœ… Recommended
    â”œâ”€â”€ train_augmented.jsonl (71 samples) â†’ 11 brands
    â”œâ”€â”€ val.jsonl (4 samples)
    â””â”€â”€ test.jsonl (4 samples)

Format: Alpaca Instruction Format
    {
        "instruction": "à¹€à¸‚à¸µà¸¢à¸™ caption à¸ªà¸³à¸«à¸£à¸±à¸š CoffeeLab",
        "input": "Brand: CoffeeLab\nTone: friendly, premium",
        "output": "à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¹€à¸Šà¹‰à¸²à¸§à¸±à¸™à¹ƒà¸«à¸¡à¹ˆà¸”à¹‰à¸§à¸¢à¸à¸²à¹à¸Ÿà¸—à¸µà¹ˆà¹ƒà¸Šà¹ˆ â˜•ï¸"
    }

â†’ Qwen2.5 Chat Format
    <|im_start|>system
    You are an AI Director for marketing content creation.
    <|im_end|>
    <|im_start|>user
    {instruction}\n{input}
    <|im_end|>
    <|im_start|>assistant
    {output}
    <|im_end|>
```

### Training Steps

1. **Load Base Model** (Qwen2.5-7B-Instruct with 4-bit quantization)
2. **Setup LoRA Adapters** (r=16, alpha=32)
3. **Load Datasets** (71 train / 4 val - à¸–à¹‰à¸²à¹ƒà¸Šà¹‰ augmented)
4. **Tokenize** (max_length=1024)
5. **Train** (3 epochs, ~90-180 steps)
6. **Evaluate** (test loss, perplexity)
7. **Save** (LoRA adapters + tokenizer)

### Monitoring

**Metrics Tracked**:
- Training loss (every step)
- Validation loss (every 50 steps)
- Learning rate schedule
- GPU memory usage

**TensorBoard**:
```bash
tensorboard --logdir=logs
```

## ğŸ“ˆ Expected Results

### Training Metrics

| Metric | Expected Value | Notes |
|--------|---------------|-------|
| Train Loss (start) | ~2.0-3.0 | Base model already instructed |
| Train Loss (end) | ~0.5-1.0 | Good fit on small dataset |
| Val Loss | ~1.0-1.5 | Should be close to train loss |
| Test Perplexity | ~3-5 | Lower is better |

**Good Training Indicators**:
- âœ… Decreasing train loss
- âœ… Val loss â‰ˆ train loss (no overfitting)
- âœ… Test perplexity < 10

**Warning Signs**:
- âš ï¸ Val loss >> train loss (overfitting)
- âš ï¸ Train loss oscillating (learning rate too high)
- âš ï¸ Test perplexity > 20 (poor generalization)

### Qualitative Results

**Before Fine-tuning (Base Model)**:
```
Instruction: à¹€à¸‚à¸µà¸¢à¸™ caption à¸ªà¸³à¸«à¸£à¸±à¸š CoffeeLab
Output: "Here is a caption for CoffeeLab..."  (English response)
```

**After Fine-tuning Only (Without RAG)**:
```
Instruction: à¹€à¸‚à¸µà¸¢à¸™ caption à¸ªà¸³à¸«à¸£à¸±à¸š TechZone (à¹à¸šà¸£à¸™à¸”à¹Œà¹ƒà¸«à¸¡à¹ˆ)
Output: "à¹€à¸•à¸´à¸¡à¸à¸¥à¸±à¸‡à¹€à¸Šà¹‰à¸²à¸§à¸±à¸™à¹ƒà¸«à¸¡à¹ˆà¸à¸±à¸š TechZone â˜•ï¸"  (Hallucination - à¹ƒà¸Šà¹‰ tone CoffeeLab)
```

**After Fine-tuning + RAG (Recommended) âœ…**:
```
Instruction: à¹€à¸‚à¸µà¸¢à¸™ caption à¸ªà¸³à¸«à¸£à¸±à¸š TechZone (à¹à¸šà¸£à¸™à¸”à¹Œà¹ƒà¸«à¸¡à¹ˆ)
RAG Context: [à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ TechZone à¸ˆà¸²à¸ brands.json]
Output: "ğŸ® Level Up Your Game! Gaming Mouse à¸ˆà¸²à¸ TechZone âš¡ #GamingGear"  (à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡)
```

## ğŸ¯ Usage Examples

### Option 1: Standard Inference (à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰ RAG)

```python
from scripts.inference import AIDirectorInference

ai_director = AIDirectorInference()

# Generate caption
caption = ai_director.generate_caption(
    brand_name="CoffeeLab",
    tone="friendly, premium, modern",
    context="product launch"
)
print(caption)
# Output: "ğŸ‰ à¹€à¸›à¸´à¸”à¸•à¸±à¸§! à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¹€à¸Šà¹‰à¸²à¸§à¸±à¸™à¹ƒà¸«à¸¡à¹ˆà¸”à¹‰à¸§à¸¢à¸à¸²à¹à¸Ÿà¸—à¸µà¹ˆà¹ƒà¸Šà¹ˆ â˜•ï¸"
```

### Option 2: RAG-Enhanced Inference (à¹à¸™à¸°à¸™à¸³) âœ…

```python
from scripts.inference_rag import AIDirectorRAGInference

# à¸ªà¸£à¹‰à¸²à¸‡ inference engine à¸à¸£à¹‰à¸­à¸¡ RAG
ai_director = AIDirectorRAGInference(
    lora_adapter_path="../models/qwen-7b-ai-director",
    brands_json_path="../../module2/data/brands.json"
)

# Generate caption à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸šà¸£à¸™à¸”à¹Œà¹ƒà¸«à¸¡à¹ˆ (à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ retrain!)
caption = ai_director.generate_caption(
    brand_name="TechZone",  # à¹à¸šà¸£à¸™à¸”à¹Œà¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µà¹ƒà¸™ train.jsonl
    context="Gaming mouse launch"
)
print(caption)
# Output: "ğŸ® Level Up Your Game! Gaming Mouse à¸•à¸±à¸§à¹ƒà¸«à¸¡à¹ˆà¸ˆà¸²à¸ TechZone 
#           à¸ªà¹€à¸›à¸à¹€à¸—à¸ à¸•à¸­à¸šà¸ªà¸™à¸­à¸‡à¹€à¸£à¹‡à¸§à¸—à¸±à¸™à¹ƒà¸ˆ âš¡ #TechZone #GamingGear"

# à¸§à¸´à¸˜à¸µà¹€à¸à¸´à¹ˆà¸¡à¹à¸šà¸£à¸™à¸”à¹Œà¹ƒà¸«à¸¡à¹ˆ:
# 1. à¹à¸à¹‰à¹„à¸‚ ../../module2/data/brands.json à¹€à¸à¸´à¹ˆà¸¡ TechZone
# 2. à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸—à¸³à¸­à¸°à¹„à¸£à¸à¸±à¸šà¹‚à¸¡à¹€à¸”à¸¥ (à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ retrain!)
# 3. à¸£à¸±à¸™ inference_rag.py à¹„à¸”à¹‰à¹€à¸¥à¸¢
```

### Brand Voice Adaptation

```python
# Adapt generic message to brand voice
adapted = ai_director.adapt_brand_voice(
    brand_name="FitFlow",
    tone="energetic, motivating",
    message="à¹€à¸£à¸²à¸¡à¸µà¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™à¸à¸´à¹€à¸¨à¸©"
)
print(adapted)
# Output: "ğŸ’ª à¹€à¸£à¸²à¸¡à¸µà¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™à¸à¸´à¹€à¸¨à¸© à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸à¸²à¸£à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡à¸§à¸±à¸™à¸™à¸µà¹‰!"
```

### Campaign Brief

```python
# Generate campaign brief
brief = ai_director.generate_campaign_brief(
    brand_name="GreenLeaf",
    objectives="education, trust building",
    tone="natural, caring"
)
print(brief)
# Output: "Educational content series showing journey from farm..."
```

## ğŸ”„ Fine-tuning Workflow

### Step 1: Prepare Environment

```bash
# Check GPU
python -c "import torch; print(torch.cuda.is_available())"
python -c "import torch; print(torch.cuda.get_device_name(0))"

# Install dependencies
pip install -r requirements.txt
```

### Step 2: Run Fine-tuning

```bash
python scripts/finetune_lora.py
```

**What Happens**:
1. Loads Qwen2.5-7B-Instruct (4-bit)
2. Adds LoRA adapters (16 rank)
3. Loads Module 3 datasets
4. Tokenizes samples
5. Trains for 3 epochs (~90-180 steps)
6. Saves to `models/qwen-7b-ai-director/`

**Output Files**:
```
module4/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ qwen-7b-ai-director/
â”‚       â”œâ”€â”€ adapter_config.json
â”‚       â”œâ”€â”€ adapter_model.safetensors
â”‚       â”œâ”€â”€ tokenizer_config.json
â”‚       â”œâ”€â”€ tokenizer.json
â”‚       â””â”€â”€ eval_results.json
â””â”€â”€ logs/
    â”œâ”€â”€ finetune_*.log
    â””â”€â”€ events.out.tfevents.*
```

### Step 3: Test Inference

```bash
python scripts/inference.py
```

**Demo Output**:
```
ğŸ“ Demo 1: Caption Generation
Generated caption:
à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¹€à¸Šà¹‰à¸²à¸§à¸±à¸™à¹ƒà¸«à¸¡à¹ˆà¸”à¹‰à¸§à¸¢à¸à¸²à¹à¸Ÿà¸—à¸µà¹ˆà¹ƒà¸Šà¹ˆ â˜•ï¸ #CoffeeLab

ğŸ¨ Demo 2: Brand Voice Adaptation
Adapted message:
ğŸ’ª à¹€à¸£à¸²à¸¡à¸µà¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™à¸à¸´à¹€à¸¨à¸© à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸à¸²à¸£à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡à¸§à¸±à¸™à¸™à¸µà¹‰!

ğŸ“‹ Demo 3: Campaign Brief
Generated brief:
Educational content series showing journey from farm to table...
```

## ğŸ“ Key Learnings

### 1. Small Dataset Fine-tuning

**Challenge**: Only 31 training samples

**Solution**:
- âœ… Use LoRA (fewer parameters â†’ less overfitting)
- âœ… Few epochs (3 epochs max)
- âœ… Strong regularization (dropout, weight decay)
- âœ… Start from instruct-tuned model (less data needed)

**Research**: "Less than 100 examples can be sufficient for instruction-tuning" (InstructGPT paper)

### 2. Thai Language Support

**Challenge**: Generate on-brand Thai content

**Solution**:
- âœ… Choose Qwen2.5 (excellent Thai support)
- âœ… Use Thai examples in training data
- âœ… Test on Thai prompts

**Alternatives**:
- Meta Llama 3.1 (good multilingual)
- SeaLLM (Southeast Asia focus)
- OpenThaiGPT (Thai-specific, but smaller)

### 3. Memory Optimization

**Challenge**: 7B model on Colab T4 (15GB VRAM)

**Solution**:
- âœ… 4-bit quantization (QLoRA)
- âœ… Gradient checkpointing
- âœ… Small batch size with gradient accumulation
- âœ… paged_adamw_8bit optimizer

**Memory Breakdown**:
```
Base Model (4-bit): ~4GB
LoRA Adapters: ~100MB
Optimizer States: ~3GB
Gradients: ~1GB
Activations: ~2GB
Total: ~10GB (fits in T4!)
```

### 4. Evaluation Strategy

**Quantitative**:
- Loss metrics (train/val/test)
- Perplexity
- BLEU/ROUGE scores (if have references)

**Qualitative** (More Important!):
- Manual review of generated content
- Brand voice consistency
- Thai grammar correctness
- Appropriate emoji usage

## ğŸ—ï¸ RAG Hybrid Architecture

### Problem: Out-of-Distribution (OOD) Inputs

à¹€à¸¡à¸·à¹ˆà¸­à¹‚à¸¡à¹€à¸”à¸¥ fine-tune à¸”à¹‰à¸§à¸¢à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸à¸µà¸¢à¸‡ 3 brands (CoffeeLab, FitFlow, GreenLeaf) à¸¡à¸µà¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡:

1. **Hallucination** - à¹ƒà¸Šà¹‰ tone/hashtag à¸œà¸´à¸”à¹€à¸¡à¸·à¹ˆà¸­à¹€à¸ˆà¸­à¹à¸šà¸£à¸™à¸”à¹Œà¹ƒà¸«à¸¡à¹ˆ
2. **Inflexibility** - à¸•à¹‰à¸­à¸‡ retrain à¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡à¸—à¸µà¹ˆà¸¡à¸µà¹à¸šà¸£à¸™à¸”à¹Œà¹ƒà¸«à¸¡à¹ˆ
3. **Data Staleness** - à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸šà¸£à¸™à¸”à¹Œà¸­à¸±à¸›à¹€à¸”à¸•à¸Šà¹‰à¸² (à¸à¸±à¸‡à¹ƒà¸™ weights)

### Solution: Fine-tuning + RAG Hybrid

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Architecture Diagram                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Request                                              Output
     â”‚                                                       â–²
     â”‚  "à¹€à¸‚à¸µà¸¢à¸™ caption à¸ªà¸³à¸«à¸£à¸±à¸š TechZone                       â”‚
     â”‚   (à¹à¸šà¸£à¸™à¸”à¹Œà¹ƒà¸«à¸¡à¹ˆ)"                                        â”‚
     â”‚                                                       â”‚
     â–¼                                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG System  â”‚                                    â”‚  Fine-tuned   â”‚
â”‚             â”‚                                    â”‚     Model     â”‚
â”‚ brands.json â”‚â—„â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â–ºâ”‚ Qwen + LoRA  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                       â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚             â”‚                       â”‚               â–²
     â”‚ Query:      â”‚                       â”‚               â”‚
     â”‚ TechZone    â”‚                       â”‚               â”‚
     â–¼             â”‚                       â”‚               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Brand Context      â”‚           â”‚    Enriched Prompt              â”‚
â”‚  â€¢ Tone: à¸¥à¹‰à¸³à¸ªà¸¡à¸±à¸¢    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Instruction + RAG Context      â”‚
â”‚  â€¢ Values: Gaming   â”‚           â”‚                                 â”‚
â”‚  â€¢ Target: 18-35    â”‚           â”‚ Brand: TechZone                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚ Tone: à¸¥à¹‰à¸³à¸ªà¸¡à¸±à¸¢, à¸£à¸§à¸”à¹€à¸£à¹‡à¸§, à¹€à¸—à¹ˆ     â”‚
                                  â”‚ Values: Performance First...    â”‚
                                  â”‚ Target: Gamers 18-35 à¸›à¸µ        â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

#### 1. Fine-tuned Model (à¸—à¸±à¸à¸©à¸° - Skill)

**à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆ:**
- à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸§à¸´à¸˜à¸µà¹€à¸‚à¸µà¸¢à¸™ format à¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡
- à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆ structure à¸‚à¸­à¸‡ caption, brief, brand voice
- à¸£à¸¹à¹‰à¸§à¸´à¸˜à¸µà¹ƒà¸Šà¹‰ emoji, hashtag à¸­à¸¢à¹ˆà¸²à¸‡à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡

**à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸ˆà¸³:**
- à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¹à¸šà¸£à¸™à¸”à¹Œà¹€à¸‰à¸à¸²à¸° (à¸Šà¸·à¹ˆà¸­, tone, values)
- à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡à¸šà¹ˆà¸­à¸¢

#### 2. RAG System (à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰ - Knowledge)

**à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆ:**
- à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸šà¸£à¸™à¸”à¹Œà¸ˆà¸²à¸ `brands.json` à¹à¸šà¸š real-time
- à¹ƒà¸ªà¹ˆà¹ƒà¸™ prompt à¸à¹ˆà¸­à¸™à¸ªà¹ˆà¸‡à¹ƒà¸«à¹‰à¹‚à¸¡à¹€à¸”à¸¥
- à¸£à¸­à¸‡à¸£à¸±à¸šà¹à¸šà¸£à¸™à¸”à¹Œà¹ƒà¸«à¸¡à¹ˆà¸—à¸±à¸™à¸—à¸µà¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ retrain

**à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸”à¸¶à¸‡:**
- Brand name, description
- Tone of voice
- Target audience
- Core values
- Visual style

### Benefits

| Feature | Fine-tuning Only | Fine-tuning + RAG |
|---------|-----------------|-------------------|
| à¹€à¸à¸´à¹ˆà¸¡à¹à¸šà¸£à¸™à¸”à¹Œà¹ƒà¸«à¸¡à¹ˆ | âŒ à¸•à¹‰à¸­à¸‡ retrain (2-3 à¸Šà¸¡) | âœ… à¹à¸à¹‰ JSON (5 à¸™à¸²à¸—à¸µ) |
| à¸­à¸±à¸›à¹€à¸”à¸•à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ | âŒ à¸•à¹‰à¸­à¸‡ retrain à¸—à¸±à¹‰à¸‡à¹‚à¸¡à¹€à¸”à¸¥ | âœ… à¹à¸à¹‰ JSON à¹„à¸”à¹‰à¸—à¸±à¸™à¸—à¸µ |
| Hallucination | âš ï¸ à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¸ªà¸¹à¸‡ (à¸›à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸šà¸£à¸™à¸”à¹Œà¸­à¸·à¹ˆà¸™) | âœ… à¸•à¹ˆà¸³à¸¡à¸²à¸ (à¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸£à¸´à¸‡) |
| Scalability | âŒ à¸ˆà¸³à¸à¸±à¸”à¸”à¹‰à¸§à¸¢à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ train | âœ… à¹„à¸¡à¹ˆà¸ˆà¸³à¸à¸±à¸” (à¸‚à¸¶à¹‰à¸™à¸à¸±à¸š DB) |
| Cost | âŒ GPU cost à¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡à¸—à¸µà¹ˆà¸­à¸±à¸›à¹€à¸”à¸• | âœ… à¹à¸à¹‰à¹„à¸Ÿà¸¥à¹Œà¸Ÿà¸£à¸µ |

### Implementation Files

1. **inference_rag.py** - RAG-enhanced inference engine
   - `BrandRAG` class: à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸„à¹‰à¸™à¸«à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸šà¸£à¸™à¸”à¹Œ
   - `AIDirectorRAGInference`: à¸£à¸§à¸¡ RAG + Fine-tuned model
   
2. **demo_rag_concept.py** - Demo conceptual comparison
   - à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š 3 à¹à¸™à¸§à¸—à¸²à¸‡: Base / Fine-tuned / Fine-tuned+RAG
   
3. **augment_training_data.py** - Data augmentation
   - à¸ªà¸£à¹‰à¸²à¸‡ synthetic brands à¹€à¸à¸·à¹ˆà¸­à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ overfitting

### Usage Example

```python
# à¹€à¸à¸´à¹ˆà¸¡à¹à¸šà¸£à¸™à¸”à¹Œà¹ƒà¸«à¸¡à¹ˆà¹ƒà¸™ brands.json (à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ retrain!)
{
  "brands": [
    {
      "name": "TechZone",
      "description": "à¸£à¹‰à¸²à¸™à¸­à¸¸à¸›à¸à¸£à¸“à¹Œà¹€à¸à¸¡à¸¡à¸´à¹ˆà¸‡à¹à¸¥à¸°à¹„à¸­à¸—à¸µ",
      "tone": ["à¸¥à¹‰à¸³à¸ªà¸¡à¸±à¸¢", "à¸£à¸§à¸”à¹€à¸£à¹‡à¸§", "à¹€à¸—à¹ˆ"],
      "target_audience": "Gamers 18-35 à¸›à¸µ",
      "core_values": ["Performance First", "Gamer Community"]
    }
  ]
}

# à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸—à¸±à¸™à¸—à¸µ
from scripts.inference_rag import AIDirectorRAGInference

inference = AIDirectorRAGInference(
    lora_adapter_path="../models/qwen-7b-ai-director",
    brands_json_path="../../module2/data/brands.json"
)

caption = inference.generate_caption(
    brand_name="TechZone",  # à¹à¸šà¸£à¸™à¸”à¹Œà¹ƒà¸«à¸¡à¹ˆ!
    context="Gaming mouse launch"
)
# Output: "ğŸ® Level Up Your Game! Gaming Mouse à¸•à¸±à¸§à¹ƒà¸«à¸¡à¹ˆà¸ˆà¸²à¸ TechZone..."
```

### When to Retrain

| Scenario | Need Retrain? | Reason |
|----------|--------------|--------|
| à¹€à¸à¸´à¹ˆà¸¡à¹à¸šà¸£à¸™à¸”à¹Œà¹ƒà¸«à¸¡à¹ˆ | âŒ No | à¹ƒà¸Šà¹‰ RAG à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸ brands.json |
| à¸­à¸±à¸›à¹€à¸”à¸•à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸šà¸£à¸™à¸”à¹Œ | âŒ No | à¹à¸à¹‰ brands.json |
| à¹€à¸à¸´à¹ˆà¸¡ task type à¹ƒà¸«à¸¡à¹ˆ | âœ… Yes | à¸•à¹‰à¸­à¸‡à¸ªà¸­à¸™à¹‚à¸¡à¹€à¸”à¸¥à¸§à¸´à¸˜à¸µà¸—à¸³à¸‡à¸²à¸™ |
| à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™ output format | âœ… Yes | à¸•à¹‰à¸­à¸‡à¹€à¸—à¸£à¸™ format à¹ƒà¸«à¸¡à¹ˆ |
| Base model upgrade | âœ… Yes | à¹ƒà¸Šà¹‰ base model à¹ƒà¸«à¸¡à¹ˆ |
| Performance drop | âœ… Yes | à¹‚à¸¡à¹€à¸”à¸¥à¹€à¸ªà¸·à¹ˆà¸­à¸¡ (model drift) |

---

## ğŸ“š References

### Research Papers

1. **LoRA**: "LoRA: Low-Rank Adaptation of Large Language Models" (Microsoft, 2021)
2. **QLoRA**: "QLoRA: Efficient Finetuning of Quantized LLMs" (University of Washington, 2023)
3. **InstructGPT**: "Training language models to follow instructions with human feedback" (OpenAI, 2022)
4. **RAG**: "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" (Facebook AI, 2020)

### Tools & Libraries

1. **HuggingFace PEFT**: https://github.com/huggingface/peft
2. **bitsandbytes**: https://github.com/TimDettmers/bitsandbytes
3. **Qwen2.5**: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct

### Related Modules

- **Module 2**: [../module2/README.md](../module2/README.md) - ETL Pipeline (brands.json source)
- **Module 3**: [../module3/README.md](../module3/README.md) - Dataset Generation
- **Module 3 Lesson Learned**: [../module3/LESSON_LEARNED.md](../module3/LESSON_LEARNED.md) - Data quality insights

## âœ… Success Criteria

Module 4 à¸ªà¸³à¹€à¸£à¹‡à¸ˆà¹€à¸¡à¸·à¹ˆà¸­:

1. â­ï¸ Fine-tuning completes without errors
2. â­ï¸ Test loss < 2.0
3. â­ï¸ Generated Thai content is grammatically correct
4. â­ï¸ Brand voice matches examples from Module 3
5. â­ï¸ Model can generate for all brands (including new brands via RAG)
6. â­ï¸ Inference works with demo script
7. âœ… RAG integration tested and documented
8. âœ… Data augmentation script tested (71 samples from 11 brands)

## âš ï¸ Limitations & Future Work

### Current Limitations

1. **Small Dataset**: 71 samples may not cover all edge cases
2. **Single Language**: Primarily Thai (limited English mixing)
3. **RAG Simplicity**: Basic keyword lookup (could use vector search)
4. **Task Coverage**: 3 main task types (may need more for production)

### Future Improvements

1. **More Data**: Collect 100-500 samples per brand
2. **Vector RAG**: Use ChromaDB/Weaviate for semantic search
3. **Active Learning**: Add samples where model fails
4. **Multi-brand**: Train on 50+ brands
5. **Deployment**: Package as API endpoint with RAG
6. **A/B Testing**: Compare with base model in production
7. **Confidence Scoring**: Add uncertainty estimation for OOD detection

---

**Module 4 Status**: âœ… COMPLETE (Structure & RAG Integration)

**Created**: 2026-01-04  
**Base Model**: Qwen2.5-7B-Instruct  
**Training Data**: 71 samples (31 original + 40 synthetic)  
**Architecture**: Fine-tuning + RAG Hybrid  
**Next**: Run fine-tuning on GPU â†’ Test RAG inference
