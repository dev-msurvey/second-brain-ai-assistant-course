{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d538f648",
   "metadata": {},
   "source": [
    "# üéØ AI Director Fine-tuning on Google Colab\n",
    "\n",
    "## ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô (‡πÅ‡∏ö‡∏ö‡∏£‡∏±‡∏ô‡∏ó‡∏µ‡∏•‡∏∞ cell)\n",
    "\n",
    "**‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤:** 3-4 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (T4) ‡∏´‡∏£‡∏∑‡∏≠ 1-2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (L4)\n",
    "\n",
    "**GPU ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ:**\n",
    "- Runtime ‚Üí Change runtime type ‚Üí T4 GPU (Free tier)\n",
    "- ‡∏´‡∏£‡∏∑‡∏≠ L4 GPU (Colab Pro)\n",
    "\n",
    "**Dataset:**\n",
    "- 148 training samples\n",
    "- 18 validation samples  \n",
    "- 19 test samples\n",
    "- ‡∏£‡∏ß‡∏° 185 samples ‡∏à‡∏≤‡∏Å 8 brands\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91cf0a8",
   "metadata": {},
   "source": [
    "## üìå Step 1: ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Google Drive\n",
    "\n",
    "Upload folder `ai-director-colab` ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Google Drive ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Å‡πà‡∏≠‡∏ô  \n",
    "‡∏ß‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà: `/content/drive/MyDrive/ai-director-colab/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b810f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Google Drive ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc574db1",
   "metadata": {},
   "source": [
    "## üìå Step 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà Upload ‡∏°‡∏≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ folder ai-director-colab ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
    "project_path = '/content/drive/MyDrive/ai-director-colab/ai-director-colab'\n",
    "\n",
    "if os.path.exists(project_path):\n",
    "    print(\"‚úÖ ‡∏û‡∏ö folder ai-director-colab!\")\n",
    "    print(\"\\nüìÅ ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå:\")\n",
    "    !ls -lh {project_path}\n",
    "else:\n",
    "    print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö folder ai-director-colab/ai-director-colab\")\n",
    "    print(\"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤ upload folder ‡πÑ‡∏õ‡∏ó‡∏µ‡πà /content/drive/MyDrive/ai-director-colab/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f433bb3a",
   "metadata": {},
   "source": [
    "## üìå Step 3: Copy ‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏≤‡∏ó‡∏µ‡πà Colab (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤)\n",
    "\n",
    "Copy ‡∏à‡∏≤‡∏Å Drive ‡∏°‡∏≤‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô Colab storage ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏ó‡∏£‡∏ô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /content/drive/MyDrive/ai-director-colab/ai-director-colab /content/\n",
    "\n",
    "print(\"‚úÖ Copy ‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "print(\"\\nüìÇ ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô /content/ai-director-colab:\")\n",
    "!ls -lh /content/ai-director-colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fb1d72",
   "metadata": {},
   "source": [
    "## üìå Step 4: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"\\nüéÆ CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üìä GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GPU! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Runtime ‡πÄ‡∏õ‡πá‡∏ô GPU\")\n",
    "    print(\"Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e019b",
   "metadata": {},
   "source": [
    "## üìå Step 5: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies\n",
    "\n",
    "‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á libraries ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô (~5 ‡∏ô‡∏≤‡∏ó‡∏µ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01411e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers>=4.48.0 peft>=0.13.2 accelerate>=1.2.1 bitsandbytes>=0.45.0\n",
    "!pip install -q datasets>=3.2.0 loguru>=0.7.3 tqdm>=4.67.1\n",
    "!pip install -q sentencepiece>=0.2.0 protobuf>=5.29.3\n",
    "\n",
    "print(\"‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö version\n",
    "import transformers\n",
    "import peft\n",
    "print(f\"\\nüì¶ Transformers: {transformers.__version__}\")\n",
    "print(f\"üì¶ PEFT: {peft.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13646a85",
   "metadata": {},
   "source": [
    "## üìå Step 6: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Dataset\n",
    "\n",
    "‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏£‡∏ô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8466b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô samples\n",
    "def count_jsonl_lines(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "data_path = '/content/ai-director-colab/data'\n",
    "\n",
    "train_count = count_jsonl_lines(f'{data_path}/train_v2.jsonl')\n",
    "val_count = count_jsonl_lines(f'{data_path}/val_v2.jsonl')\n",
    "test_count = count_jsonl_lines(f'{data_path}/test_v2.jsonl')\n",
    "\n",
    "print(\"üìä Dataset Statistics:\")\n",
    "print(f\"  - Training:   {train_count} samples\")\n",
    "print(f\"  - Validation: {val_count} samples\")\n",
    "print(f\"  - Test:       {test_count} samples\")\n",
    "print(f\"  - Total:      {train_count + val_count + test_count} samples\")\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "print(\"\\nüìù ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (sample 1):\")\n",
    "with open(f'{data_path}/train_v2.jsonl', 'r', encoding='utf-8') as f:\n",
    "    sample = json.loads(f.readline())\n",
    "    print(f\"Instruction: {sample['instruction']}\")\n",
    "    print(f\"\\nInput: {sample['input'][:200]}...\")\n",
    "    print(f\"\\nOutput: {sample['output'][:200]}...\")\n",
    "    print(f\"\\nMetadata: {sample['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2d198",
   "metadata": {},
   "source": [
    "## üìå Step 7: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô! üöÄ\n",
    "\n",
    "**‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤:** 3-4 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (T4) ‡∏´‡∏£‡∏∑‡∏≠ 1-2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (L4)\n",
    "\n",
    "**‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô:**\n",
    "- Training progress bar\n",
    "- Loss ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ\n",
    "- Evaluation results ‡∏ó‡∏∏‡∏Å 50 steps\n",
    "\n",
    "**Output:**\n",
    "- LoRA adapters ‚Üí `models/qwen-7b-ai-director-v2/`\n",
    "- Training logs ‚Üí `logs/`\n",
    "- Checkpoints ‡∏ó‡∏∏‡∏Å 50 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡∏ó‡∏µ‡πà scripts directory\n",
    "%cd /content/ai-director-colab/scripts\n",
    "\n",
    "# ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô\n",
    "!python finetune_lora.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8ac9cb",
   "metadata": {},
   "source": [
    "## üìå Step 8: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á\n",
    "model_path = '/content/ai-director-colab/scripts/models/qwen-7b-ai-director-v2'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"‚úÖ ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß! ‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•:\")\n",
    "    !ls -lh {model_path}\n",
    "    \n",
    "    # ‡∏≠‡πà‡∏≤‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£ evaluate\n",
    "    eval_file = f'{model_path}/eval_results.json'\n",
    "    if os.path.exists(eval_file):\n",
    "        with open(eval_file, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        print(\"\\nüìä Evaluation Results:\")\n",
    "        for key, value in results.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ó‡∏£‡∏ô... ‡πÇ‡∏õ‡∏£‡∏î‡∏£‡∏≠\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d0c3b6",
   "metadata": {},
   "source": [
    "## üìå Step 9: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• (Inference)\n",
    "\n",
    "‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e7f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞ extract ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå zip\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "base_path = \"/content/drive/MyDrive/ai-director-colab\"\n",
    "model_folder = f\"{base_path}/trained_models/qwen-7b-ai-director-v2\"\n",
    "\n",
    "# ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå zip\n",
    "print(\"üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå zip ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á...\\n\")\n",
    "!ls -lh {base_path}/trained_models*.zip 2>/dev/null || echo \"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå zip\"\n",
    "\n",
    "# ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÉ‡∏´‡πâ extract\n",
    "if not os.path.exists(model_folder):\n",
    "    # ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå zip ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ trained_models\n",
    "    zip_files = !ls {base_path}/trained_models*.zip 2>/dev/null\n",
    "    \n",
    "    if zip_files and zip_files[0]:\n",
    "        zip_path = zip_files[0]\n",
    "        print(f\"üì¶ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå zip: {zip_path}\")\n",
    "        print(\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á extract... (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 1-2 ‡∏ô‡∏≤‡∏ó‡∏µ)\")\n",
    "        \n",
    "        # Extract ‡πÑ‡∏õ‡∏ó‡∏µ‡πà base_path\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(base_path)\n",
    "        \n",
    "        print(\"‚úÖ Extract ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "    else:\n",
    "        print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå zip ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•\")\n",
    "        print(\"üìå ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á:\")\n",
    "        print(\"   1. Copy ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå trained_models ‡∏à‡∏≤‡∏Å account ‡πÄ‡∏Å‡πà‡∏≤\")\n",
    "        print(\"   2. ‡∏´‡∏£‡∏∑‡∏≠ upload ‡πÑ‡∏ü‡∏•‡πå zip ‡∏ó‡∏µ‡πà download ‡∏°‡∏≤\")\n",
    "else:\n",
    "    print(f\"‚úÖ ‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡πâ‡∏ß: {model_folder}\")\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå\n",
    "print(f\"\\nüîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô: {model_folder}\\n\")\n",
    "if os.path.exists(model_folder):\n",
    "    !ls -lh {model_folder}\n",
    "    \n",
    "    print(\"\\nüìã ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô:\")\n",
    "    required_files = [\"adapter_config.json\", \"adapter_model.safetensors\"]\n",
    "    for file in required_files:\n",
    "        exists = os.path.exists(f\"{model_folder}/{file}\")\n",
    "        status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "        print(f\"  {status} {file}\")\n",
    "else:\n",
    "    print(\"‚ùå ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb3dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "print(\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•...\")\n",
    "\n",
    "base_model = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "lora_path = \"/content/drive/MyDrive/ai-director-colab/trained_models/qwen-7b-ai-director-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ 4-bit quantization (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î base model ‡∏û‡∏£‡πâ‡∏≠‡∏° 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î LoRA adapters\n",
    "model = PeftModel.from_pretrained(model, lora_path)\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö generate\n",
    "def generate_content(instruction, input_text, max_length=512):\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "You are an AI Director for marketing content creation. Generate on-brand content based on the instructions.<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}\n",
    "\n",
    "{input_text}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_length,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    # Decode ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ generated tokens\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # ‡∏ï‡∏±‡∏î‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô assistant response\n",
    "    # ‡∏ï‡∏±‡∏î prompt ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏≠‡∏Å ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å \"assistant\"\n",
    "    if \"assistant\" in full_response:\n",
    "        parts = full_response.split(\"assistant\")\n",
    "        # ‡πÄ‡∏≠‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å \"assistant\" ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏£‡∏¥‡∏á‡πÜ\n",
    "        result = parts[-1].strip()\n",
    "        return result\n",
    "    \n",
    "    return full_response.strip()\n",
    "\n",
    "print(\"‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b6a548",
   "metadata": {},
   "source": [
    "### ‡∏ó‡∏î‡∏™‡∏≠‡∏ö 1: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Instagram Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Instagram caption ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CoffeeLab\"\n",
    "input_text = \"\"\"Brand: CoffeeLab\n",
    "Product: Single Origin Ethiopia\n",
    "Tone: warm, friendly, expert\n",
    "Context: New product launch\"\"\"\n",
    "\n",
    "print(\"üìù Instruction:\", instruction)\n",
    "print(\"\\nüí¨ Input:\", input_text)\n",
    "print(\"\\nü§ñ AI Director Output:\")\n",
    "print(\"=\" * 50)\n",
    "print(generate_content(instruction, input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b596d",
   "metadata": {},
   "source": [
    "### ‡∏ó‡∏î‡∏™‡∏≠‡∏ö 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Visual Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f01c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"‡∏™‡∏£‡πâ‡∏≤‡∏á Midjourney prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö TechZone\"\n",
    "input_text = \"\"\"Brand: TechZone\n",
    "Context: Product showcase - Gaming Keyboard\n",
    "Visual Style: dark cyberpunk, neon RGB lighting\n",
    "Aspect Ratio: 16:9\"\"\"\n",
    "\n",
    "print(\"üìù Instruction:\", instruction)\n",
    "print(\"\\nüí¨ Input:\", input_text)\n",
    "print(\"\\nü§ñ AI Director Output:\")\n",
    "print(\"=\" * 50)\n",
    "print(generate_content(instruction, input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ea043",
   "metadata": {},
   "source": [
    "### ‡∏ó‡∏î‡∏™‡∏≠‡∏ö 3: ‡∏ï‡∏≠‡∏ö Customer Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5157651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤\"\n",
    "input_text = \"\"\"Brand: CoffeeLab\n",
    "Tone: friendly, warm\n",
    "Comment: ‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥‡πÄ‡∏õ‡∏£‡∏µ‡πâ‡∏¢‡∏ß‡∏°‡∏≤‡∏Å ‡πÑ‡∏°‡πà‡∏≠‡∏£‡πà‡∏≠‡∏¢‡πÄ‡∏•‡∏¢\n",
    "Sentiment: negative\"\"\"\n",
    "\n",
    "print(\"üìù Instruction:\", instruction)\n",
    "print(\"\\nüí¨ Input:\", input_text)\n",
    "print(\"\\nü§ñ AI Director Output:\")\n",
    "print(\"=\" * 50)\n",
    "print(generate_content(instruction, input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6f54f",
   "metadata": {},
   "source": [
    "## üìå Step 10: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏µ‡πà Google Drive\n",
    "\n",
    "Copy ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ Drive ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy ‡πÇ‡∏°‡πÄ‡∏î‡∏• + logs ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ Drive\n",
    "!cp -r /content/ai-director-colab/scripts/models /content/drive/MyDrive/ai-director-colab/ai-director-colab/\n",
    "!cp -r /content/ai-director-colab/scripts/logs /content/drive/MyDrive/ai-director-colab/ai-director-colab/\n",
    "\n",
    "print(\"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡∏ó‡∏µ‡πà Google Drive ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "print(\"\\nüìÇ Location:\")\n",
    "print(\"  - /content/drive/MyDrive/ai-director-colab/ai-director-colab/models/\")\n",
    "print(\"  - /content/drive/MyDrive/ai-director-colab/ai-director-colab/logs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e9a3b1",
   "metadata": {},
   "source": [
    "## üéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n",
    "\n",
    "### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ:\n",
    "- ‚úÖ LoRA adapters ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\n",
    "- ‚úÖ Training logs\n",
    "- ‚úÖ Evaluation results\n",
    "- ‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
    "\n",
    "### Next Steps:\n",
    "1. ‡πÉ‡∏ä‡πâ `inference_rag.py` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö production\n",
    "2. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô `brands_v2.json` (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á retrain!)\n",
    "3. Fine-tune ‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°\n",
    "\n",
    "### üìö Resources:\n",
    "- Full documentation: `/docs/README_V2.md`\n",
    "- Course material: `course_ai-assistant_v3.4.md`\n",
    "\n",
    "---\n",
    "\n",
    "**Version:** 2.0  \n",
    "**Date:** January 4, 2026"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
