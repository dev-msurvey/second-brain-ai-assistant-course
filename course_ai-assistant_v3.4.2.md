# üéì Self-Paced Course: Building Your AI Director (v3.4)

**Project Theme:** The AI Director ‚Äî One-Man Marketing Agency  
**Based on:** `decodingai-magazine/second-brain-ai-assistant-course`  
**Instructor:** GitHub Copilot (Directed by You)  
**Platform:** GitHub Codespaces (Learning) & Google Colab (Training)  
**Last Updated:** January 2026

---

## üé¨ AI Director Concept

> **"One Brain (Director), Many Hands (Tools)"**  
> ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà Chatbot ‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∑‡∏≠ **Head of Marketing** ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏¥‡∏ï‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

### What is AI Director?

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    üß† AI DIRECTOR ARCHITECTURE v3.4             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ                    ‚îÇ   AI DIRECTOR    ‚îÇ                         ‚îÇ
‚îÇ                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                         ‚îÇ
‚îÇ                    ‚îÇ  ‚îÇ  T5Gemma 2 ‚îÇ  ‚îÇ  ‚Üê Multimodal Brain     ‚îÇ
‚îÇ                    ‚îÇ  ‚îÇ  (4B-4B)   ‚îÇ  ‚îÇ    (Text + Image)       ‚îÇ
‚îÇ                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                         ‚îÇ
‚îÇ                    ‚îÇ        +         ‚îÇ                         ‚îÇ
‚îÇ                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                         ‚îÇ
‚îÇ                    ‚îÇ  ‚îÇ Function   ‚îÇ  ‚îÇ  ‚Üê Tool Orchestrator    ‚îÇ
‚îÇ                    ‚îÇ  ‚îÇ   Gemma    ‚îÇ  ‚îÇ    (270M, On-device)    ‚îÇ
‚îÇ                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                         ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îÇ                             ‚îÇ                                    ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ                   ‚îÇ               ‚îÇ
‚îÇ         ‚ñº                   ‚ñº                   ‚ñº               ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ   ‚îÇ STRATEGY  ‚îÇ      ‚îÇ  CONTENT  ‚îÇ      ‚îÇPRODUCTION ‚îÇ          ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§          ‚îÇ
‚îÇ   ‚îÇ‚Ä¢ Campaign ‚îÇ      ‚îÇ‚Ä¢ Script   ‚îÇ      ‚îÇ‚Ä¢ Image Gen‚îÇ          ‚îÇ
‚îÇ   ‚îÇ‚Ä¢ Audience ‚îÇ      ‚îÇ‚Ä¢ Prompts  ‚îÇ      ‚îÇ‚Ä¢ Voice Gen‚îÇ          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ‚Ä¢ Video    ‚îÇ          ‚îÇ
‚îÇ                                          ‚îÇ  Compose  ‚îÇ          ‚îÇ
‚îÇ                                          ‚îÇ‚Ä¢ Smart Cut‚îÇ          ‚îÇ
‚îÇ                                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê      ‚îÇ
‚îÇ   AI Director: ‡∏£‡∏±‡∏ö‡πÇ‡∏à‡∏ó‡∏¢‡πå ‚Üí ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô ‚Üí ‡∏™‡∏£‡πâ‡∏≤‡∏á/‡∏ï‡∏±‡∏î‡∏ï‡πà‡∏≠ ‚Üí ‡∏™‡πà‡∏á‡∏°‡∏≠‡∏ö          ‚îÇ
‚îÇ   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê      ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üí∞ Zero Cost Strategy Edition

> **"‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏°‡πâ‡πÅ‡∏ï‡πà‡∏ö‡∏≤‡∏ó‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"** - ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏ö‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡∏π‡∏Å‡∏ö‡∏±‡∏ï‡∏£‡πÄ‡∏Ñ‡∏£‡∏î‡∏¥‡∏ï

---

## üÜï What's New in v3.4

| ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á | ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î |
|---------|-----------|
| üìö **Comprehensive Documentation** | ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å component |
| üîó **Official Links** | Links ‡πÑ‡∏õ‡∏¢‡∏±‡∏á official documentation |
| üìñ **Tutorials & Examples** | ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á notebooks ‡πÅ‡∏•‡∏∞ tutorials |
| üáπüá≠ **Thai Language Guide** | Best practices ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ |
| üéØ **Quick Reference** | ‡∏™‡∏£‡∏∏‡∏õ API, parameters, best practices |

### v3.4 vs v3.3 Comparison

| Feature | v3.3 | v3.4 |
|---------|------|------|
| Smart Cut | ‚úÖ | ‚úÖ |
| Documentation | Basic | **Comprehensive** |
| Official Links | ‚ùå | ‚úÖ **NEW** |
| Example Notebooks | ‚ùå | ‚úÖ **NEW** |
| Thai Language Guide | ‚ùå | ‚úÖ **NEW** |
| Troubleshooting | ‚ùå | ‚úÖ **NEW** |

---

## ‚úÖ Zero Cost Stack (Updated v3.4)

| ‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö | ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ (‡∏ü‡∏£‡∏µ 100%) | ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏ |
|-----------|---------------------|---------|
| **Compute** | GitHub Codespaces (2-core) | 120 ‡∏ä‡∏°./‡πÄ‡∏î‡∏∑‡∏≠‡∏ô |
| **Database** | MongoDB Atlas (M0) | Singapore, 512MB |
| **Vector DB** | ChromaDB (Local) | ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô Codespace |
| **GPU (Train)** | Google Colab (Free T4) | Save ‡∏•‡∏á Drive |
| **LLM API** | Gemini 1.5 Flash | Google AI Studio |
| **Thinker Model** | T5Gemma 2 (1B-1B) | HF Inference API |
| **Executor Model** | FunctionGemma (270M) | Run locally |
| **Optimization** | Meta Ax | Bayesian hyperparameter tuning |
| **Image Gen** | HF Inference API | SDXL/Flux |
| **Voice Gen** | Edge-TTS | Microsoft Edge voices |
| **Video Compose** | MoviePy | Python library |
| **Smart Cut** | FFmpeg + Whisper | ‡∏ï‡∏±‡∏î‡∏ï‡πà‡∏≠‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ |
| **Transcription** | Whisper (tiny/base) | ‡∏£‡∏±‡∏ô CPU ‡πÑ‡∏î‡πâ |

---

## üìÖ Course Syllabus

### üó∫Ô∏è Learning Path

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 AI DIRECTOR LEARNING PATH v3.4                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ   FOUNDATION                                                     ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ   ‚îÇModule 1 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇModule 2 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇModule 3 ‚îÇ                    ‚îÇ
‚îÇ   ‚îÇDual-Arch‚îÇ    ‚îÇ  ETL    ‚îÇ    ‚îÇDataset+ ‚îÇ                    ‚îÇ
‚îÇ   ‚îÇ Design  ‚îÇ    ‚îÇ         ‚îÇ    ‚îÇPrompts  ‚îÇ                    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                                       ‚îÇ                         ‚îÇ
‚îÇ   CORE                                ‚ñº                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ   ‚îÇModule 4 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇMod. 4.5 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇModule 5 ‚îÇ                    ‚îÇ
‚îÇ   ‚îÇFine-tune‚îÇ    ‚îÇ Meta Ax ‚îÇ    ‚îÇ  RAG    ‚îÇ                    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                                       ‚îÇ                         ‚îÇ
‚îÇ   PRODUCTION                          ‚ñº                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ   ‚îÇModule 6 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇMod. 6.5 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇModule 7 ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇModule 8 ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ Tools   ‚îÇ    ‚îÇSmart Cut‚îÇ    ‚îÇ  Agent  ‚îÇ    ‚îÇ Deploy  ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ Setup   ‚îÇ    ‚îÇ         ‚îÇ    ‚îÇ         ‚îÇ    ‚îÇ         ‚îÇ    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                       ‚îÇ                         ‚îÇ
‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ                              ‚îÇ  FINAL PROJECT  ‚îÇ                ‚îÇ
‚îÇ                              ‚îÇ  Full Pipeline  ‚îÇ                ‚îÇ
‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

# üìö COMPREHENSIVE DOCUMENTATION & REFERENCES

> **v3.4 ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å component ‡∏û‡∏£‡πâ‡∏≠‡∏° links ‡πÅ‡∏•‡∏∞ tutorials**

---

## üß† Models & AI Documentation

### T5Gemma 2 (Thinker Model)

**Overview:**
T5Gemma 2 ‡πÄ‡∏õ‡πá‡∏ô encoder-decoder model ‡∏à‡∏≤‡∏Å Google ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö multimodal (text + image) ‡πÅ‡∏•‡∏∞ long context (128K tokens) ‡πÉ‡∏ô 140+ ‡∏†‡∏≤‡∏©‡∏≤

**Available Sizes:**

| Model | Total Params | VRAM | Model ID |
|-------|--------------|------|----------|
| 270M-270M | ~370M | ~1GB | `google/t5gemma-2-270m-270m` |
| 1B-1B | ~1.7B | ~4GB | `google/t5gemma-2-1b-1b` |
| 4B-4B | ~7B | ~16GB | `google/t5gemma-2-4b-4b` |

**Key Features:**
- Multimodal: ‡∏£‡∏±‡∏ö text + image input
- Long Context: 128K token window
- Multilingual: 140+ languages ‡∏£‡∏ß‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
- Tied embeddings: ‡∏•‡∏î parameters
- Merged attention: ‡∏£‡∏ß‡∏° self + cross attention

**Official Documentation:**

| Resource | Link |
|----------|------|
| üìÑ Model Card (270M) | https://huggingface.co/google/t5gemma-2-270m-270m |
| üìÑ Model Card (1B) | https://huggingface.co/google/t5gemma-2-1b-1b |
| üìÑ Model Card (4B) | https://huggingface.co/google/t5gemma-2-4b-4b |
| üìñ HF Transformers Doc | https://huggingface.co/docs/transformers/model_doc/t5gemma2 |
| üì∞ Google Blog | https://blog.google/technology/developers/t5gemma-2/ |
| üì¶ HF Collection | https://huggingface.co/collections/google/t5gemma-2 |
| üìù ArXiv Paper | Search "T5Gemma 2: Seeing, Reading, and Understanding Longer" |

**Quick Start Code:**

```python
from transformers import AutoProcessor, AutoModelForSeq2SeqLM
import requests
from PIL import Image

# Load model and processor
processor = AutoProcessor.from_pretrained("google/t5gemma-2-1b-1b")
model = AutoModelForSeq2SeqLM.from_pretrained("google/t5gemma-2-1b-1b")

# Text-only example
text = "Translate to Thai: Hello, how are you?"
inputs = processor(text=text, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=50)
print(processor.decode(outputs[0]))

# Multimodal example (text + image)
url = "https://example.com/image.jpg"
image = Image.open(requests.get(url, stream=True).raw)
prompt = "<start_of_image> Describe this image in detail"
inputs = processor(text=prompt, images=image, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)
print(processor.decode(outputs[0]))
```

---

### FunctionGemma (Executor Model)

**Overview:**
FunctionGemma ‡πÄ‡∏õ‡πá‡∏ô 270M model ‡∏à‡∏≤‡∏Å Google ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö function/tool calling ‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏ö‡∏ô edge devices

**Key Features:**
- **Tiny but Powerful:** 270M params, ‡∏£‡∏±‡∏ô CPU ‡πÑ‡∏î‡πâ
- **Tool Calling Expert:** Natural language ‚Üí JSON
- **Fine-tune Friendly:** 58% ‚Üí 85% accuracy after fine-tuning
- **On-device Ready:** 550MB RAM

**Official Documentation:**

| Resource | Link |
|----------|------|
| üìÑ Model Card | https://huggingface.co/google/functiongemma-270m-it |
| üìÑ Unsloth GGUF | https://huggingface.co/unsloth/functiongemma-270m-it-GGUF |
| üì∞ Google Blog | https://blog.google/technology/developers/functiongemma/ |
| üìñ Unsloth Guide | https://docs.unsloth.ai/models/functiongemma |
| üéÆ Ollama | https://ollama.com/library/functiongemma |
| üìñ LM Studio Guide | https://lmstudio.ai/blog/functiongemma-unsloth |

**Fine-Tuning Notebooks (Unsloth):**

| Notebook | Description |
|----------|-------------|
| [Reason Before Tool Calling](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/FunctionGemma_(270M)-Reasoning.ipynb) | Fine-tune to "think" before calling |
| [Mobile Actions](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/FunctionGemma_(270M)-MobileActions.ipynb) | Android system actions |
| [Multi-Turn Tool Calling](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/FunctionGemma_(270M)-MultiTurn.ipynb) | Chain tool calls |
| [LM Studio Export](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/FunctionGemma_(270M)-LMStudio.ipynb) | Fine-tune + export to GGUF |

**Tool Definition Format:**

```python
def get_weather(city: str) -> str:
    """
    Get the current weather for a city.
    
    Args:
        city: The name of the city
        
    Returns:
        A string describing the weather
    """
    return json.dumps({'city': city, 'temperature': 22, 'unit': 'celsius'})

# Apply chat template
tokenizer.apply_chat_template(
    [{"role": "user", "content": "What's the weather in Bangkok?"}],
    tools=[get_weather],
    add_generation_prompt=True,
    tokenize=False,
)
```

**Output Format:**

```
<start_function_call>call:get_weather{city:<escape>Bangkok<escape>}<end_function_call>
```

---

### Gemini API (Dataset Generation)

**Overview:**
Gemini API ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö generate training datasets ‡πÅ‡∏•‡∏∞ fallback inference

**Free Tier Limits (Dec 2025):**

| Model | RPM | TPM | RPD |
|-------|-----|-----|-----|
| Gemini 2.5 Pro | 2 | 250K | 50 |
| Gemini 2.5 Flash | 10 | 250K | 500 |
| Gemini 2.5 Flash-Lite | 15 | 250K | 1,000 |

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† API Homepage | https://ai.google.dev/gemini-api/docs |
| üöÄ Quickstart | https://ai.google.dev/gemini-api/docs/quickstart |
| üìã Models List | https://ai.google.dev/gemini-api/docs/models |
| üí∞ Pricing | https://ai.google.dev/pricing |
| üîë Get API Key | https://aistudio.google.com/apikey |

**Quick Start Code:**

```python
from google import genai

# Initialize client (API key from environment)
client = genai.Client()

# Simple generation
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Explain AI Director concept in Thai"
)
print(response.text)

# With system instruction
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Generate a marketing brief for coffee shop",
    config={
        "system_instruction": "You are a Thai marketing expert.",
        "temperature": 0.7,
        "max_output_tokens": 1000
    }
)
```

---

### Meta Ax (Hyperparameter Optimization)

**Overview:**
Ax ‡πÄ‡∏õ‡πá‡∏ô open-source Bayesian optimization platform ‡∏à‡∏≤‡∏Å Meta ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hyperparameter tuning

**Key Features:**
- **Bayesian Optimization:** ‡πÉ‡∏ä‡πâ Gaussian Process
- **Multi-objective:** Optimize ‡∏´‡∏•‡∏≤‡∏¢ metrics ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
- **Built on BoTorch:** PyTorch-based
- **Production Ready:** ‡πÉ‡∏ä‡πâ‡∏ó‡∏µ‡πà Meta scale

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† Ax Website | https://ax.dev/ |
| üì¶ GitHub | https://github.com/facebook/Ax |
| üìñ Tutorials | https://ax.dev/tutorials/ |
| üìÑ BoTorch | https://botorch.org/docs/introduction/ |
| üì∞ Meta Blog | https://engineering.fb.com/2025/11/18/open-source/efficient-optimization-ax-open-platform-adaptive-experimentation/ |

**Installation:**

```bash
pip install ax-platform botorch
```

**Quick Start Code:**

```python
from ax import Client, RangeParameterConfig

# Initialize client
client = Client()

# Define search space
client.configure_experiment(
    parameters=[
        RangeParameterConfig(name="learning_rate", bounds=(1e-5, 1e-3), parameter_type="float"),
        RangeParameterConfig(name="batch_size", bounds=(4, 32), parameter_type="int"),
        RangeParameterConfig(name="lora_r", bounds=(4, 32), parameter_type="int"),
        RangeParameterConfig(name="epochs", bounds=(1, 5), parameter_type="int"),
    ],
)

# Define objective (minimize loss)
client.configure_optimization(objective="-1 * eval_loss")

# Run optimization loop
for _ in range(20):
    for trial_index, parameters in client.get_next_trials(max_trials=1).items():
        # Train model with these parameters
        eval_loss = train_model(
            learning_rate=parameters["learning_rate"],
            batch_size=parameters["batch_size"],
            lora_r=parameters["lora_r"],
            epochs=parameters["epochs"]
        )
        
        # Report result
        client.complete_trial(
            trial_index=trial_index,
            raw_data={"eval_loss": eval_loss}
        )

# Get best parameters
best_params = client.get_best_parameterization()
print(f"Best parameters: {best_params}")
```

**Multi-Objective Example:**

```python
from ax.service.ax_client import AxClient
from ax.service.utils.instantiation import ObjectiveProperties

ax_client = AxClient()
ax_client.create_experiment(
    parameters=[
        {"name": "learning_rate", "type": "range", "bounds": [1e-5, 1e-3]},
        {"name": "lora_r", "type": "range", "bounds": [4, 32]},
    ],
    objectives={
        "eval_loss": ObjectiveProperties(minimize=True),
        "inference_speed": ObjectiveProperties(minimize=False),
        "memory_usage": ObjectiveProperties(minimize=True),
    },
)
```

---

### LoRA/PEFT (Efficient Fine-Tuning)

**Overview:**
LoRA (Low-Rank Adaptation) ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ parameter-efficient fine-tuning ‡∏ó‡∏µ‡πà train ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ small adapter weights

**Key Benefits:**
- **Memory Efficient:** ‡∏•‡∏î trainable params ~90%
- **Fast Training:** ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ full fine-tuning ‡∏°‡∏≤‡∏Å
- **Small Checkpoints:** Adapter weights ~10-50MB
- **No Inference Latency:** Merge weights ‡πÑ‡∏î‡πâ

**Official Documentation:**

| Resource | Link |
|----------|------|
| üì¶ PEFT GitHub | https://github.com/huggingface/peft |
| üìñ PEFT Docs | https://huggingface.co/docs/peft |
| üìñ LoRA Guide | https://huggingface.co/docs/peft/main/en/conceptual_guides/lora |
| üìñ Transformers PEFT | https://huggingface.co/docs/transformers/en/peft |
| üì∞ PEFT Blog | https://huggingface.co/blog/peft |
| üìñ Smol Course | https://huggingface.co/learn/smol-course/en/unit1/3a |

**Key Parameters:**

| Parameter | Description | Recommended |
|-----------|-------------|-------------|
| `r` | Rank of update matrices | 8-16 |
| `lora_alpha` | Scaling factor | 16-32 |
| `target_modules` | Which layers to adapt | `q_proj, k_proj, v_proj, o_proj` |
| `lora_dropout` | Dropout for LoRA layers | 0.05-0.1 |
| `bias` | Whether to train bias | `none` |

**Quick Start Code:**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType

# Load base model
model = AutoModelForCausalLM.from_pretrained("google/gemma-2-2b")
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-2b")

# Configure LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
)

# Apply LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Output: trainable params: 4,194,304 || all params: 2,506,172,416 || trainable%: 0.1673

# Train with HuggingFace Trainer
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./lora_model",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=2e-4,
    fp16=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)
trainer.train()

# Save adapter only (~50MB)
model.save_pretrained("./lora_adapter")

# Load and merge for inference
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained("google/gemma-2-2b")
model = PeftModel.from_pretrained(base_model, "./lora_adapter")
model = model.merge_and_unload()  # Merge weights
```

**QLoRA (4-bit) for Low VRAM:**

```python
from transformers import BitsAndBytesConfig
import torch

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

# Load quantized model
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2-2b",
    quantization_config=bnb_config,
    device_map="auto",
)

# Apply LoRA on quantized model
model = get_peft_model(model, lora_config)
```

---

## üóÑÔ∏è Data & Storage Documentation

### MongoDB Atlas (Free Tier)

**Overview:**
MongoDB Atlas M0 ‡πÉ‡∏´‡πâ cloud database ‡∏ü‡∏£‡∏µ 512MB ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö brand data, campaigns, transcripts

**Free Tier Specs:**
- Storage: 512MB
- Connections: 100 max
- RAM: Shared
- vCPU: Shared
- Regions: AWS, GCP, Azure

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† MongoDB Atlas | https://www.mongodb.com/atlas |
| üìñ Getting Started | https://www.mongodb.com/docs/atlas/getting-started/ |
| üìñ Deploy Free Cluster | https://www.mongodb.com/docs/atlas/tutorial/deploy-free-tier-cluster/ |
| üìñ M0 Limits | https://www.mongodb.com/docs/atlas/reference/free-shared-limitations/ |
| üìñ PyMongo Docs | https://pymongo.readthedocs.io/ |

**Setup Steps:**

1. ‡∏™‡∏£‡πâ‡∏≤‡∏á account ‡∏ó‡∏µ‡πà https://www.mongodb.com/cloud/atlas
2. Create Free Cluster (M0)
3. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Region: Singapore (ap-southeast-1)
4. Whitelist IP: 0.0.0.0/0 (for development)
5. Create Database User
6. Get Connection String

**Quick Start Code:**

```python
from pymongo import MongoClient
from datetime import datetime

# Connect to MongoDB Atlas
MONGO_URI = "mongodb+srv://user:password@cluster.mongodb.net/"
client = MongoClient(MONGO_URI)
db = client["ai_director"]

# Collections
brands = db["brands"]
campaigns = db["campaigns"]
transcripts = db["transcripts"]

# Insert brand data
brand_data = {
    "name": "CoffeeLab",
    "tone": "friendly, premium, modern",
    "colors": ["#8B4513", "#F5F5DC", "#2C1810"],
    "target_audience": "young professionals 25-35",
    "created_at": datetime.now()
}
brands.insert_one(brand_data)

# Query
brand = brands.find_one({"name": "CoffeeLab"})
print(brand)
```

**Schema Design for AI Director:**

```python
# Brand Schema
{
    "_id": ObjectId,
    "name": str,
    "description": str,
    "tone": str,
    "colors": [str],
    "fonts": [str],
    "target_audience": str,
    "brand_values": [str],
    "created_at": datetime,
    "updated_at": datetime
}

# Campaign Schema
{
    "_id": ObjectId,
    "brand_id": ObjectId,
    "name": str,
    "brief": str,
    "generated_prompts": [str],
    "assets": [{
        "type": "image|video|audio",
        "url": str,
        "metadata": dict
    }],
    "status": "draft|active|completed",
    "created_at": datetime
}

# Transcript Schema
{
    "_id": ObjectId,
    "video_id": str,
    "filename": str,
    "duration": float,
    "language": str,
    "segments": [{
        "start": float,
        "end": float,
        "text": str,
        "words": [{
            "word": str,
            "start": float,
            "end": float,
            "confidence": float
        }]
    }],
    "silence_regions": [{
        "start": float,
        "end": float,
        "duration": float
    }],
    "created_at": datetime
}
```

---

### ChromaDB (Vector Database)

**Overview:**
ChromaDB ‡πÄ‡∏õ‡πá‡∏ô open-source vector database ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAG ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡πá‡∏ö embeddings ‡∏Ç‡∏≠‡∏á brand knowledge ‡πÅ‡∏•‡∏∞ prompt templates

**Key Features:**
- In-memory ‡∏´‡∏£‡∏∑‡∏≠ persistent storage
- Built-in embedding functions
- Metadata filtering
- Python-native

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† ChromaDB | https://www.trychroma.com/ |
| üì¶ GitHub | https://github.com/chroma-core/chroma |
| üìñ Documentation | https://docs.trychroma.com/ |
| üìñ LangChain Integration | https://python.langchain.com/docs/integrations/vectorstores/chroma |
| üìñ Real Python Tutorial | https://realpython.com/chromadb-vector-database/ |

**Installation:**

```bash
pip install chromadb
```

**Quick Start Code:**

```python
import chromadb
from chromadb.utils import embedding_functions

# Create persistent client
client = chromadb.PersistentClient(path="./chroma_db")

# Use sentence-transformers for embeddings
embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# Create collection
collection = client.get_or_create_collection(
    name="brand_knowledge",
    embedding_function=embedding_fn,
    metadata={"hnsw:space": "cosine"}
)

# Add documents
collection.add(
    documents=[
        "CoffeeLab ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå‡∏Å‡∏≤‡πÅ‡∏ü‡∏û‡∏£‡∏µ‡πÄ‡∏°‡∏µ‡∏¢‡∏° ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ô‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà",
        "‡πÇ‡∏ó‡∏ô‡∏™‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏Ñ‡∏∑‡∏≠‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏•‡πÄ‡∏Ç‡πâ‡∏° ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏µ‡∏° ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô",
        "Target ‡∏Ñ‡∏∑‡∏≠ young professionals ‡∏≠‡∏≤‡∏¢‡∏∏ 25-35 ‡∏õ‡∏µ",
    ],
    metadatas=[
        {"category": "brand", "type": "description"},
        {"category": "brand", "type": "colors"},
        {"category": "brand", "type": "audience"},
    ],
    ids=["doc1", "doc2", "doc3"]
)

# Query
results = collection.query(
    query_texts=["‡πÉ‡∏Ñ‡∏£‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á CoffeeLab"],
    n_results=2,
    where={"category": "brand"}
)
print(results)
```

**RAG Pattern:**

```python
class BrandKnowledgeRAG:
    def __init__(self, collection):
        self.collection = collection
    
    def get_context(self, query: str, n_results: int = 3) -> str:
        results = self.collection.query(
            query_texts=[query],
            n_results=n_results
        )
        
        context = "\n".join(results["documents"][0])
        return context
    
    def generate_with_context(self, query: str, llm) -> str:
        context = self.get_context(query)
        
        prompt = f"""Use the following context to answer the question.

Context:
{context}

Question: {query}

Answer:"""
        
        return llm.generate(prompt)
```

---

### Hugging Face Datasets

**Overview:**
‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prepare ‡πÅ‡∏•‡∏∞ format training datasets

**Official Documentation:**

| Resource | Link |
|----------|------|
| üì¶ GitHub | https://github.com/huggingface/datasets |
| üìñ Documentation | https://huggingface.co/docs/datasets |
| üìñ Loading Datasets | https://huggingface.co/docs/datasets/loading |
| üìñ Process Data | https://huggingface.co/docs/datasets/process |

**Quick Start Code:**

```python
from datasets import Dataset, DatasetDict

# Create dataset from dict
train_data = {
    "instruction": [
        "‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏Å‡∏≤‡πÅ‡∏ü",
        "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå target audience",
    ],
    "input": [
        "‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå: CoffeeLab, ‡∏™‡πÑ‡∏ï‡∏•‡πå: minimal",
        "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤: Cold Brew Premium",
    ],
    "output": [
        "A minimalist coffee cup on marble surface...",
        "‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡∏Ñ‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏® ‡∏≠‡∏≤‡∏¢‡∏∏ 25-35...",
    ],
}

dataset = Dataset.from_dict(train_data)

# Split dataset
dataset = dataset.train_test_split(test_size=0.1)

# Save to disk
dataset.save_to_disk("./ai_director_dataset")

# Load from disk
loaded_dataset = DatasetDict.load_from_disk("./ai_director_dataset")
```

---

## üé® Production Tools Documentation

### Image Generation (HF Inference API)

**Overview:**
‡πÉ‡∏ä‡πâ Hugging Face Inference API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö generate images ‡∏î‡πâ‡∏ß‡∏¢ SDXL/Flux

**Official Documentation:**

| Resource | Link |
|----------|------|
| üìñ Inference API | https://huggingface.co/docs/api-inference |
| üìñ SDXL Model | https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0 |
| üìñ Flux Models | https://huggingface.co/black-forest-labs |

**Quick Start Code:**

```python
import requests
import io
from PIL import Image

HF_TOKEN = "your_token_here"
API_URL = "https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-xl-base-1.0"

def generate_image(prompt: str, negative_prompt: str = "") -> Image.Image:
    headers = {"Authorization": f"Bearer {HF_TOKEN}"}
    
    payload = {
        "inputs": prompt,
        "parameters": {
            "negative_prompt": negative_prompt,
            "num_inference_steps": 30,
            "guidance_scale": 7.5,
        }
    }
    
    response = requests.post(API_URL, headers=headers, json=payload)
    
    if response.status_code == 200:
        image = Image.open(io.BytesIO(response.content))
        return image
    else:
        raise Exception(f"Error: {response.status_code}")

# Usage
image = generate_image(
    prompt="A premium coffee cup on marble surface, soft morning light, minimal style, 4K",
    negative_prompt="blurry, low quality, watermark"
)
image.save("coffee_ad.png")
```

---

### Edge-TTS (Voice Generation)

**Overview:**
Edge-TTS ‡πÉ‡∏ä‡πâ Microsoft Edge's TTS service ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏û‡∏≤‡∏Å‡∏¢‡πå ‡∏ü‡∏£‡∏µ 100%

**Key Features:**
- Neural voices ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á
- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤ ‡∏£‡∏ß‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
- ‡∏õ‡∏£‡∏±‡∏ö rate, pitch, volume ‡πÑ‡∏î‡πâ
- ‡∏™‡∏£‡πâ‡∏≤‡∏á subtitles (.srt/.vtt) ‡πÑ‡∏î‡πâ

**Official Documentation:**

| Resource | Link |
|----------|------|
| üì¶ PyPI | https://pypi.org/project/edge-tts/ |
| üì¶ GitHub | https://github.com/rany2/edge-tts |

**Thai Voices:**

| Voice | Gender | Style |
|-------|--------|-------|
| `th-TH-PremwadeeNeural` | Female | Friendly, Positive |
| `th-TH-NiwatNeural` | Male | Friendly, Positive |

**Installation:**

```bash
pip install edge-tts
```

**Quick Start Code:**

```python
import edge_tts
import asyncio

async def generate_voice(text: str, voice: str, output_file: str):
    """Generate voice from text using Edge TTS"""
    communicate = edge_tts.Communicate(
        text=text,
        voice=voice,
        rate="+0%",      # -50% to +100%
        volume="+0%",    # -50% to +100%
        pitch="+0Hz"     # -50Hz to +50Hz
    )
    
    await communicate.save(output_file)
    print(f"Saved to {output_file}")

# Usage
text = "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡πÅ‡∏ü Cold Brew ‡πÉ‡∏´‡∏°‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å CoffeeLab"
asyncio.run(generate_voice(
    text=text,
    voice="th-TH-NiwatNeural",
    output_file="voiceover.mp3"
))

# Generate with subtitles
async def generate_with_subtitles(text: str, voice: str, audio_file: str, subtitle_file: str):
    communicate = edge_tts.Communicate(text, voice)
    
    submaker = edge_tts.SubMaker()
    
    with open(audio_file, "wb") as audio:
        async for chunk in communicate.stream():
            if chunk["type"] == "audio":
                audio.write(chunk["data"])
            elif chunk["type"] == "WordBoundary":
                submaker.feed(chunk)
    
    with open(subtitle_file, "w", encoding="utf-8") as srt:
        srt.write(submaker.generate_srt())

asyncio.run(generate_with_subtitles(
    text=text,
    voice="th-TH-NiwatNeural",
    audio_file="voiceover.mp3",
    subtitle_file="voiceover.srt"
))
```

**List Available Voices:**

```bash
edge-tts --list-voices | grep th-TH
```

---

### MoviePy (Video Composition)

**Overview:**
MoviePy ‡πÄ‡∏õ‡πá‡∏ô Python library ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö video editing: cuts, concatenations, text overlays, compositing

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† Documentation | https://zulko.github.io/moviepy/ |
| üì¶ GitHub | https://github.com/Zulko/moviepy |
| üì¶ PyPI | https://pypi.org/project/moviepy/ |
| üìñ User Guide | https://zulko.github.io/moviepy/user_guide/index.html |

**Installation:**

```bash
pip install moviepy

# Also need FFmpeg (usually auto-installed)
# Ubuntu: sudo apt-get install ffmpeg
# Mac: brew install ffmpeg
```

**Quick Start Code:**

```python
from moviepy import VideoFileClip, AudioFileClip, TextClip, CompositeVideoClip, concatenate_videoclips

# Load video
clip = VideoFileClip("raw_video.mp4")

# Basic operations
clip = clip.subclipped(10, 30)  # Cut from 10s to 30s
clip = clip.with_volume_scaled(0.8)  # Reduce volume to 80%
clip = clip.resized(width=1080)  # Resize

# Add text overlay
txt_clip = TextClip(
    text="CoffeeLab - Premium Coffee",
    font="Arial.ttf",
    font_size=70,
    color='white',
    bg_color='black',
    size=(1080, None)
)
txt_clip = txt_clip.with_position('center').with_duration(5)

# Composite
final = CompositeVideoClip([clip, txt_clip])

# Add audio
audio = AudioFileClip("voiceover.mp3")
final = final.with_audio(audio)

# Export
final.write_videofile(
    "output.mp4",
    fps=30,
    codec="libx264",
    audio_codec="aac"
)

# Concatenate multiple clips
clips = [
    VideoFileClip("intro.mp4"),
    VideoFileClip("content.mp4"),
    VideoFileClip("outro.mp4"),
]
final = concatenate_videoclips(clips, method="compose")
final.write_videofile("full_video.mp4")
```

**Common Operations:**

```python
# Fade in/out
clip = clip.with_effects([vfx.FadeIn(1), vfx.FadeOut(1)])

# Speed up/slow down
clip = clip.with_speed_scaled(1.5)  # 1.5x speed

# Crop
clip = clip.cropped(x1=100, y1=100, x2=800, y2=600)

# Rotate
clip = clip.rotated(90)

# Mirror
clip = clip.with_effects([vfx.MirrorX()])
```

---

### Pillow (Image Processing)

**Overview:**
Pillow ‡πÄ‡∏õ‡πá‡∏ô Python library ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö image processing

**Official Documentation:**

| Resource | Link |
|----------|------|
| üìñ Documentation | https://pillow.readthedocs.io/ |
| üì¶ GitHub | https://github.com/python-pillow/Pillow |
| üì¶ PyPI | https://pypi.org/project/pillow/ |

**Quick Start Code:**

```python
from PIL import Image, ImageDraw, ImageFont, ImageFilter

# Open image
img = Image.open("input.jpg")

# Resize
img = img.resize((1080, 1080))

# Add text
draw = ImageDraw.Draw(img)
font = ImageFont.truetype("Arial.ttf", 60)
draw.text((100, 100), "CoffeeLab", fill="white", font=font)

# Apply filter
img = img.filter(ImageFilter.GaussianBlur(radius=2))

# Save
img.save("output.jpg", quality=95)
```

---

## ‚òÅÔ∏è Infrastructure Documentation

### GitHub Codespaces

**Overview:**
GitHub Codespaces ‡πÉ‡∏´‡πâ cloud development environment ‡∏ü‡∏£‡∏µ 120 ‡∏ä‡∏°./‡πÄ‡∏î‡∏∑‡∏≠‡∏ô

**Free Tier Limits:**
- 120 core-hours/month
- 15GB storage/month
- 2-core, 8GB RAM default

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† Codespaces | https://github.com/features/codespaces |
| üìñ Quickstart | https://docs.github.com/en/codespaces/getting-started/quickstart |
| üìñ Billing | https://docs.github.com/en/billing/managing-billing-for-github-codespaces |

**Tips:**
1. ‡πÉ‡∏ä‡πâ 2-core machine (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î hours)
2. Stop codespace ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ
3. Set auto-stop timeout (30 mins)
4. ‡πÉ‡∏ä‡πâ `.devcontainer.json` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö setup

**devcontainer.json Example:**

```json
{
  "name": "AI Director",
  "image": "mcr.microsoft.com/devcontainers/python:3.11",
  "postCreateCommand": "pip install -r requirements.txt",
  "customizations": {
    "vscode": {
      "extensions": [
        "ms-python.python",
        "GitHub.copilot"
      ]
    }
  }
}
```

---

### Google Colab

**Overview:**
Google Colab ‡πÉ‡∏´‡πâ free GPU (T4) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö training

**Free Tier:**
- GPU: Tesla T4 (15GB VRAM)
- Runtime: ~12 hours max
- Storage: Save to Google Drive

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† Colab | https://colab.research.google.com/ |
| üìñ FAQ | https://research.google.com/colaboratory/faq.html |

**Tips:**

```python
# Check GPU
!nvidia-smi

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Save model to Drive
model.save_pretrained('/content/drive/MyDrive/models/ai_director')

# Install packages
!pip install transformers peft accelerate bitsandbytes

# Keep session alive (run in separate cell)
import time
while True:
    time.sleep(60)
```

---

### Hugging Face Spaces (Deployment)

**Overview:**
HF Spaces ‡πÉ‡∏´‡πâ free hosting ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Gradio apps

**Free Tier:**
- CPU: 2 vCPU
- RAM: 16GB
- Storage: Ephemeral
- Auto-sleep: After inactivity

**Official Documentation:**

| Resource | Link |
|----------|------|
| üè† Spaces | https://huggingface.co/spaces |
| üìñ Gradio Spaces | https://huggingface.co/docs/hub/en/spaces-sdks-gradio |
| üìñ Using HF Integrations | https://www.gradio.app/guides/using-hugging-face-integrations |

**Quick Deploy:**

1. Create new Space at https://huggingface.co/new-space
2. Select Gradio SDK
3. Create `app.py` and `requirements.txt`
4. Push files

**app.py Example:**

```python
import gradio as gr
from transformers import pipeline

# Load model
pipe = pipeline("text-generation", model="your-model")

def generate(prompt):
    result = pipe(prompt, max_length=200)
    return result[0]["generated_text"]

# Create interface
demo = gr.Interface(
    fn=generate,
    inputs=gr.Textbox(label="Brief", lines=5),
    outputs=gr.Textbox(label="Output", lines=10),
    title="AI Director",
    description="Generate marketing content"
)

if __name__ == "__main__":
    demo.launch()
```

**requirements.txt:**

```
gradio
transformers
torch
```

---

## üìñ Tutorials & Example Notebooks

### End-to-End RAG Examples

| Notebook | Description | Link |
|----------|-------------|------|
| RAG with ChromaDB | Basic RAG implementation | [DataCamp Tutorial](https://www.datacamp.com/tutorial/chromadb-tutorial-step-by-step-guide) |
| RAG with Ollama | Local LLM RAG | [Medium Article](https://medium.com/@arunpatidar26/rag-chromadb-ollama-python-guide-for-beginners-30857499d0a0) |
| LlamaIndex + ChromaDB | Production RAG | [Dev.to Tutorial](https://dev.to/sophyia/how-to-build-a-rag-solution-with-llama-index-chromadb-and-ollama-20lb) |
| LangChain + Chroma | LangChain integration | [LangChain Docs](https://docs.langchain.com/oss/python/integrations/vectorstores/chroma) |

### LoRA Fine-Tuning Notebooks

| Notebook | Description | Link |
|----------|-------------|------|
| Fine-tune LLM with PEFT | Comprehensive guide | [HF Blog](https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face) |
| QLoRA on T4 | Free Colab training | [Phil Schmid](https://www.philschmid.de/fine-tune-llms-in-2025) |
| FunctionGemma Fine-tune | Tool calling | [Unsloth Notebooks](https://docs.unsloth.ai/models/functiongemma) |
| Gemma 3 Fine-tune | Vision + Text | [Unsloth Guide](https://docs.unsloth.ai/models/gemma-3-how-to-run-and-fine-tune) |
| DreamBooth LoRA | Stable Diffusion | [PEFT Guide](https://huggingface.co/docs/peft/main/en/task_guides/dreambooth_lora) |

### Multi-Modal Prompting Guides

| Resource | Description | Link |
|----------|-------------|------|
| T5Gemma 2 Usage | Image + Text prompting | [HF Model Card](https://huggingface.co/google/t5gemma-2-1b-1b) |
| PaliGemma Guide | Vision-Language | [Google AI](https://ai.google.dev/gemma/docs/paligemma) |
| Gemini Multimodal | API examples | [Google AI Docs](https://ai.google.dev/gemini-api/docs/vision) |

---

## üáπüá≠ Thai Language Best Practices

### Recommended Models for Thai

| Task | Model | Notes |
|------|-------|-------|
| **Text Generation** | Qwen 2.5-7B | Strong Thai support |
| **Embeddings** | paraphrase-multilingual-MiniLM-L12-v2 | Good for Thai RAG |
| **TTS** | th-TH-NiwatNeural / th-TH-PremwadeeNeural | Edge-TTS voices |
| **Transcription** | Whisper (base/small) | Decent Thai accuracy |
| **Translation** | Helsinki-NLP/opus-mt-th-en | Thai ‚Üî English |

### Thai Text Processing Tips

```python
# Thai word segmentation
from pythainlp.tokenize import word_tokenize

text = "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏î‡∏µ‡∏°‡∏≤‡∏Å"
tokens = word_tokenize(text, engine="newmm")
print(tokens)  # ['‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ', '‡∏Ñ‡∏£‡∏±‡∏ö', '‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ', '‡∏≠‡∏≤‡∏Å‡∏≤‡∏®', '‡∏î‡∏µ', '‡∏°‡∏≤‡∏Å']

# Thai sentence segmentation
from pythainlp.tokenize import sent_tokenize

text = "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡πÑ‡∏õ‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÄ‡∏ñ‡∏≠‡∏∞"
sentences = sent_tokenize(text)

# Thai spell check
from pythainlp.spell import spell

misspelled = "‡∏™‡∏ß‡∏±‡∏î‡∏î‡∏µ"
corrections = spell(misspelled)
```

### Prompt Engineering for Thai

```python
# Good: Be specific about language
prompt = """
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢
‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô copy ‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå‡∏Å‡∏≤‡πÅ‡∏ü CoffeeLab
‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡∏Ñ‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏® ‡∏≠‡∏≤‡∏¢‡∏∏ 25-35 ‡∏õ‡∏µ
‡πÇ‡∏ó‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á: ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÅ‡∏ï‡πà‡∏î‡∏π‡∏û‡∏£‡∏µ‡πÄ‡∏°‡∏µ‡∏¢‡∏°
‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß: 100-150 ‡∏Ñ‡∏≥
"""

# Include examples (few-shot)
prompt = """
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á copy ‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ:
- "‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡πÅ‡∏ü‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πà CoffeeLab Cold Brew"
- "‡∏ó‡∏∏‡∏Å‡∏´‡∏¢‡∏î‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏¥‡∏ñ‡∏µ‡∏û‡∏¥‡∏ñ‡∏±‡∏ô ‡∏à‡∏≤‡∏Å bean ‡∏ñ‡∏∂‡∏á cup"

‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô copy ‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô...
"""
```

---

## üîß Troubleshooting Guide

### Common Issues

#### 1. CUDA Out of Memory

```python
# Solution 1: Use 4-bit quantization
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(load_in_4bit=True)
model = AutoModel.from_pretrained("...", quantization_config=bnb_config)

# Solution 2: Reduce batch size
training_args = TrainingArguments(per_device_train_batch_size=2)

# Solution 3: Gradient checkpointing
model.gradient_checkpointing_enable()
```

#### 2. Colab Session Disconnect

```python
# Keep session alive
import time
from IPython.display import display, Javascript

def keep_alive():
    display(Javascript('function Click(){document.querySelector("colab-connect-button").click()}setInterval(Click, 60000)'))

keep_alive()
```

#### 3. MongoDB Connection Issues

```python
# Check connection
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure

try:
    client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
    client.admin.command('ping')
    print("Connected!")
except ConnectionFailure:
    print("Failed to connect. Check:")
    print("1. IP whitelist (0.0.0.0/0 for dev)")
    print("2. Username/password")
    print("3. Network connectivity")
```

#### 4. Edge-TTS Rate Limiting

```python
# Add delay between requests
import asyncio

async def generate_batch(texts, voice, output_dir):
    for i, text in enumerate(texts):
        await generate_voice(text, voice, f"{output_dir}/audio_{i}.mp3")
        await asyncio.sleep(1)  # 1 second delay
```

#### 5. FFmpeg Not Found

```bash
# Ubuntu/Codespaces
sudo apt-get update && sudo apt-get install -y ffmpeg

# Mac
brew install ffmpeg

# Check installation
ffmpeg -version
```

---

## üìä Quick Reference Cards

### API Keys Required

| Service | Get Key At | Env Variable |
|---------|------------|--------------|
| Hugging Face | https://huggingface.co/settings/tokens | `HF_TOKEN` |
| Google Gemini | https://aistudio.google.com/apikey | `GEMINI_API_KEY` |
| MongoDB Atlas | Atlas Dashboard | `MONGO_URI` |

### Model Loading Cheatsheet

```python
# T5Gemma 2
from transformers import AutoProcessor, AutoModelForSeq2SeqLM
processor = AutoProcessor.from_pretrained("google/t5gemma-2-1b-1b")
model = AutoModelForSeq2SeqLM.from_pretrained("google/t5gemma-2-1b-1b")

# FunctionGemma (with Unsloth)
from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained("unsloth/functiongemma-270m-it")

# With LoRA
from peft import PeftModel, LoraConfig, get_peft_model
lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "o_proj"])
model = get_peft_model(model, lora_config)

# 4-bit Quantization
from transformers import BitsAndBytesConfig
bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4")
model = AutoModel.from_pretrained("...", quantization_config=bnb_config)
```

### Common pip Installs

```bash
# Core
pip install transformers torch accelerate

# Fine-tuning
pip install peft bitsandbytes trl datasets

# Optimization
pip install ax-platform botorch

# Vector DB
pip install chromadb pymongo

# Production Tools
pip install edge-tts moviepy pillow

# Smart Cut
pip install openai-whisper ffmpeg-python pydub

# Deployment
pip install gradio
```

---

## üéØ Success Metrics

| Metric | Target |
|--------|--------|
| Strategy generation (T5Gemma 2) | < 30s |
| Tool calling (FunctionGemma) | < 1s per call |
| Tool calling accuracy | ‚â• 85% |
| Prompt quality | 1000+ chars |
| Total pipeline | < 5 min |
| Transcription speed | < 0.5x realtime |
| Auto trim | < 1 min per 10 min video |
| **Cost** | **$0.00** |

---

## üìö Additional Resources

### Official Documentation Hub

| Category | Links |
|----------|-------|
| **Models** | [T5Gemma 2](https://huggingface.co/collections/google/t5gemma-2) ‚Ä¢ [FunctionGemma](https://huggingface.co/google/functiongemma-270m-it) ‚Ä¢ [Gemini API](https://ai.google.dev/) |
| **Training** | [PEFT](https://huggingface.co/docs/peft) ‚Ä¢ [Unsloth](https://docs.unsloth.ai/) ‚Ä¢ [Meta Ax](https://ax.dev/) |
| **Data** | [MongoDB Atlas](https://www.mongodb.com/docs/atlas/) ‚Ä¢ [ChromaDB](https://docs.trychroma.com/) ‚Ä¢ [HF Datasets](https://huggingface.co/docs/datasets) |
| **Production** | [MoviePy](https://zulko.github.io/moviepy/) ‚Ä¢ [Edge-TTS](https://github.com/rany2/edge-tts) ‚Ä¢ [Whisper](https://github.com/openai/whisper) |
| **Deploy** | [HF Spaces](https://huggingface.co/docs/hub/spaces) ‚Ä¢ [Gradio](https://www.gradio.app/) |

### Community Resources

| Resource | Link |
|----------|------|
| Hugging Face Forum | https://discuss.huggingface.co/ |
| Unsloth Discord | https://discord.gg/unsloth |
| MoviePy Reddit | https://www.reddit.com/r/moviepy/ |

---

**üé¨ AI Director v3.4 - Complete Documentation Edition**

**New in v3.4:**
- üìö Comprehensive documentation for all components
- üîó Official links to documentation
- üìñ Tutorial notebooks and examples
- üáπüá≠ Thai language best practices
- üîß Troubleshooting guide
- üìä Quick reference cards

**Zero Cost Stack ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°: $0.00**

**Happy Directing! üöÄ**
